[["index.html", "Notas de Clase: Series de Tiempo Análisis de Series de Tiempo Contexto del documento", " Notas de Clase: Series de Tiempo Benjamín Oliva, Omar Alfaro Rivera &amp; Emiliano Pérez Caullieres 2025-01-28 Análisis de Series de Tiempo Notas de Clase Otoño 2024 by Benjamín Oliva, Omar Alfaro Rivera &amp; Emiliano Pérez Caullieres Contexto del documento Estas son notas de clase de la materia de Análisis de Series de Tiempo de la Facultad de Economía de la Universidad Nacional Autónoma de México. Este es un trabajo siempre en proceso de mejora. Para cualquier comentario o aclaración escribir a los correos benjov@ciencias.unam.mx o omarxalpha@gmail.com. La versión PDF de estas notas se encuentra en: https://github.com/benjov/Series-Tiempo/blob/main/docs/Notas-Series-Tiempo.pdf Las tablas de datos empleadas en los diferentes ejemplos se encuentran en: https://github.com/benjov/Series-Tiempo/tree/main/BD "],["introducción.html", "Chapter 1 Introducción", " Chapter 1 Introducción Estas notas son un resumen, una síntesis comparativa y, en algunos casos, una interpretación propia de los libros de texto de Brooks (2019), Cowpertwait y Metcalfe (2009), Guerrero-Guzmán (2014), Enders (2015), Franses y van Dijk (2003), Kirchgassner, Wolters, y Hassler (2012), Lutkepohl (2005), Wei (2019), entre otros. En algunos casos se incorpora información adicional para efectos de dar contexto al tema analizado (ver sección de Bibliografía para mayores detalles). El objetivo de este documento es proporcionar un conjunto de apuntes o notas que sirvan de apoyo para la clase de Series de Tiempo en la Facultad de Economía de la UNAM. Por esta razón, no deben considerarse como notas exhaustivas o como un sustituto de la clase y los laboratorios. Asimismo, es deseable que los alumnos puedan aportar sus observaciones y correcciones a estas notas, las observaciones a estas notas son esperadas y siempre serán bienvenidas y agradecidas. Este es un trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com. En estas notas se estudian los temas que típicamente son incluidos como parte de un curso estándar de análisis de series de tiempo y agrega otros tantos, los cuales son: Modelos estacionarios univaraidos: \\(AR(p)\\), \\(MA(q)\\), \\(ARMA(p, q)\\) y \\(ARIMA(p, d, q)\\), y filtros para eliminar estacionalidad, entre otros; Modelos no estacionarios univariados y Pruebas de raíz unitaria (o pruebas para determinar que una serie es estacionaria); Modelos multivariados, entre lo que se incluye a los Vectores Autoregresivos (VAR) y los procedimientos de Cointegración Modelación de series univariadas con errores con heterocedasticidad y autocorrelación: ARCH(r), GARCH(n), etc.; Modelos multivariados con errores con heterocedasticidad y autocorrelación: M-GARCH y M-GARCH-M; Casos particulares en los que las series incluidas en un modelo multivariado no son del mismo orden de integración, conocidos como modelos ADRL. Modelos de Datos Panel en series de tiempo, y Modelos no lineales como los de cambios de régimen. "],["introducción-al-análisis-de-series-de-tiempo-y-elementos-de-ecuaciones-en-diferencia.html", "Chapter 2 Introducción al Análisis de Series de Tiempo y Elementos de Ecuaciones en Diferencia 2.1 La naturaleza de los datos de Series de Tiempo 2.2 Ejemplos y aplicaciones de las Series de Tiempo 2.3 Ecuaciones en Diferencia para procesos deterministas 2.4 Operador de rezago L", " Chapter 2 Introducción al Análisis de Series de Tiempo y Elementos de Ecuaciones en Diferencia 2.1 La naturaleza de los datos de Series de Tiempo El análisis de series de tiempo tiene muchas aplicaciones en diversos campos de la ciencia. Por ejemplo, en la economía continuamente se está expuesto a información recopilada de los mercados financieros, indicadores de empleo, índices o indicadores del nivel de producción, índices de precios, etc. En otros campos de las ciencias sociales se emplea el análisis de series de tiempo para analizar la evolución de la población, los nacimientos, o el número de personas con matrículas escolares. Finalmente, en las ciencias exactas se pueden encontrar casos como los de un epidemiólogo que puede estar interesado en el número de casos de influenza observados en algún periodo de tiempo dado y si a estos se les puede asociar con algún tipo de estacionalidad o si se trata del inicio de un fenómeno atípico. La primera aproximación que se suele tener a las series de tiempo es mediante el examen de datos puestos en una gráfica, en la cual uno de los ejes es el tiempo y el otro es el valor tomado por la variable. No obstante, en este tipo de exámenes existen dos enfoques. Por un lado, existe el enfoque de la importancia del tiempo, el cual consiste en reconocer cómo lo que sucede hoy es afectado por lo que pasó ayer o, en general, en períodos pasados, o cómo lo que pasa hoy afectará los eventos futuros. Por otro lado, existe el enfoque del análisis frecuentista o de frecuencia, mediante el cual se busca reconocer la importancia que tiene para los investigadores los ciclos –por ejemplo, ciclos estacionales, momentos de crisis económicas, etc.– 2.2 Ejemplos y aplicaciones de las Series de Tiempo Un primer ejemplo que puede ilustrar la presencia de los dos tipos de enfoques antes mencionados es la Figura 2.1. En esta figura se muestra la evolución del Indicador Global de la Actividad Económica (IGAE) en su versión global o del total de la economía y en su versión únicamente para las actividades primarias entre enero de 2002 y mayo de 2023. library(readxl) Base_1 &lt;- read_excel(&quot;BD/Base_1_TimeSeries.xlsx&quot;) IGAE_2013 &lt;- ts(Base_1$IGAE_2013, start = 2002, freq = 12) IGAE_PRIM_2013 &lt;- ts(Base_1$IGAE_PRIM_2013, start = 2002, freq = 12) ICC &lt;- ts(Base_1$ICC, start = 2002, freq = 12) ICC_LAG &lt;- ts(Base_1$ICC_LAG, start = 2002, freq = 12) IPC_BMV &lt;- ts(Base_1$IPC_BMV, start = 2002, freq = 12) TDC &lt;- ts(Base_1$TDC, start = 2002, freq = 12) plot(IGAE_2013, type = &quot;l&quot;, lwd = 1, col = &quot;red&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, ylim = c(60,160)) par(new = T) # Indicador Global de la Actividad Económica, Act. Prim., base 2008 plot(IGAE_PRIM_2013, type = &quot;l&quot;, lwd = 1, col = &quot;blue&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, ylim = c(60,160)) # Leyenda legend(&quot;topleft&quot;, c(&quot;IGAE&quot;,&quot;IGAE Act. Prim.&quot;), cex = 0.8, lty = 1:1, col = c(&quot;red&quot;, &quot;blue&quot;)) Figure 2.1: Indicador Global de Actividad Económica (IGAE) Global y para las Actividades Primarias (2008=100), Ene.2002 - May.2023 par(new = F) Como se puede observar, el IGAE del total de la economía muestra, principalmente, que el enfoque del tiempo es más relevante. Es decir, que existe cierta persistencia en el indicador, lo que significa que la economía crece en razón del crecimiento reportado en períodos pasados. No obstante, lo que no podemos reconocer es que los eventos futuros tienen un efecto en el desempeño de la economía hoy día. Así, no es común observar cambios abruptos del indicador, salvo por la crisis global de 2008 y la reciente crisis causada por la COVID-19. Por el contrario, el IGAE de las actividades primarias muestra una presencia significativa de la importancia de la frecuencia. No pasa desapercibido que existen muchos ciclos en la evolución del indicador. Algo que suena común en las actividades primarias, cuya producción depende de eventos que son cíclicos y que están asociados con el clima u otros factores determinantes de la oferta de productos agrícolas. Otro factor que puede influir en el indicador son elementos de demanda, más que los de oferta. Por ejemplo, el consumo de alimentos típicos de algunas temporadas del año. Como segundo ejemplo, en la Figura 2.2 se ilustra la evolución reciente del índice de Confianza del Consumidor (ICC) en dos de sus versiones: i) el Índice global y ii) el Índice de confianza de los consumidores cuando estos consideran la situación actual en la economía en relación el año anterior. plot(ICC, type = &quot;l&quot;, lwd = 1, col = &quot;red&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, ylim = c(29, 50)) # Comando que indica a R que sin borrar la grafica anterior, # grafique la siguiente. par(new = T) # Indice ??Como considera usted la situacion economica del pais # hoy en dia comparada con la de hace 12 meses?, base enero 2003 plot(ICC_LAG, type = &quot;l&quot;, lwd = 1, col = &quot;blue&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, ylim = c(29,50)) # Leyenda legend(&quot;bottomleft&quot;, c(&quot;ICC&quot;,&quot;ICC lag&quot;), cex = 0.8, lty = 1:1, col = c(&quot;red&quot;, &quot;blue&quot;)) Figure 2.2: Índice de Confianza del Consumidor (ICC): General y resultado de ¿Cómo considera usted la situación economica del país hoy en día comparada con la de hace 12 meses? (puntos), Ene.2002-may.2023 par(new = F) Destacamos que el ICC mide las expectativas de los consumidores en razón de la información pasada y de la esperada, según dichos consumidores. Así, es probable que las dos series de tiempo exhiban un gran peso para los eventos pasados, pero a la vez, un componente –probablemente menor– del componente de frecuencia. Esto último en razón de que los consumidores suelen considerar en sus expectativas de consumo los períodos cíclicos de la economía: temporadas navideñas, pagos de colegiaturas, etc. Este segundo ejemplo también ilustra que la confianza del consumidor no necesariamente está directamente correlacionada con el desempeño de la economía. Como tercer ejemplo se muestra la evolución de dos series. La Figura 2.3 ilustra el comportamiento reciente de dos indicadores que son referencia para los inversionistas. Por un lado, se ubica el índice de Precios y Cotizaciones de la BMV (IPC), el cual refleja el valor de las acciones de empresas que cotizan en la BMV y el volumen de acciones comercializadas, en conjunto. En el mundo de las finanzas se ha interpretado que el IPC refleja el rendimiento del capital promedio invertido en las empresas que cotizan en la BMV. par(mfrow=c(1,2)) # Indice de Precios y Cotizaciones de la Bolsa Mexicana de Valores plot(IPC_BMV, type = &quot;l&quot;, lwd = 1, col = &quot;red&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, main = &quot;Indice de Precios y \\nCotizaciones BMV&quot;) # Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera plot(TDC, type = &quot;l&quot;, lwd = 1, col = &quot;blue&quot;, ylab = &quot;Pesos X Dolar&quot;, xlab = &quot;Tiempo&quot;, main = &quot;Tipo de Cambio&quot;) Figure 2.3: índice de Precios y Cotizaciones de la Bolsa Mexicana de Valores (Panel Derecho) y Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera, pesos por dólar (Panel izquierdo), Ene.2002-May.2023 par(mfrow=c(1,1)) Por otro lado, en la Figura 2.3 se presenta la evolución del Tipo de Cambio (TDC) –indicador financiero que se suele utilizar como medio de reserva de valor–. Esto, en razón de que el TDC es conocido como un instrumento que en momentos de crisis toma valores contracíclicos de la economía mexicana. No obstante, ambos indicadores no son comparables. Para hacerlos comparables en la Figura 2.4 se presentan en versión índice con una base en el primer mes de la muestra. IPC_BMV_I &lt;- 100*IPC_BMV/IPC_BMV[1] TDC_I &lt;- 100*TDC/TDC[1] # Indice del indice de Precios y Cotizaciones de la Bolsa Mexicana # de Valores plot(IPC_BMV_I, type = &quot;l&quot;, lwd = 1, col = &quot;red&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, ylim = c(80,740)) # Comando que indica a R que sin borrar la grafica anterior, # grafique la siguiente. par(new = T) # Indice del Tipo de Cambio para Solventar Obligaciones en # Moneda Extranjera plot(TDC_I, type = &quot;l&quot;, lwd = 1, col = &quot;blue&quot;, ylab = &quot;Indice&quot;, xlab = &quot;Tiempo&quot;, ylim = c(80,740)) # Leyenda legend(&quot;topleft&quot;, c(&quot;Indice del IPC&quot;,&quot;Indice del TDC&quot;), cex = 0.8, lty = 1:1, col = c(&quot;red&quot;, &quot;blue&quot;)) Figure 2.4: Índice del índice de Precios y Cotizaciones de la Bolsa Mexicana de Valores e Índice del Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera (ambos, enero de 2002 = 100), pesos por dólar, Ene.2002-May.2023 par(new = F) En la perspectiva de la Figura 2.4 se puede apreciar que el TDC no es tan rentable, ya que una inversión en la BMV mediante el IPC, en el largo plazo, muestra más rendimientos. Asimismo, la Figura 2.4 ilustra que en ambas series se observa un dominio de la condición de tiempo y no uno de la frecuencia. Es decir, tanto el IPC como el TDC no responden a condiciones como ciclos o temporadas que sí son observables en actividades económicas como las primarias. Finalmente, la Figura 2.5 ilustra una característica que también resulta de gran interés en el análisis de series de tiempo: los datos de alta frecuencia y de comportamiento no regular. En la Figura 2.5 se muestran las diferencias logarítmicas de las series de IGAE de la actividad total, el IPC y el TDC. par(mfrow=c(3,1)) # Indicador Global de la Actividad Econ?mica, base 2008 plot(diff(log(IGAE_2013), lag = 1), type = &quot;l&quot;, lwd = 1, col = &quot;darkred&quot;, ylab = &quot;Var. %&quot;, xlab = &quot;Tiempo&quot;, main = &quot;Indicador Global de la Actividad Economica&quot;) # Indice de Precios y Cotizaciones de la Bolsa Mexicana de Valores plot(diff(log(IPC_BMV), lag = 1), type = &quot;l&quot;, lwd = 1, col = &quot;darkgreen&quot;, ylab = &quot;Var. %&quot;, xlab = &quot;Tiempo&quot;, main = &quot;Indice de Precios y Cotizaciones BMV&quot;) # Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera plot(diff(log(TDC), lag = 1), type = &quot;l&quot;, lwd = 1, col = &quot;darkblue&quot;, ylab = &quot;Pesos X Dolar&quot;, xlab = &quot;Tiempo&quot;, main = &quot;Tipo de Cambio&quot;) Figure 2.5: Tasas de Crecimiento mensuales (diferencias logarítmicas) de Indicador Global de la Actividad Económica, Índice de Precios y Cotizaciones de la Bolsa Mexicana de Valores (Panel Derecho) y Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera, Ene.2002-May.2023 par(mfrow=c(1,1)) Dichas diferencias se pueden interpretar como una tasa de crecimiento de las series por las siguientes razones. Consideremos una serie de tiempo dada por \\(y_t\\), cuya versión logarítmica es \\(ln(y_t\\)). De esta forma, la diferencia logarítmica está dada por la ecuación (2.1): \\[\\begin{equation} \\Delta ln(y_t) = ln(y_t) - ln(y_{t-1}) = ln \\left( \\frac{y_t}{y_{t-1}} \\right) \\tag{2.1} \\end{equation}\\] Ahora bien, si retomamos la definición de tasa de crecimiento (\\(TC\\)) de una serie de tiempo \\(y_t\\) entre el periodo \\(t\\) y \\(t-1\\) podemos obtener que: \\[\\begin{equation} TC = \\frac{y_t - y_{t-1}}{y_{t-1}} = \\frac{y_t}{y_{t-1}} - 1 \\tag{2.2} \\end{equation}\\] De esta forma, si tomamos el logaritmo de la expresión de la ecuación (2.2) obtenemos la siguiente aproximación: \\[\\begin{equation} \\frac{y_t}{y_{t-1}} -1 \\approx ln \\left( \\frac{y_t}{y_{t-1}} \\right) = ln(y_t) - ln(y_{t-1}) \\tag{2.3} \\end{equation}\\] La ecuación (2.3) es cierta cuando los valores de \\(y_t\\) y \\(y_{t-1}\\) son muy parecidos, es decir, cuando las variaciones no son tan abruptas. Otra forma de interpretar la ecuación (2.3) es que para tasas de crecimiento pequeñas, se puede utilizar como una buena aproximación a la diferencia logarítmica mostrada en la ecuación (2.1). En la Figura 2.5 se reportan las diferencias logarítmicas del IGAE, IPC y TDC, todos, como una media de distintos tipos de rendimientos. Es decir, podemos decir que un capitalista promedio (suponiendo que solo puede invertir en la actividad económica, en la bolsa o en el dólar), puede observar que le es más redituable en función de sus preferencias. Notése que la dinámica de las variaciones de cada una de las series es significativamente diferente. Destaca que el TDC es una de las variables que, en general, no muestra grandes cambios a lo largo del tiempo. No obstante, se han observado cambios radicales, cuando menos en el año 2008 y durante la pandemia de COVID-19. Esta situación también se ha observado para el IPC. En cambio, el IGAE muestra un comportamiento más estable o estacionario –concepto que abordaremos más adelante–. 2.3 Ecuaciones en Diferencia para procesos deterministas En las secciones previas se hizo una introducción al concepto de series de tiempo. En esta sección se pretende desarrollar la construcción de los procesos generadores de datos de las series de tiempo. En un sentido más formal, se expondrá que las series de tiempo se pueden considerar como una secuencia de variables aleatorias. Para tales efectos, se desarrollará una introducción al concepto de ecuaciones en diferencia. Así, las preguntas que se pretende responder son: ¿Cuál es la solución de la ecuación en diferencia que se estudia? ¿Cuáles son las condiciones para que un proceso estocástico, representado mediante una ecuación en diferencia, llegue a alcanzar un punto de equilibrio en el largo plazo? El término de ecuación en diferencia sirve para denominar un proceso similar o equivalente dentro de las ecuaciones diferenciales, dentro del cual se consideran a un conjunto de variables que están en función del tiempo. Así, si consideramos al tiempo como una variable continua, es decir, consideramos una variable \\(Z(t)\\), podemos expresar las siguientes expresiones para la ecuación diferencial: \\[\\begin{equation} \\frac{dZ(t)}{dt}; \\frac{d^2Z(t)}{dt^2}; \\ldots; \\frac{d^kZ(t)}{dt^k} \\tag{2.4} \\end{equation}\\] Por otro lado, suponiendo el caso del tiempo en forma discreta, es decir, con \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\), entonces el comportamiento de la serie de variables dadas por \\(Z_t\\), la cual se puede expresar como: \\[\\begin{equation} \\Delta Z_t; \\Delta^2 Z_t; \\ldots; \\Delta^k Z_t \\tag{2.5} \\end{equation}\\] Observemos que una forma técnicamente más correcta es escribir las expresiones anteriores como: \\[\\begin{equation} \\frac{\\Delta Z_t}{\\Delta t}; \\frac{\\Delta^2 Z_t}{\\Delta t^2}; \\ldots; \\frac{\\Delta^k Z_t}{\\Delta t^k} \\tag{2.6} \\end{equation}\\] No obstante, no pasa desapercibido que \\(\\Delta t = 1\\), por lo que resultan equivalentes ambos conjuntos de expresiones (2.5) y (2.6). 2.3.1 Ecuaciones en Diferencia Lineales de Primer Orden El primer caso que se suele estudiar en relación a Ecuaciones en Diferencia es el de las Ecuaciones en Diferencia Lineales de Primer Orden. Al respecto, al igual que en el caso continuo, las variaciones de la variable \\(Z_t\\) se pueden expresar como se ilustra en el siguiente ejemplo. Consideremos la siguiente ecuación: \\[\\begin{equation} Z_t = a_0 + a_1 Z_{t-1} \\tag{2.7} \\end{equation}\\] Donde, \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\), y \\(a_0\\) y \\(a_1 \\neq 0\\) son números reales constantes. De (2.7) podemos despejar la variable \\(Z_{t-1}\\) y obtener una forma de ecuación en diferencia: \\[\\begin{equation} Z_t - a_1 Z_{t-1} = a_0 \\tag{2.8} \\end{equation}\\] Ahora denotemos a \\(L Z_t = Z_{t-1}\\), es decir, mediante el operador \\(L\\) se puede rezagar una variable dada. En general, podemos decir que el operador tiene dos propiedades, la primera es que es lineal –en el sentido de que abre sumas y saca escalares– como se muestra en la siguiente expresión para el caso de un (1) rezago: \\[\\begin{equation} L(\\alpha Z_{t} + \\beta) = \\alpha Z_{t-1} + \\beta \\tag{2.9} \\end{equation}\\] Donde \\(\\alpha, \\beta \\in \\mathbb{R}\\) y \\(\\alpha, \\beta \\neq 0\\). Otro resultado implícito en esta primera propiedad es que el operador rezago aplicado a cualquier escalar dará como resultado el escalar, puesto que este es una constante sin importar el momento \\(t\\) en el cual se encuentre la variable \\(Z\\). La segunda propiedad del operador es que se puede aplicar de forma consecutiva a una misma variable. Es decir, \\(L ( Z_{t-1}) = L L Z_{t} = L^2 Z_{t}\\), por lo que en general tendremos: \\(L^p Z_t = Z_{t-p}\\) (con \\(p \\in \\mathbb{Z}\\)). Así, en el caso de \\(p\\) rezagos la propiedad de linealidad del operador rezago será: \\[\\begin{equation} L^p (\\alpha Z_{t} + \\beta) = \\alpha Z_{t-p} + \\beta \\tag{2.10} \\end{equation}\\] Dicho lo anterior, podemos escribir la solución general de (2.8) como: \\[\\begin{eqnarray} Z_t - a_1 L Z_t &amp; = &amp; a_0 \\nonumber \\\\ (1 - a_1 L)Z_t &amp; = &amp; a_0 \\nonumber \\\\ Z_t &amp; = &amp; a_0 \\frac{1}{1 - a_1 L} + s a^t_1 \\nonumber \\\\ Z_t &amp; = &amp; a_0 \\frac{1}{1 - a_1} + s a^t_1 \\tag{2.11} \\end{eqnarray}\\] Donde \\(a_1 \\neq 1\\) y \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Notése que la aplicación del operador rezago \\(L\\) a la constante \\(a_1\\) dará como resultado el valor de la misma constante, ya que ésta no depende del momento \\(t\\) en el cual observemos a la variable \\(Z_t\\). En la ecuación (2.11) se adiciona un término \\(s a^t_1\\) que permite ubicar la trayectoria inicial de la solución de la ecuación. El componente no significa un cambio respecto de la ecuación (2.8) original, ya que si buscáramos reconstruir esta ecuación tendríamos: \\[\\begin{eqnarray} (1 - a_1 L) s a^t_1 &amp; = &amp; s a^t_1 - a_1 s L a^{t}_1 \\nonumber \\\\ &amp; = &amp; s a^t_1 - a_1 s a^{t - 1}_1 \\nonumber \\\\ &amp; = &amp; s a^t_1 - s a^t_1 \\nonumber \\\\ &amp; = &amp; 0 \\nonumber \\end{eqnarray}\\] La ecuación (2.11) se suele interpretar como la solución de largo plazo. Ahora demostraremos por qué es cierta la ecuación y discutiremos algunas condiciones que se deben observar en esta solución para que sea una solución convergente. No obstante, primero discutiremos un método indirecto e incompleto para demostrar el resultado. Dicho método es conocido como el método iterativo. Plantearemos las siguientes ecuaciones particulares donde suponemos la existencia del valor inicial \\(Z_0\\) del proceso: \\[\\begin{equation*} Z_1 = a_0 + a_1 Z_0 \\end{equation*}\\] \\[\\begin{eqnarray*} Z_2 &amp; = &amp; a_0 + a_1 Z_1 \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_1 Z_0) \\\\ &amp; = &amp; a_0 + a_0 a_1 + a^2_1 Z_0 \\\\ &amp; = &amp; a_0 (1 + a_1) + a^2_1 Z_0 \\end{eqnarray*}\\] \\[\\begin{eqnarray*} Z_3 &amp; = &amp; a_0 + a_1 Z_2 \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_0 a_1 + a^2_1 Z_0) \\\\ &amp; = &amp; a_0 + a_0 a_1 + a_0 a^2_1 + a^3_1 Z_0 \\\\ &amp; = &amp; a_0 (1 + a_1 + a^2_1) + a^3_1 Z_0 \\end{eqnarray*}\\] De lo anterior se puede inferir que el método iterativo convergerá hacia una expresión como la siguiente en el momento \\(t\\): \\[\\begin{eqnarray} Z_t &amp; = &amp; a_0 + a_1 Z_{t-1} \\nonumber \\\\ &amp; = &amp; a_0 (1 + a_1 + a^2_1 + \\ldots + a^{t-1}_1) + a^t_1 Z_0 \\nonumber \\\\ &amp; = &amp; a_0 \\sum^{t-1}_{i = 0}{a^i_1} + a^t_1 Z_0 \\tag{2.12} \\end{eqnarray}\\] Donde, es necesario que en la ecuación (2.12) se cumpla que \\(\\lvert{a_1}\\lvert &lt; 1\\) para que la suma sea convergente –más adelante detallaremos esta afirmación–. A este tipo de ecuaciones se les puede denominar como lineales. Esto en razón de que ningún término de la variable \\(Z\\) aparece elevado a ninguna potencia distinta a 1. También, son de primer orden, ya que el rezago de la variable \\(Z\\) es sólo de un período. En adelante trabajaremos con ecuaciones en las que la variable \\(Z\\) se encuentra rezagada en cualquiera de los siguientes casos: \\[\\begin{equation} Z_t, Z_{t-1}, Z_{t-2}, Z_{t-3}, \\ldots, Z_{t-p}, \\ldots \\tag{2.13} \\end{equation}\\] Por lo que diremos que en adelante el curso versará sobre ecuaciones en diferencia lineales y de cualquier orden \\(p \\in \\mathbb{N}\\). Retomando la ecuación (2.12) y considerando la parte de la suma de los términos de \\(a^i_1\\), de tal forma que buscaremos dar una expresión más comprensible a dicho término. Definamos la siguiente expresión como: \\[\\begin{equation} S_{t-1} = \\sum^{t-1}_{i = 0}{a^i_1} \\tag{2.14} \\end{equation}\\] Por lo tanto, \\(S_t\\) estaría dado por la siguiente expresión: \\[\\begin{eqnarray} S_{t} &amp; = &amp; a_1 \\sum^{t-1}_{i = 0}{a^i_1} \\nonumber \\\\ &amp; = &amp; a_1 (1 + a_1 + a^2_1 + \\ldots + a^{t-1}_1) \\nonumber \\\\ &amp; = &amp; a_1 + a^2_1 + a^3_1 + \\ldots + a^{t}_1 \\nonumber \\\\ &amp; = &amp; a_1 S_{t-1} \\tag{2.15} \\end{eqnarray}\\] Tomando los dos resultados de las ecuaciones (2.14) y (2.15) anteriores, podemos expresar que si a \\(S_{t-1}\\) le restamos \\(S_t\\), y desarrollando ambos lados de la ecuación anterior podemos obtener: \\[\\begin{eqnarray} S_{t-1} - a_1 S_{t-1} &amp; = &amp; S_{t-1} - S_{t} \\nonumber \\\\ (1 - a_1) S_{t-1} &amp; = &amp; (1 + a_1 + a^2_1 + \\ldots + a^{t-1}_1) - (a_1 + a^2_1 + a^3_1 + \\ldots + a^{t}_1) \\nonumber \\\\ (1 - a_1) S_{t-1} &amp; = &amp; 1 - a^{t}_1 \\nonumber \\end{eqnarray}\\] Así, podemos concluir que: \\[\\begin{equation} S_{t-1} = \\frac{1 - a^{t}_1}{1 - a_1} \\tag{2.16} \\end{equation}\\] Conjuntando este último resultado de la ecuación (2.16) con la ecuación (2.12) tenemos la siguiente solución por el método de iteración: \\[\\begin{equation} Z_t = a_0 \\left( \\frac{1 - a^{t}_1}{1 - a_1} \\right) + a^t_1 Z_0 \\tag{2.17} \\end{equation}\\] De esta forma la ecuación (2.17) es una solución para la ecuación (2.12), que es una ecuación de un proceso de una Ecuación en Diferencia planteado en la ecuación (2.7). Esta solución aún no es general, en el sentido de que sea válida para cualquier tipo de proceso: convergente o divergente. Dicha convergencia o divergencia estará determinada por el parámetro \\(a_1\\). No debe pasar desapercibido que cuando \\(t \\rightarrow \\infty\\) o cuando la muestra es muy grande (lo que es equivalente), podemos decir que la solución solo puede converger a la siguiente expresión cuando se considera que \\(|a_1| &lt; 1\\): \\[\\begin{equation} Z_t = a_0 \\left( \\frac{1}{1 - a_1} \\right) \\tag{2.18} \\end{equation}\\] Retomemos ahora el caso general descrito en la ecuación (2.11) y determinemos una solución general en la cual \\(a_1 \\neq 1\\) y \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Para ello, observemos que el siguiente componente en la ecuación mencionada se puede interpretar como la suma infinita de términos descritos como: \\[\\begin{eqnarray} \\frac{1}{1 - a_1} &amp; = &amp; 1 + a_1 + a_1^2 + \\ldots + a_1^t + \\ldots \\nonumber \\\\ &amp; = &amp; \\sum_{i = 0}^{\\infty} a_1^{i} \\tag{2.19} \\end{eqnarray}\\] Donde claramente es necesario que \\(|a_1| &lt; 1\\). Por lo tanto, sólo faltaría determinar el valor de la constante \\(s\\) en la ecuación (2.11) de la siguiente forma, supongamos que observamos el proceso en el momento inicial, por lo que es posible determinar el valor de la constante conociendo el valor inicial del proceso como sigue: \\[\\begin{equation} Z_0 = a_0 \\frac{1}{1 - a_1} + s \\tag{2.20} \\end{equation}\\] De la ecuación (2.20) tenemos que: \\[\\begin{equation} s = Z_0 - a_0 \\frac{1}{1 - a_1} \\tag{2.21} \\end{equation}\\] Así, juntando la ecuación (2.11) y la ecuación (2.21) tenemos la expresión: \\[\\begin{equation} Z_t = a_0 \\frac{1 - a^t_1}{1 - a_1} + a^t_1 Z_0 \\tag{2.22} \\end{equation}\\] No debe pasar desapercibido que esta solución es la misma que la mostrada en la ecuación (2.17), por lo que en realidad ambas ecuaciones son una solución general indistintamente entre las ecuaciones (2.17) y (2.22). Ambas convergen a la expresión como la ecuación (2.18), con la misma condición de convergencia \\(|a_1| &lt; 1\\). Para ilustrar estas ecuaciones, veamos algunos ejemplos al respecto. . Consideremos que tenemos un proceso \\(Z_t\\) que es descrito por una ecuación en diferencia lineal de primer orden dada por: \\[\\begin{equation} Z_t = 2 + 0.9 Z_{t-1} \\tag{2.23} \\end{equation}\\] Siguiendo la expresión mostrada en la ecuación (2.22), obtenemos la expresión: \\[\\begin{equation} Z_t = 2 \\left( \\frac{1 - 0.9^{t}}{1 - 0.9} \\right) + 0.9^t Z_0 \\tag{2.24} \\end{equation}\\] Donde asumiremos que el valor inicial es \\(Z_0 = 10\\) y que la expresión debe converger al valor de 20, cuando \\(t\\) es muy grande o tiende a infinito. . De forma similar, tomemos otro ejemplo en el cual asumimos la siguiente expresión: \\[\\begin{equation} Z_t = 2 - 0.5 Z_{t-1} \\tag{2.25} \\end{equation}\\] Siguiendo la expresión mostrada en la ecuación (2.22), obtenemos: \\[\\begin{equation} Z_t = 2 \\left( \\frac{1 - (-0.5)^{t}}{1 + 0.5} \\right) + (-0.5)^t Z_0 \\tag{2.26} \\end{equation}\\] Donde asumiremos que el valor inicial es \\(Z_0 = 10\\) y que la ecuación converge al valor de \\(1.3333333 \\ldots\\), cuando \\(t\\) es muy grande o tiende a infinito. Ahora simulemos el comportamiento de ambos procesos y estableceremos los resultados del Cuadro 2.1. Notemos que el segundo proceso converge de una forma más rápida que el primero. El Cuadro 2.1 se ilustra en las siguientes dos Figura 2.6 y Figura 2.7. library(knitr) library(tidyverse) library(kableExtra) Tiempo = c(0:100) Zt = rep(NA, 101) Zt2 = rep(NA,101) Zt[1] = 10 Zt2[1] = 10 for (i in c(2:101)){ Zt[i] = 2+0.9*Zt[i-1] Zt2[i] = 2 - 0.5*Zt2[i-1] } lista = c(1:16, 97:101) Tiempo1 = Tiempo[lista] Zt1=Zt[lista] Zt21=Zt2[lista] tabla1 = data.frame(Tiempo1, Zt1, Zt21) colnames(tabla1) &lt;- c(&quot;Tiempo&quot;, &quot;$Z_t =2+0.9Z_{t-1}$&quot;, &quot;$Z_t = 2-0.5Z_{t-1}$&quot;) kable(tabla1, caption = &quot;Dos ejemplos de Ecuaciones Lineales de Primer Orden&quot;, format = &quot;pandoc&quot;) %&gt;% kable_styling(font_size = 10) Table 2.1: Dos ejemplos de Ecuaciones Lineales de Primer Orden Tiempo \\(Z_t =2+0.9Z_{t-1}\\) \\(Z_t = 2-0.5Z_{t-1}\\) 0 10.00000 10.000000 1 11.00000 -3.000000 2 11.90000 3.500000 3 12.71000 0.250000 4 13.43900 1.875000 5 14.09510 1.062500 6 14.68559 1.468750 7 15.21703 1.265625 8 15.69533 1.367188 9 16.12580 1.316406 10 16.51322 1.341797 11 16.86189 1.329102 12 17.17570 1.335449 13 17.45813 1.332275 14 17.71232 1.333862 15 17.94109 1.333069 96 19.99960 1.333333 97 19.99964 1.333333 98 19.99967 1.333333 99 19.99970 1.333333 100 19.99973 1.333333 tabla &lt;- data.frame( Tiempo, Zt, Zt2 ) ggplot( data = tabla , aes(x = Tiempo, y = Zt) )+ geom_line(col=&quot;blue4&quot;) + geom_point(col= &quot;blue4&quot;) + labs(y=expression(Z[t])) Figure 2.6: Evolución del proceso dado por \\(Z_t =2+0.9Z_{t-1}\\) ggplot(data = tabla , aes(x = Tiempo, y=Zt2)) + geom_line(col=&quot;red4&quot;) + geom_point(col= &quot;red4&quot;) + labs(y=expression(Z[t])) Figure 2.7: Evolución del proceso dado por \\(Z_t =2-0.5Z_{t-1}\\) 2.3.2 Ecuaciones en Diferencia Lineales de Segundo Orden y de orden superior Como un segundo caso a estudiar se ubica el caso de las Ecuaciones en Diferencia Lineales de Segundo Orden y, en su caso, de orden superior. Primero, sea una ecuación como la siguiente, la cual es lineal y de segundo orden, ya que tiene asociado un término de \\(Z_t\\) rezagado dos períodos: \\[\\begin{equation} Z_t = a_0 + a_1 Z_{t-1} + a_2 Z_{t-2} \\tag{2.27} \\end{equation}\\] Donde \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\) y \\(a_1, a_2 \\neq 0\\). Reordenando la ecuación (2.27) podemos escribir: \\[\\begin{eqnarray} Z_t - a_1 Z_{t-1} - a_2 Z_{t-2} &amp; = &amp; a_0 \\nonumber \\\\ Z_t - a_1 L Z_{t} - a_2 L^2 Z_{t} &amp; = &amp; a_0 \\nonumber \\\\ (1 - a_1 L - a_2 L^2)Z_t &amp; = &amp; a_0 \\tag{2.28} \\end{eqnarray}\\] Así, la solución general propuesta para la ecuación (2.28) es la siguiente, la cual es una forma análoga a una Ecuación Lineal en Diferencia de Primer Orden: \\[\\begin{equation} Z_t = \\frac{a_0}{1 - a_1 - a_2} + s_1 g^t_1 + s_2 g^t_2 \\tag{2.29} \\end{equation}\\] En donde \\(s_1\\) y \\(s_2\\) son constantes que se determinan mediante dos condiciones iniciales –por lo que para resolver este tipo de ecuaciones requerimos conocer dos condiciones iniciales–. Los valores de \\(g_1\\) y \\(g_2\\) están relacionados con los coeficientes \\(a_1\\) y \\(a_2\\), de esta forma: \\[\\begin{equation} a_1 = g_1 + g_2 \\tag{2.30} \\end{equation}\\] \\[\\begin{equation} a_2 = - g_1 g_2 \\tag{2.31} \\end{equation}\\] Lo anterior surge del siguiente procedimiento y recordando que siempre es posible descomponer una ecuación cuadrática en expresiones como las siguientes: \\[\\begin{eqnarray} (1 - a_1 L - a_2 L^2) &amp; = &amp; (1 - g_1 L)(1 - g_2 L) \\nonumber \\\\ &amp; = &amp; 1 - g_1 L - g_2 L + g_1 g_2 L^2 \\nonumber \\\\ &amp; = &amp; 1 - (g_1 + g_2) L + g_1 g_2 L^2 \\tag{2.32} \\end{eqnarray}\\] Donde se observa la equivalencia mostrada en las ecuaciones (2.30) y (2.31). Así, considerando la ecuación (2.29) tenemos que: \\[\\begin{eqnarray} (1 - a_1 L - a_2 L^2) Z_t &amp; = &amp; (1 - g_1 L)(1 - g_2 L) Z_t \\nonumber \\\\ &amp; = &amp; a_0 + (1 - g_1 L)(1 - g_2 L) s_1 g^t_1 \\nonumber \\\\ &amp; &amp; + (1 - g_1 L)(1 - g_2 L) s_2 g^t_2 \\tag{2.33} \\end{eqnarray}\\] Por lo tanto, buscamos que, para que el proceso sea equivalente y podamos interpretar que la ecuación (2.29) sea una solución general, deberá pasar lo siguiente: \\[\\begin{equation} (1 - g_1 L) (1 - g_2 L) s_1 g^t_1 + (1 - g_1 L) (1 - g_2 L) s_2 g^t_2 = 0 \\tag{2.34} \\end{equation}\\] O, escrito de otra forma: \\[\\begin{equation} (1 - g_1 L) s_1 g^t_1 = (1 - g_2 L) s_2 g^t_2 = 0 \\tag{2.35} \\end{equation}\\] Ahora determinemos cuáles son los valores \\(g_1\\) y \\(g_2\\) dados los valores \\(a_1\\) y \\(a_2\\) que nos permitan determinar si el proceso será convergente. Para ello debemos resolver la siguiente ecuación que se deriva de la ecuación (2.32): \\[\\begin{equation} 1 - a_1 x - a_2 x^2 = (1 - g_1 x)(1 - g_2 x) = 0 \\tag{2.36} \\end{equation}\\] Donde, claramente existen dos raíces: \\(x_1 = g^{-1}_1\\) y \\(x_2 = g^{-1}_2\\). Así, la solución estará dada por las raíces de la ecuación característica: \\[\\begin{eqnarray} 1 - a_1 x - a_2 x^2 = 0 \\nonumber \\\\ a_2 x^2 + a_1 x - 1 = 0 \\tag{2.37} \\end{eqnarray}\\] Cuya solución es: \\[\\begin{equation} x = \\frac{- a_1 \\pm \\sqrt{a^2_1 + 4 a_2}}{2 a_2} \\tag{2.38} \\end{equation}\\] Es importante distinguir tres diferentes casos en relación con las raíces que surgen como solución de la ecuación (2.37), estos son: Caso I. Si \\(a^2_1 + 4 a_2 &gt; 0\\), la ecuación (2.37) proporcionará dos valores de raíces reales y distintos, eso es \\(x_1 = g^{-1}_1 \\neq x_2 = g^{-1}_2\\). Si por ahora suponemos que \\(|{g_1} &lt; 1|\\) y que \\(|{g_2} &lt; 1|\\), entonces tendremos que: \\[\\begin{eqnarray} (1 - g_1 L)^{-1} (1 - g_2 L)^{-1} a_0 &amp; =&amp; \\left( \\sum^{\\infty}_{j = 0}{g^j_1 L^j} \\right) \\left( \\sum^{\\infty}_{j = 0}{g^j_2 L^j} \\right) a_0 \\nonumber \\\\ &amp; = &amp; \\left( \\sum^{\\infty}_{j = 0}{g^j_1} \\right) \\left( \\sum^{\\infty}_{j = 0}{g^j_2} \\right) a_0 \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{(1 - g_1)(1 - g_2)} \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} \\tag{2.39} \\end{eqnarray}\\] Esto último es el punto de equilibrio de la ecuación (2.29); considerando que \\(|{g_1} &lt; 1|\\) y que \\(|{g_2} &lt; 1|\\) –notemos que los demás casos son divergentes, ya que la suma anterior no convergería–. De esta forma, la solución de la ecuación estará dada por: \\[\\begin{equation} \\lim_{t \\to \\infty} Z_t = \\frac{a_0}{1 - a_1 - a_2} \\tag{2.40} \\end{equation}\\] Caso II. Si \\(a_1^2 + 4a_2 &lt; 0\\) en la ecuación (2.37), entonces las raíces serán números complejos conjugados, es decir: \\[\\begin{equation} g_i^{-1}=a \\pm ib \\tag{2.41} \\end{equation}\\] \\[\\begin{eqnarray} g_i = u \\pm iv \\tag{2.42} \\end{eqnarray}\\] Dichas raíces las podemos escribir en coordenadas polares como: \\[\\begin{eqnarray} g_1^{-1} = r e^{i \\theta} = r (cos(\\theta) + i sen(\\theta)) \\tag{2.43} \\end{eqnarray}\\] \\[\\begin{eqnarray} g_2^{-1} = r e^{-i \\theta} = r (cos(\\theta) - i sen(\\theta)) \\tag{2.44} \\end{eqnarray}\\] Donde: \\(r = \\sqrt{u^2 + v^2}\\), a esta expresión también se le conoce como módulo Alternativamente, podemos escribir que \\(r = \\sqrt{g_1 g_2}\\). La única condición es que \\(r &lt; 1\\) para que el proceso descrito en la ecuación (2.29) sea convergente. Al igual que en el Caso I, el punto de equilibrio de la ecuación se debería ubicar alrededor de la ecuación (2.40), siempre que \\(r &lt; 1\\), por lo que el factor que determina la convergencia es el módulo, ya que si el módulo es mayor a 1, el proceso será divergente, pero si es menor a 1 convergerá a (2.40). Para ilustrar, el caso contrario es divergente puesto que representa trayectorias senoidales (oscilatorias) que sólo pueden converger si a medida que pasa el tiempo las ondas son menos amplias. Analicemos la solución: \\[\\begin{equation*} Z_t = \\frac{a_0}{1 - a_1 - a_2} + s_1 g_1^t + s_2 g_2^t \\end{equation*}\\] Donde \\(s_1\\) y \\(s_2\\) las determinamos usando las condiciones iniciales al solucionar: \\[\\begin{eqnarray*} Z_0 &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 g_1^t + s_2 g_2^t \\\\ Z_1 &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 g_1^t + s_2 g_2^t \\end{eqnarray*}\\] Donde, una vez encontradas las \\(g_1\\) y \\(g_2\\), las incognitas son \\(s_1\\) y \\(s_2\\). Para determinar \\(g_1\\) y \\(g_2\\) realizamos el siguiente procedimiento. Partimos de la ecuación: \\[\\begin{equation*} 1 - a_1 x - a_2 x^2 = 0 \\end{equation*}\\] Al factorizar notamos que: \\[\\begin{equation*} (1 - g_1 x)(1 - g_2 x) = 0 \\end{equation*}\\] De lo anterior debe quedar claro que: \\[\\begin{eqnarray*} x_1 &amp; = &amp; g_1^{-1} \\\\ x_2 &amp; = &amp; g_2^{-1} \\end{eqnarray*}\\] Donde \\(g_1\\) y \\(g_2\\) se determinan: \\[\\begin{equation*} x = \\frac{-a_1 \\pm \\sqrt{a_1^2 + 4a_2}}{2 a_2} \\end{equation*}\\] Asumiendo que obtienen números complejos: \\[\\begin{eqnarray*} x_1 &amp; = &amp; g_1^{-1} = u + iv \\\\ x_2 &amp; = &amp; g_2^{-1} = u - iv \\end{eqnarray*}\\] Las cuales se pueden reescribir en coordenadas polares: \\[\\begin{eqnarray*} g_1^{-1} &amp; = &amp; s e^{i \\theta} = s [cos(\\theta) + i sen(\\theta)] \\\\ g_2^{-1} &amp; = &amp; s e^{-i \\theta} = s [cos(\\theta) - i sen(\\theta)] \\\\ s &amp; = &amp; \\sqrt{u^2 + v^2} = \\sqrt{g_1^{-1} \\cdot g_2^{-1}} \\\\ cos(\\theta) &amp; = &amp; \\frac{u}{s} \\\\ sen(\\theta) &amp; = &amp; \\frac{v}{s} \\end{eqnarray*}\\] Recuerde que: \\(r = s^{-1} = \\sqrt{g_1 \\cdot g_2}\\), de esta forma: \\[\\begin{eqnarray*} g_1 &amp; = &amp; r[cos(\\theta) - isen(\\theta)] \\\\ g_2 &amp; = &amp; r[cos(\\theta) + isen(\\theta)] \\end{eqnarray*}\\] Retomemos nuestra solución original: \\[\\begin{eqnarray*} Z_t &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 g_1^t + s_2 g_2^t \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 r^t[cos(\\theta) - isen(\\theta)]^t + s_2 r^t[cos(\\theta) + isen(\\theta)]^t \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 r^t\\left(e^{-i \\theta}\\right)^t + s_2 r^t\\left(e^{i \\theta}\\right)^t \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 r^t\\left(e^{-i t \\theta}\\right) + s_2 r^t\\left(e^{i t \\theta}\\right) \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 r^t[cos(\\theta t) - isen(\\theta t)] + s_2 r^t[cos(\\theta t) + isen(\\theta t)] \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 r^t[(s_1 + s_2)cos(\\theta t) - i(s_2 - s_1)sen(\\theta t)] \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} + s_1 r^t[A cos(\\theta t) - B sen(\\theta t)] \\end{eqnarray*}\\] Donde \\(A\\) y \\(B\\) son constantes arbitrarias y la convergencia está definida por \\(r\\), si \\(r&gt;1\\) el proceso diverge, y converge si \\(r&lt;1\\). Finalmente: \\[\\begin{eqnarray*} \\theta &amp; = &amp; cos^{-1}cos(\\theta) \\\\ \\theta &amp; = &amp; sen^{-1}sen(\\theta) \\end{eqnarray*}\\] Caso III. Ahora revisemos el caso en el que \\(a_1^2 + 4a_2 = 0\\), de esta forma las raíces serán idénticas: \\[\\begin{equation} g = g_1^{-1} = g_2^{-1} = \\frac{-a_1}{2 a_2} \\tag{2.45} \\end{equation}\\] Así, el punto de equilibrio será dado por la solución descrita como: \\[\\begin{eqnarray} (1 - g L)^2 Z_t &amp; = &amp; a_0 \\nonumber \\\\ Z_t &amp; = &amp; \\frac{a_0}{(1 - g L)^2} + s_1 g^t + s_2 t g^t \\nonumber \\\\ &amp; = &amp; a_0 \\sum_{i = 0}^{\\infty} (1 + i) g^j + s_1 g^t + s_2 t g^t \\tag{2.46} \\end{eqnarray}\\] Donde la expresión anterior es resultado de considerar el siguiente procedimiento. Sea: \\[\\begin{eqnarray} f(g) &amp; = &amp; \\frac{1}{(1 - g)} = \\sum_{j = 0}^{\\infty} g^j \\nonumber \\end{eqnarray}\\] Por lo que si hacemos la primer derivada del la expresión anterior tenemos que: \\[\\begin{eqnarray} f&#39;(g) &amp; = &amp; \\frac{1}{(1 - g)^2} \\nonumber \\\\ &amp; = &amp; \\sum_{j = 0}^{\\infty} j g^{j-1} \\nonumber \\\\ &amp; = &amp; 0 + g^0 + 2 g^1 + 3 g^2 + \\ldots \\nonumber \\\\ &amp; = &amp; \\sum_{j = 0}^{\\infty} (1 + j) g^j \\nonumber \\end{eqnarray}\\] Ahora veámos un ejemplo de una Ecuación Lineal en Diferencia de Segundo Orden. Supongamos la ecuación y el desarrollo siguientes: \\[\\begin{eqnarray} Z_t &amp; = &amp; 3 + 0.9 Z_{t-1} - 0.2 Z_{t-2} \\nonumber \\\\ (1 - 0.9 L + 0.2 L^2) Z_t &amp; = &amp; 3 \\nonumber \\end{eqnarray}\\] La solución dada por una ecuación similar a la expresión (2.37), obtendríamos la solución dada por las ecuaciones equivalentes a: \\[\\begin{eqnarray} 1 - 0.9 x + 0.2 x^2 = 0 \\nonumber \\\\ - 0.2 x^2 + 0.9 x - 1 = 0 \\nonumber \\end{eqnarray}\\] De donde las raíces del polinomio característico \\(x_1 = g_1^{-1}\\) y \\(x_2 = g_2^{-1}\\) se obtienen de la expresión dada por: \\[\\begin{eqnarray} x &amp; = &amp;\\frac{-0.9 \\pm \\sqrt{0.81 + (4)(-0.2)}}{(2)(-0.2)} \\nonumber \\\\ &amp; = &amp; \\frac{0.9 \\pm 0.1}{0.4} \\nonumber \\end{eqnarray}\\] Dado que el componente \\(a^2_1 + 4 a_2\\) es positivo, obtendremos dos raíces reales. Las raíces estarán dadas por \\(x_1 = 2.5\\) y \\(x_2 = 2.0\\), de lo cual podemos determinar que \\(g_1 = 0.4\\) y \\(g_2 = 0.5\\). De esta forma tenemos que \\(|g_1| &lt; 1\\) y \\(|g_2| &lt; 1\\), así la ecuación converge a la expresión dada por las siguientes expresiones: \\[\\begin{eqnarray} Z_t &amp; = &amp; \\frac{3}{1 - 0.9 L + 0.2 L^2} + s_1 (0.4)^t + s_2 (0.5)^t \\nonumber \\\\ &amp; = &amp; \\frac{3}{1 - 0.9 + 0.2} + s_1 (0.4)^t + s_2 (0.5)^t \\nonumber \\\\ &amp; = &amp; \\frac{3}{(1 - 0.4)(1 - 0.5)} + s_1 (0.4)^t + s_2 (0.5)^t \\nonumber \\end{eqnarray}\\] Al final, la ecuación que describe la solución general será: \\[\\begin{equation} z_t = 10 + s_1 (0.4)^t + s_2 (0.5)^t \\tag{2.47} \\end{equation}\\] Para determinar los valores de \\(s_1\\) y \\(s_2\\) necesitamos obtener dos valores iniciales de la ecuación. Para lo cual iniciaremos con \\(t = 0\\) y, luego, obtenemos el valor de \\(t = 1\\), y consideremos el valor de \\(Z_0 = 0\\) y \\(Z_1 = 50\\): \\[\\begin{eqnarray*} Z_0 &amp; = &amp; 10 + s_1(0.4)^0 + s_2(0.5)^0 \\\\ 0 &amp; = &amp; 10 + s_1 + s_2 \\\\ Z_1 &amp; = &amp; 10 + s_1(0.4)^1 + s_2(0.5)^1 \\\\ 50 &amp; = &amp; 10 + 0.4 s_1 + 0.5 s_2 \\end{eqnarray*}\\] Por lo que la solución es: \\(s_1 = -450\\) y \\(s_2 = 440\\), de donde podemos expresar la ecuación como: \\[\\begin{equation} Z_t = 10 - 450(0.4)^t + 440(0.5)^t \\tag{2.48} \\end{equation}\\] La ecuación (2.48) anterior convergerá al valor de 10 cuando \\(t \\rightarrow \\infty\\). Para ilustrar la trayectoria de esta ecuación tomemos un cuadro similar al de los ejemplos anteriores. En el Cuadro 2.2 y la Figura 2.8 mostramos los resultados de la trayectoria para 100 períodos. t = ts(c(0:100)) Zt = 10-450*(0.4^t)+440*(0.5^t) tabla_2 &lt;- data.frame( t, Zt) lista = c(1:16, 97:101) t1 = t[lista] Zt1=Zt[lista] tabla1 = data.frame(Tiempo1, Zt1) colnames(tabla1) &lt;- c(&quot;Tiempo&quot;, &quot;$Z_t =10-450(0.4)^t+440(0.5)^t$&quot;) kable(tabla1, caption = &quot;Un ejemplo de Ecuación de Segundo Orden&quot;, format = &quot;pandoc&quot;)%&gt;% kable_styling(font_size = 10) Table 2.2: Un ejemplo de Ecuación de Segundo Orden Tiempo \\(Z_t =10-450(0.4)^t+440(0.5)^t\\) 0 0.00000 1 50.00000 2 48.00000 3 36.20000 4 25.98000 5 19.14200 6 15.03180 7 12.70022 8 11.42384 9 10.74141 10 10.38250 11 10.19597 12 10.09987 13 10.05069 14 10.02565 15 10.01294 96 10.00000 97 10.00000 98 10.00000 99 10.00000 100 10.00000 ggplot(data = tabla_2, aes(x = t, y=Zt)) + geom_line(col=&quot;green4&quot;) + geom_point(col= &quot;green4&quot;) + labs(y=expression(Z[t])) Figure 2.8: Evolución del proceso dado por \\(Z_t =3+0.9Z_{t-1}-0.2Z_{t-2}\\) Finalmente, discutiremos la solución para las Ecuaciones Lineales en Diferencia de Orden \\(p\\), donde \\(p \\geq 2\\). En general, una ecuación de este tipo se puede escribir como: \\[\\begin{equation} Z_t = a_0 + a_1 Z_{t-1} + a_2 Z_{t-2} + \\ldots + a_p Z_{t-p} \\tag{2.49} \\end{equation}\\] Donde \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\) y \\(a_p \\neq 0\\). La ecuación (2.49) se puede escribir como: \\[\\begin{eqnarray} Z_t - a_1 Z_{t-1} - a_2 Z_{t-2} - \\ldots - a_p Z_{t-p} &amp; = &amp; a_0 \\nonumber \\\\ Z_t - a_1 L Z_t - a_2 L^2 Z_t - \\ldots - a_p L^p Z_t &amp; = &amp; a_0 \\nonumber \\\\ (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p) Z_t &amp; = &amp; a_0 \\tag{2.50} \\end{eqnarray}\\] Por el Teorema Fundamental del Álgebra es posible escribir la ecuación (2.50) como: \\[\\begin{eqnarray} (1 - g_1 L)(1 - g_1 L) \\ldots (1 - g_p L) Z_t &amp; = &amp; a_0 \\tag{2.51} \\end{eqnarray}\\] Utilizando la ecuación (2.50) y la ecuación (2.51) tenemos que la solución general de una ecuación como la descrita en (2.49) se puede escribir como: \\[\\begin{equation} Z_t = \\frac{a_0}{1 - a_1 - a_2 - \\ldots - a_p} + s_1 g^t_1 + s_2 g^t_2 + \\ldots + s_p g^t_p \\\\ \\tag{2.52} \\end{equation}\\] \\[\\begin{eqnarray} Z_t &amp; = &amp; \\frac{a_0}{(1 - g_1)(1 - g_1) \\ldots (1 - g_p)} + s_1 g^t_1 + s_2 g^t_2 + \\ldots + s_p g^t_p \\tag{2.53} \\end{eqnarray}\\] Donde \\(s_1\\), \\(s_2\\), …, \\(s_p\\) son constantes que se determinan utilizando \\(p\\) valores partículares de \\(Z_t\\), y la solución general descrita en las ecuaciones (2.52) y (2.53) implica encontrar \\(p\\) raíces: \\(x_1 = g^{-1}_1\\), \\(x_2 = g^{-1}_2\\), …, \\(x_p = g^{-1}_p\\) de los siguientes polinomios equivalentes: \\[\\begin{eqnarray} (1 - g_1)(1 - g_1) \\ldots (1 - g_p) = 0 \\tag{2.54} \\end{eqnarray}\\] \\[\\begin{eqnarray} 1 - a_1 x - a_2 x^2 - \\ldots - a_p x^p = 0 \\tag{2.55} \\end{eqnarray}\\] \\[\\begin{eqnarray} a_p x^p + \\ldots + a_2 x^2 + a_1 x - 1 = 0 \\tag{2.56} \\end{eqnarray}\\] Antes de plantear la solución general, analicemos una solución particular cuando un conjunto de las \\(p\\) raíces, digamos un total de \\(m\\), son iguales, es decir, cuando sucede que \\(g_1 = g_2 = \\ldots = g_m = g\\) (con \\(1 &lt; m \\leq p\\)). En este caso la solución general en la ecuación (2.53) se escribe como: \\[\\begin{eqnarray} Z_t &amp; = &amp; \\frac{a_0}{(1 - g)^m(1 - g_{m+1}) \\ldots (1 - g_p)} \\nonumber \\\\ &amp; &amp; + s_1 g^t + s_2 t g^t + \\ldots + s_m t^{m-1} g^t + s_{m+1} g^t_{m+1} + \\ldots + s_{p} g^t_{p} \\tag{2.57} \\end{eqnarray}\\] Definamos: \\[\\begin{equation} f(g) = \\frac{1}{1 - g} = \\sum_{j = 0}^{\\infty} g^j \\tag{2.58} \\end{equation}\\] Si retomamos el método descrito párrafos arriba tenemos las siguientes expresiones. Cuando \\(m = 2\\): \\[\\begin{equation} f&#39;(g) = \\frac{1}{(1 - g)^2} = \\sum_{j = 0}^{\\infty} j g^{j-1} = \\sum_{j = 0}^{\\infty} (1 + j) g^j \\nonumber \\end{equation}\\] En el otro extremo, cuando \\(m = p\\): \\[\\begin{equation} f^{(p-1)}(g) = \\frac{p-1}{(1 - g)^p} = \\sum_{j = 0}^{\\infty} \\frac{(p-1+j)(p-2+j) \\ldots (2+j)(1+j)}{(p-1)!} g^j \\tag{2.59} \\end{equation}\\] Así, en el extremo cuando \\(m = p\\) la solución general podría estar dada por: \\[\\begin{eqnarray} Z_t &amp; = &amp; a_0 \\sum_{j = 0}^{\\infty} \\frac{(p-1+j)(p-2+j) \\ldots (2+j)(1+j)}{(p-1)!} g^j \\nonumber \\\\ &amp; &amp; + g^t \\sum_{i = 0}^p s_i t^{i-1} \\tag{2.60} \\end{eqnarray}\\] Donde \\(|{g} &lt; 1|\\), \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Para finalizar esta sección, plantearemos la expresión del polinomio característico que nos permitirá hacer el análisis de convergencia de los procesos. Partamos de que la ecuación (2.56) se puede escribir como: \\[\\begin{equation} (x^{-1})^p - a_1 (x^{-1})^{p-1} - a_2 (x^{-1})^{p-1} - \\ldots - a_p = 0 \\tag{2.61} \\end{equation}\\] La ecuación (2.61) permite interpretar las raíces del polinomio característico de forma directa ya que \\(x^{-1}_1 = g_1\\), \\(x^{-1}_2 = g_2\\), …, \\(x^{-1}_p = g_p\\). Así, siempre que \\(p \\geq 1\\) en la ecuación (2.49), diremos que el proceso descrito en esa ecuación dará como resultado un proceso convergente si se cumplen las dos condiciones (2.62) y (2.63): \\[\\begin{equation} |a_p| &lt; 1 \\tag{2.62} \\end{equation}\\] \\[\\begin{equation} a_1 + a_2 + \\ldots + a_p &lt; 1 \\tag{2.63} \\end{equation}\\] Alternativamente, cuando las raíces son reales, lo anterior es equivalente a la expresión (2.64): \\[\\begin{eqnarray} |g_i| &lt; 1 \\tag{2.64} \\end{eqnarray}\\] Para \\(\\forall i = 1, 2, \\ldots, p\\). Cuando las raíces son imaginarias, las dos condiciones (2.62) y (2.63) son equivalentes a la expresión (2.65): \\[\\begin{eqnarray} \\sqrt{g_i g_j} = \\sqrt{u^2 + v^2} &lt; 1 \\tag{2.65} \\end{eqnarray}\\] Para \\(\\forall i \\neq j\\) y \\(i, j = 1, 2, \\ldots, p\\). Cuando \\(g_1 = g_2 = \\ldots = g_p = g\\), la condición de la ecuación (2.64) se resume a que \\(|g| &lt; 1\\). En resumen, las condiciones descritas en las ecuaciones (2.64) y (2.65) se puden ilustrar mediante un circulo unitario como el de la Figura 2.9 en que sí las raíces se ubican dentro de éste, podemos decir que el proceso es convergente en el largo plazo. # Crear datos para el círculo theta &lt;- seq(0, 2*pi, length.out=100) x &lt;- cos(theta) y &lt;- sin(theta) circle_data &lt;- data.frame(x, y) # Dibujar el círculo ggplot(circle_data, aes(x=x, y=y)) + geom_polygon(fill=NA, color=&quot;black&quot;) + coord_fixed(ratio=1) + xlim(-1.5, 1.5) + ylim(-1.5, 1.5) Figure 2.9: Circulo unitario en el que se cumple que \\(|g_i|&lt;1\\) y \\((g_i g_j)^{1/2} = (u^2 + v^2)^{1/2} &lt; 1\\) 2.4 Operador de rezago L Denotemos, como se ha mencionado con anterioridad, con \\(L\\) al operador de rezago, el cual nos permitirá construir una relación entre diferencias y medias móviles como se verá más adelante en los procesos univariados \\(AR(p)\\), \\(MA(q)\\) y, en general, \\(ARIMA(p, d, q)\\). Sean \\(X\\), \\(Y\\) o \\(Z\\) variables con las que denotaremos a una serie de tiempo (note que hasta el momento no hemos definido qué es una serie de tiempo, no obstante, no es necesario definirla para hacer uso del operador). En esta sección resumiremos algunas propiedades usadas en el capítulo y en capítulos más adelante. Así, si a dicha serie le aplicamos el operador rezago antes definido, el resultado deberá ser que cada uno de los valores de la serie es retardado o regresado un período. Es decir: \\[\\begin{equation} L Z_t = Z_{t-1} \\tag{2.66} \\end{equation}\\] De esta forma, si aplicamos el operador rezago \\(L\\) a la nueva serie de tiempo dada por \\(Z_{t-1}\\) podemos obtener \\(Z_{t-2}\\). Haciendo uso de la ecuación (2.66) podemos obtener: \\[\\begin{equation} L Z_{t-1} = L(L Z_t) = L^2 Z_t = Z_{t-2} \\tag{2.67} \\end{equation}\\] Mediante una generalización podemos obtener: \\[\\begin{equation} L^k Z_t = Z_{t-k} \\tag{2.68} \\end{equation}\\] Para \\(k = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Así, para \\(k = 0\\) obtenemos la identidad dado que \\(L^0 Z_t = Z_t\\), de tal forma que siempre asumiremos que \\(L^0 = 1\\). En otro caso, cuando \\(k &gt; 0\\) a la serie de tiempo a la cual se le aplique el operador rezago \\(L\\) se le deberá aplicar un rezago de \\(k\\) periodos a cada uno de los elementos de la serie. Por el contrario, cuando \\(k &lt; 0\\) el operador rezago significa que se deberá adelantar \\(|k|\\) veces a cada elemento de la serie. Por ejemplo, \\(L^{-3} Z_t = Z_{t+3}\\). Las reglas descritas en lo subsecuente se mantienen indistintamente cuando aplican para el caso de rezagar como para cuando se adelanta una serie. Como primera propiedad tomemos a la siguiente propiedad: \\[\\begin{equation} L^{m} Z_{t-n} = L^{m} (L^{n} Z_{t}) = L^{m + n} Z_{t} = Z_{t-(n + m)} \\tag{2.69} \\end{equation}\\] De lo anterior, podemos inferir el siguiente resultado: \\[\\begin{equation} \\Delta Z_{t} = Z_{t} - Z_{t-1} = (1 - L) Z_{t} \\tag{2.70} \\end{equation}\\] En el caso de la diferencia de órden cuatro o cuarta diferencia se puede expresar como: \\[\\begin{equation} \\Delta_{4} Z_{t} = Z_{t} - Z_{t-4} = (1 - L^4) Z_{t} \\tag{2.71} \\end{equation}\\] Al respecto, vale la pena aclarar que en ocasiones se hará uso de una notación alternativa dada por: \\(\\Delta^k\\) o \\(\\Delta_k\\), donde \\(k = 1, 2, 3, \\ldots\\), indistintamente, ya que en ambos casos se referirá a una diferencia de orden \\(k\\). Esta notación resulta de gran utilidad cuando se quiere comparar períodos equivalentes como, por ejemplo, el mismo trimestre pero de un año anterior. De forma similar, para el caso de logaritmos podemos escribir la ecuación (2.71) como: \\[\\begin{equation} \\Delta^{4} ln(Z_{t}) = \\Delta_{4} ln(Z_{t}) = ln(Z_{t}) - ln(Z_{t-4}) = (1 - L^4) ln(Z_{t}) \\tag{2.72} \\end{equation}\\] Para el caso de una serie de tiempo que se le ha transformado mediante medias móviles, digamos de \\(4\\) periodos, podemos escribirla como: \\[\\begin{equation} Zs_{t} = \\frac{1}{4}(Z_{t} + Z_{t-1} + Z_{t-2} + Z_{t-3}) = \\frac{1}{4}(1 + L + L^2 + L^3)Z_{t} \\tag{2.73} \\end{equation}\\] Una generalización del anterior caso puede ser escrita como un polinomio de orden \\(p\\) con el operador rezago \\(L\\) dado como: \\[\\begin{eqnarray} \\alpha(L) Z_{t} &amp; = &amp; (1 - \\alpha_1 L - \\alpha_2 L^2 - \\ldots - \\alpha_p L^p) Z_{t} \\nonumber \\\\ &amp; = &amp; Z_{t} - \\alpha_1 Z_{t-1} - \\alpha_2 Z_{t-2} - \\ldots - \\alpha_p Z_{t-p} \\tag{2.74} \\end{eqnarray}\\] Donde \\(\\alpha_i\\) puede ser reemplazada por cualquier constante \\(a_i\\), con \\(i = 1, 2, 3, \\ldots\\), para escribir ecuaciones como las anteriores. Adicionalmente, podemos decir que la ecuación (2.74) es una generalización del caso de medias móviles, el cual admite una ponderación distinta para cada uno de los elementos rezagados. Existe la posibilidad de operar más de un polinomio a la vez. Para múltiples polinomios (digamos, los polinomios \\(\\alpha(L)\\) y \\(\\beta(L)\\)) podemos escribir el siguiente resultado: \\[\\begin{equation} \\alpha(L) \\beta(L) = \\beta(L) \\alpha(L) \\tag{2.75} \\end{equation}\\] Tales polinomios del operador rezago también son llamados filtros lineales. A manera de ejemplo, tomemos el siguiente caso de diferencias para una serie de \\(Z_t\\): \\[\\begin{equation} \\Delta Z_{t} = (1 - L) Z_{t} = Z_{t} - Z_{t-1} \\tag{2.76} \\end{equation}\\] y un proceso de medias móviles para la misma serie de \\(Z_t\\): \\[\\begin{equation} Zs_{t} = \\frac{1}{4}(1 + L^1 + L^2 + L^3) Z_{t} = \\frac{1}{4}(Z_{t} + Z_{t-1} + Z_{t-2} + Z_{t-3}) \\tag{2.77} \\end{equation}\\] De tal forma que el producto de ambos procesos se puede escribir como: \\[\\begin{equation} (1 - L) \\times \\frac{1}{4}(1 + L^1 + L^2 + L^3) Z_{t} = \\frac{1}{4}(1 - L^4) Z_{t} \\tag{2.78} \\end{equation}\\] Es decir, que el producto de dos polinomios, uno de diferencias y otro más de medias móviles, resulta en uno de diferencias pero de mayor grado, en este caso de grado 4. "],["procesos-estacionarios-y-modelos-univariados.html", "Chapter 3 Procesos Estacionarios y Modelos Univariados 3.1 Definición de ergodicidad y estacionariedad 3.2 Función de autocorrelación 3.3 Procesos estacionarios univariados 3.4 Procesos Autoregresivos (AR) 3.5 Procesos de Medias Móviles (MA) 3.6 Procesos ARMA(p, q) y ARIMA(p, d, q) 3.7 Función de Autocorrelación Parcial 3.8 Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q) 3.9 Pronósticos 3.10 Desestacionalización y filtrado de Series 3.11 Motivación 3.12 Filtro Hodrick-Prescott", " Chapter 3 Procesos Estacionarios y Modelos Univariados 3.1 Definición de ergodicidad y estacionariedad A partir de esta sección introduciremos mayor formalidad matemática al análisis de las series de tiempo. Por ello cambiaremos un poco la notación y ocuparemos a \\(X_t\\) en lugar de \\(Z_t\\) como objeto de nuestro análisis. Con \\(X_t\\) denotaremos a una serie de tiempo, ya que con \\(Z_t\\) denotaremos a una variable, sin que ella fuera necesariamente una serie de tiempo en los términos que a continuación discutimos. Asimismo, iniciaremos por establecer una serie de definiciones. De esta forma, definiremos a una serie de tiempo como un vector de variables aleatorias de dimensión \\(T\\), dado como: \\[\\begin{equation} X_1, X_2, X_3, \\ldots ,X_T \\tag{3.1} \\end{equation}\\] Cada una de las \\(X_t\\) (\\(t = 1, 2, \\ldots, T\\)) consideradas como una variable aleatoria. Así, también podemos denotar a la serie de tiempo como: \\[\\begin{equation} \\{ X_t \\}^T_{t = 1} \\tag{3.2} \\end{equation}\\] Es decir, definiremos a una serie de tiempo como una realización de un proceso estocástico –o un Proceso Generador de Datos (PGD). Consideremos una muestra de los múltiples posibles resultados de muestras de tamaño \\(T\\), la colección dada por: \\[\\begin{equation} \\{X^{(1)}_1, X^{(1)}_2, \\ldots, X^{(1)}_T\\} \\tag{3.3} \\end{equation}\\] Digamos que la ecuación (3.3) es una de las tantas posibles resultantes del proceso estocástico o PGD. Eventualmente podríamos estar dispuestos a observar este proceso indefinidamente, de forma tal que estemos interesados en la secuencia dada por \\(\\{ X^{(1)}_t \\}^{\\infty}_{t = 1}\\), lo cual no dejaría de ser sólo una de las tantas realizaciones o secuencias del proceso estocástico original. Tan solo por poner un ejemplo, podríamos observar las siguientes realizaciones del mismo PGD: \\[\\begin{eqnarray*} &amp; \\{X^{(2)}_1, X^{(2)}_2, \\ldots, X^{(2)}_T\\} &amp; \\\\ &amp; \\{X^{(3)}_1, X^{(3)}_2, \\ldots, X^{(3)}_T\\} &amp; \\\\ &amp; \\{X^{(4)}_1, X^{(4)}_2, \\ldots, X^{(4)}_T\\} &amp; \\\\ &amp; \\vdots &amp; \\\\ &amp; \\{X^{(j)}_1, X^{(j)}_2, \\ldots, X^{(j)}_T\\} &amp; \\end{eqnarray*}\\] Donde \\(j \\in \\mathbb{Z}\\). En lo subsecuente, diremos que una serie de tiempo es una realización del proceso estocástico subyacente. Considerando, en consecuencia, al proceso estocástico con todas sus posibilidades de realización. Para hacer más sencilla la notación no distinguiremos entre el proceso en sí mismo y una de sus realizaciones, es decir, siempre escribiremos a una serie de tiempo como la secuencia mostrada en la ecuación (3.2), o más precisamente como la siguiente realización: \\[\\begin{equation} \\{ X_1, X_2, \\ldots, X_T \\} \\tag{3.4} \\end{equation}\\] O simplemente: \\[\\begin{equation} X_1, X_2, \\ldots, X_T \\tag{3.5} \\end{equation}\\] El proceso estocástico de dimensión \\(T\\) puede ser completamente descrito por su función de distribución multivariada de dimensión \\(T\\). No obstante, esto no resulta ser práctico cuando se opere más adelante en el curso. Por ello, en el curso y, en general, en casi todos los textos lo hacen, sólo nos enfocaremos en sus primero y segundo momentos. Es decir, en sus media o valor esperado: \\[\\begin{equation*} \\mathbb{E}[X_t] \\end{equation*}\\] Para \\(t = 1, 2, \\ldots, T\\); o: \\[\\begin{equation*} \\left[ \\begin{array}{c} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_T] \\end{array} \\right] \\end{equation*}\\] o, \\[\\begin{equation*} \\left[ \\begin{array}{c} \\mathbb{E}[X_1], \\mathbb{E}[X_2], \\ldots, \\mathbb{E}[X_T] \\end{array} \\right] \\end{equation*}\\] Y en su variaza: \\[\\begin{equation*} Var[X_t] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^2] \\end{equation*}\\] Para \\(t = 1, 2, \\ldots, T\\), y de sus \\(T(T-1)/2\\) covarianzas: \\[\\begin{equation*} Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])(X_s - \\mathbb{E}[X_s])] \\end{equation*}\\] Para \\(t &lt; s\\). Por lo tanto, en la forma matricial podemos escribir lo siguiente: \\[\\begin{equation*} \\left[ \\begin{array}{c c c c} Var[X_1] &amp; Cov[X_1,X_2] &amp; \\cdots &amp; Cov[X_1,X_T] \\\\ Cov[X_2,X_1] &amp; Var[X_2] &amp; \\cdots &amp; Cov[X_2,X_T] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov[X_T,X_1] &amp; Cov[X_T,X_2] &amp; \\cdots &amp; Var[X_T] \\\\ \\end{array} \\right] \\end{equation*}\\] \\[\\begin{equation} = \\left[ \\begin{array}{c c c c} \\sigma_1^2 &amp; \\rho_{12} &amp; \\cdots &amp; \\rho_{1T} \\\\ \\rho_{21} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\rho_{2T} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{T1} &amp; \\rho_{T2} &amp; \\cdots &amp; \\sigma_T^2 \\\\ \\end{array} \\right] \\tag{3.6} \\end{equation}\\] Donde es claro que en la matriz de la ecuación (3.6) existen \\(T(T-1)/2\\) covarianzas distintas, ya que se cumple que \\(Cov[X_t,X_s] = Cov[X_s,X_t]\\), para \\(t \\neq s\\). A menudo, esas covarianzas son denominadas como autocovarianzas puesto que ellas son covarianzas entre variables aleatorias pertenecientes al mismo proceso estocástico pero en un momento \\(t\\) diferente. Si el proceso estocástico tiene una distribución normal multivariada, su función de distribución estará totalmente descrita por sus momentos de primer y segundo orden. Ahora introduciremos el concepto de ergodicidad, el cual indica que los momentos muestrales, los cuales son calculados en la base de una serie de tiempo con un número finito de observaciones, en la medida que \\(T \\rightarrow \\infty\\) sus correspondientes momentos muestrales, tienden a los verdaderos valores poblacionales, los cuales definiremos como \\(\\mu\\), para la media, y \\(\\sigma^2_X\\) para la varianza. Este concepto sólo es cierto si asumimos que, por ejemplo, el valor esperado y la varianza son como se dice a continuación para todo \\(t = 1, 2, \\ldots, T\\): \\[\\begin{eqnarray} \\mathbb{E}[X_t] = \\mu_t = \\mu \\\\ \\tag{3.7} \\end{eqnarray}\\] \\[\\begin{eqnarray} Var[X_t] = \\sigma^2_X \\tag{3.8} \\end{eqnarray}\\] Mas formalmente, se dice que el PGD o el proceso estocástico es ergódico en la media si: \\[\\begin{equation} \\displaystyle\\lim_{T \\to \\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) \\right) ^2 \\right]} = 0 \\tag{3.9} \\end{equation}\\] y ergódico en la varianza si: \\[\\begin{equation} \\displaystyle\\lim_{T \\to \\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) ^2 - \\sigma^2_X \\right) ^2 \\right]} = 0 \\tag{3.10} \\end{equation}\\] Estas condiciones se les conoce como propiedades de consistencia para las variables aleatorias. Sin embargo, éstas no pueden ser probadas. Por ello se les denomina como un supuesto que pueden cumplir algunas de las series. Más importante aún: un proceso estocástico que tiende a estar en equilibrio estadístico en un orden ergódico, es estacionario. Podemos distinguir dos tipos de estacionariedad. Si asumimos que la función común de distribución del proceso estocástico no cambia a lo largo del tiempo, se dice que el proceso es estrictamente estacionario. Como este concepto es difícil de aplicar en la práctica, solo consideraremos a la estacionariedad débil o estacionariedad en sus momentos. Definiremos a la estacionariedad por sus momentos del correspondiente proceso estocástico dado por \\(\\{X_t\\}\\): Estacionariedad en media: Un proceso estocástico es estacionario en media si \\(E[X_t] = \\mu_t = \\mu\\) es constante para todo \\(t\\). Estacionariedad en varianza: Un proceso estocástico es estacionario en varianza si \\(Var[X_t] = \\mathbb{E}[(X_t - \\mu_t)^2] = \\sigma^2_X = \\gamma(0)\\) es constante y finita para todo \\(t\\). Estacionariedad en covarianza: Un proceso estocástico es estacionario en covarianza si \\(Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] = \\gamma(|s-t|)\\) es sólo una función del tiempo y de la distancia entre las dos variables aleatorias. Por lo que no depende del tiempo denotado por \\(t\\) (no depende de la información contemporánea). Estacionariedad débil: Como la estacionariedad en varianza resulta de forma inmediata de la estacionariedad en covarianza cuando se asume que \\(s = t\\), un proceso estocástico es débilmente estacionario cuando es estacionario en media y covarianza. Puesto que resulta poco factible asumir una estacionariedad diferente a la débil, es adelante siempre que digamos que un proceso es estacionario se referirá al caso débil y sólo diremos que el proceso es estacionario, sin el apelativo de débil. . Supongamos una serie de tiempo denotada por: \\(\\{U_t\\}^T_{t = 0}\\). Decimos que el proceso estocástico \\(\\{U_t\\}\\) es un proceso estocástico puramente aleatorio o es un proceso estocástico de ruido blanco o caminata aleatoria, si este tiene las siguientes propiedades: \\(\\mathbb{E}[U_t] = 0\\), \\(\\forall t\\); \\(Var[U_t] = \\mathbb{E}[(U_t - \\mu_t)^2] = \\mathbb{E}[(U_t - \\mu)^2] = \\mathbb{E}[(U_t)^2] = \\sigma^2\\), \\(\\forall t\\), y \\(Cov[U_t,U_s] = \\mathbb{E}[(U_t - \\mu_t)(U_s - \\mu_s)] = \\mathbb{E}[(U_t - \\mu)(U_s - \\mu)] = \\mathbb{E}[U_t U_s] = 0\\), \\(\\forall t \\neq s\\). En otras palabras, un proceso \\(U_t\\) es un ruido blanco si su valor esperado (promedio) es cero (0), tiene una varianza finita y constante, y además no le importa la historia pasada. Así, su valor presente no se ve influenciado por sus valores pasados, no importando respecto de qué período se tome referencia. En apariencia, por sus propiedades, este proceso es débilmente estacionario –o simplemente, estacionario–. Todas las variables aleatorias tienen una media de cero, una varianza \\(\\sigma^2\\) y no existe correlación entre ellas. Ahora, supongamos que definimos un nuevo proceso estocástico \\(\\{X_t\\}\\) como: \\[\\begin{equation} X_t = \\left\\{ \\begin{array}{l} U_0 \\mbox{ para } t = 0 \\\\ X_{t-1} + U_t \\mbox{ para } t = 1, 2, 3, \\ldots \\end{array}\\right. \\tag{3.11} \\end{equation}\\] Donde \\(\\{ U_t \\}\\) es un proceso puramente aleatorio. Este proceso estocástico, o caminata aleatoria sin tendencia (ajuste - drift), puede ser reescrito como: \\[\\begin{equation} X_t = \\sum^t_{j = 0} U_j \\tag{3.12} \\end{equation}\\] Tratemos de dar más claridad al ejemplo, para ello asumamos que generamos a \\(\\{U_t\\}\\) por medio del lanzamiento de una moneda. Donde obtenemos una cara (águila) con una probabilidad de \\(0.5\\), en cuyo caso decimos que la variable aleatoria \\(U_t\\) tomará el valor de \\(+1\\), y una cruz (sol) con una probabilidad de \\(0.5\\), en cuyo caso decimos que la variable aleatoria \\(U_t\\) toma el valor de \\(-1\\). Este planteamiento cumple con las propiedades enunciadas ya que: \\(\\mathbb{E}[U_t] = 0.5 \\times -1 + 0.5 \\times 1 = 0\\), \\(\\forall t\\) \\(Var[U_t] = \\mathbb{E}[(U_t - 0)^2] = \\frac{1}{2}((-1)^2) + \\frac{1}{2}((1)^2) = 1\\), \\(\\forall t\\) \\(Cov[U_t,U_s] = \\mathbb{E}[(U_t - 0)(U_s - 0)] = \\mathbb{E}[U_t \\cdot U_s] = 0\\), \\(\\forall t \\neq s\\). Retomando a nuestro proceso \\(X_t\\), diremos que el caso de \\(X_0 = 0\\), para \\(t = 0\\). Si verificamos cuáles son sus primeros y segundos momentos de \\(\\{X_t\\}\\), tenemos: \\[\\begin{equation} \\mathbb{E}[X_t] = \\mathbb{E}\\left[ \\sum^t_{j=1} U_j \\right] = \\sum^t_{j=1} \\mathbb{E}[U_j] = 0 \\tag{3.13} \\end{equation}\\] En cuanto a la varianza: \\[\\begin{eqnarray} Var[X_t] &amp; = &amp; Var \\left[ \\sum^t_{j=1} U_j \\right] \\nonumber \\\\ &amp; = &amp; \\sum^t_{j=1} Var[U_j] + 2 * \\sum_{j \\neq k} Cov[U_j,U_k] \\nonumber \\\\ &amp; = &amp; \\sum^t_{j=1} 1 \\nonumber \\\\ &amp; = &amp; t \\tag{3.14} \\end{eqnarray}\\] Lo anterior, dado que hemos supuesto que en la caminata aleatoria todas las variables aleatorias son independientes, es decir, \\(Cov[U_t,U_s] = E[U_t \\cdot U_s] = 0\\). Por su parte, la covarianza del proceso estocástico se puede ver como: \\[\\begin{eqnarray*} Cov[X_t,X_s] &amp; = &amp; \\mathbb{E} \\left[ \\left( \\sum^t_{j=1} U_j - 0 \\right) \\left( \\sum^s_{i=1} U_i - 0 \\right) \\right] \\\\ &amp; = &amp; \\mathbb{E}[(U_1 + U_2 + \\ldots + U_t)(U_1 + U_2 + \\ldots + U_s)] \\\\ &amp; = &amp; \\sum^t_{j=1} \\sum^s_{i=1} \\mathbb{E}[U_j U_i] \\\\ &amp; = &amp; \\mathbb{E}[U^2_1] + \\mathbb{E}[U^2_2] + \\ldots + \\mathbb{E}[U^2_k] \\\\ &amp; = &amp; \\sigma^2 + \\sigma^2 + \\ldots + \\sigma^2 \\\\ &amp; = &amp; 1 + 1 + 1 + 1 \\\\ &amp; = &amp; min(t,s) \\end{eqnarray*}\\] Así, el proceso estocástico dado por la caminata alaeatoria sin un término de ajuste es estacionario en media, pero no en varianza o en covarianza, y consecuentemente, en general no estacionario, condición que contraria al caso del proceso simple descrito en \\(U_t\\). Es fácil ver que muchas de las posibilidades de realización de este proceso estocástico (series de tiempo) pueden tomar cualquiera de las rutas consideradas en la Figura 3.1. set.seed(1234) # Utilizaremos una función guardada en un archivo a parte # Llamamos a la función: source(&quot;Caminata.R&quot;) # Definimos argumentos de la función Opciones &lt;- c(-1, 1) # Soporte &lt;- 10000 # Vamos a réplicar el proceso con estos parámetros Rango &lt;- 200 # Caminos &lt;- 10 # for(i in 1:Caminos){ TT &lt;- data.matrix(data.frame(Caminata(Opciones, Soporte)[1])) # G_t &lt;- data.matrix(data.frame(Caminata(Opciones, Soporte)[2])) # plot(TT, G_t, col = &quot;blue&quot;, type = &quot;l&quot;, ylab = &quot;Ganancias&quot;, xlab = &quot;Tiempo&quot;, ylim = c(-Rango,Rango)) # par(new = TRUE) # i &lt;- i +1 } # par(new = FALSE) Figure 3.1: Ejemplo de 10 trayectorias de la caminata aleatoria, cuando sólo es posible cambios de +1 y -1 3.2 Función de autocorrelación Para ampliar la discusión, es posible calcular la fuerza o intensidad de la dependencia de las variables aleatorias dentro de un proceso estocástico, ello mediante el uso de las autocovarianzas. Cuando las covarianzas son normalizadas respecto de la varianza, el resultado es un término que es independiente de las unidades de medida aplicadas, y se conoce como la función de autocorrelación. Para procesos estacionarios, dicha función de autocorrelación esta dada por: \\[\\begin{equation} \\rho(\\tau) = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t+\\tau} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\frac{\\gamma(\\tau)}{\\gamma(0)} \\tag{3.15} \\end{equation}\\] Donde \\(\\tau = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Dicha función tiene las siguientes propiedades: \\(\\rho(0) = 1\\). Es fácil demostrar que la función \\(\\rho(0)\\) es: \\[\\begin{equation} \\rho(0) = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t + 0} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\frac{\\mathbb{E}[(X_t - \\mu)^2]}{\\mathbb{E}[(X_t - \\mu)^2]} = 1 \\end{equation}\\] \\(\\rho(\\tau) = \\rho(-\\tau)\\). Partiendo de la definción de \\(\\rho(\\tau)\\) podemos ver que la distancia que existe entre \\(t\\) y \\(t + \\tau\\) es \\(\\tau\\), de esta forma la autocorrelación de la variable \\(X\\) entre los periodos antes señalados debería ser la misma para el caso en que \\(\\rho(-\\tau)\\). Partamos de la ecuación para ver más claramente: \\[\\begin{equation} \\rho(\\tau) = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t + \\tau} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t - \\tau} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\rho(-\\tau) \\end{equation}\\] \\(\\lvert\\rho(\\tau)\\lvert \\leq 1\\), para todo \\(\\tau\\). Derivado de las propiedades 1 y 2 antes descritas se puede concluir que sólo es necesario conocer la función de autocorrelación para el caso de \\(\\tau = 1, 2, 3, \\ldots\\), ya que de estos casos podemos derivar los valores de la función de autocorrelación complementarios de \\(\\tau = \\ldots, -3, -2, -1\\). Partiendo de los supuestos de ergodicidad en relación a la media, varianza y covarianzas de un proceso estacionario, podemos estimar dichos parámetros con las siguientes formulaciones o propuestas de estimadores puntuales: \\[\\begin{equation} \\hat{\\mu} = \\frac{1}{T} \\sum^T_{t=1} X_t \\tag{3.16} \\end{equation}\\] \\[\\begin{equation} \\hat{\\gamma}(0) = \\frac{1}{T} \\sum^T_{t=1} (X_t - \\hat{\\mu})^2 = \\hat{\\sigma}^2 \\tag{3.17} \\end{equation}\\] \\[\\begin{equation} \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum^{T - \\tau}_{t=1} (X_t - \\hat{\\mu})(X_{t+\\tau} - \\hat{\\mu}) \\mbox{, para } \\tau = 1, 2, \\ldots, T-1 \\tag{3.18} \\end{equation}\\] No hacemos la demostración en estas notas –sería deseable que el alumno revisara la siguiente afirmación–, pero estos últimos son estimadores consistentes de \\(\\mu\\), \\(\\gamma(0)\\) y \\(\\gamma(\\tau)\\). Por su parte, un estimador consistente de la función de autocorrelación estará dado por: \\[\\begin{equation} \\hat{\\rho}(\\tau) = \\frac{\\sum^{T - \\tau}_{t=1} (X_t - \\hat{\\mu})(X_{t+\\tau} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(\\tau)}{\\hat{\\gamma}(0)} \\tag{3.19} \\end{equation}\\] El estimador de la ecuación (3.19) es asintóticamente insesgado. Por ejemplo, para el caso de un proceso de ruido blanco o caminata aleatoria, su varianza puede ser aproximada por el valor dado \\(1/T\\). Ésta tiene, asintóticamente, una distribución normal. Dado esto, el intervalo de confianza al \\(95\\%\\) será el dado por \\(\\pm 2/\\sqrt{T}\\), en el cual se encuentra la mayoría de los coeficientes de autocorrelación estimados. Ahora discutamos algunos ejemplos o aplicaciones. Cuando se realiza la evaluación de la estimación de un modelo de series de tiempo es importante saber si los residuales del modelo realmente tienen propiedades de un proceso puramente aleatorio, en particular, si ellos no están correlacionados entre sí. Así, la hipótesis a probar será: \\[\\begin{equation} H_0 : \\rho(\\tau) = 0 \\mbox{, para todo } \\tau = 1, 2, \\ldots, m \\mbox{ y } m &lt; T \\tag{3.20} \\end{equation}\\] Esta expresión se puede interpretar como una prueba respecto de si la correlación entre la información de periodos atrás es cero con la información contemporánea. Para hacer una pruena global de la hipotésis de sí un número \\(m\\) de coeficientes de autocovarianzas son cero Box y Pierce (1970) desarrollarón la siguiente estadística: \\[\\begin{equation} Q^* = T \\sum_{j = 1}^{m} \\hat{\\rho} (j)^2 \\tag{3.21} \\end{equation}\\] Bajo la hipotésis nula esta estadística se distribulle asintóticamente como una chi cuadrado (\\(\\chi^2\\)) con \\(m-k\\) grados de libertad y con \\(k\\) que representa al número de paramétros estimados. Haciendo una aplicación estricta de la distribución de esta estadística, sabemos que esta se mantiene asintóticamente. Greta, Ljung y Box (1978) propusieron la siguiente modificación de la estadística para muestras pequeñas: \\[\\begin{equation} Q = T(T + 2) \\sum_{j = 1}^{m} \\frac{\\hat{\\rho} (j)^2}{T - j} \\tag{3.22} \\end{equation}\\] La cual también se distribulle asintóticamente como \\(\\chi^2\\) con \\(m-k\\) grados de libertad. También es intuitivamente claro que la hipótesis nula de no autocorrelación de residuales debería ser rechazada si alguno de los valores \\(\\hat{\\rho} (j)\\) es muy grande, es decir, si \\(Q\\) o \\(Q^*\\) es muy grande. O más precisamente, si estas estadísticas son más grandes que los correspondientes valores críticos de la distribución \\(\\chi^2\\) con \\(m-k\\) grados de libertad a algún grado dado de signficancia. Una alternativa para esta prueba es una del tipo Multiplicadores de Lagrange (o LM) desarrollada por Breusch (1978) y Godfrey (1978). La cual, al igual que las estadísticas \\(Q\\) y \\(Q^*\\), la hipotesis nula está dada por: \\(H_0\\): Los residuales no están autocorrelacionados. \\(H_a\\): Los residuales muestran alguna acutocorrelación de forma autoregresiva o de medias móviles. La prueba consiste en realizar una regresión auxiliar en la cual los residuales se estiman en función de las variables explicativas del modelo original y en los residuales mismos pero rezagados hasta el término \\(m\\) (regresión auxiliar). La prueba resulta en una estadística con una distribución \\(\\chi^2\\) con \\(m\\) grados de libertad la cual está dada por la expresión: \\[\\begin{equation} LM = T \\times R^2 \\tag{3.23} \\end{equation}\\] Donde \\(R^2\\) es el resultante de la regresión auxiliar y \\(T\\) es el número de observaciones totales. En comparación con una prueba Durbin - Watson que es comúnmente usada en la econometría tradicional, para probar autocorrelación de los residuales, las estadísticas \\(Q\\), \\(Q^*\\) y \\(LM\\) tienen las siguientes ventajas: Permiten corroborar la existencia de autocorrelación para cualquier orden, y no solo para un primer orden (es decir, para cualquier valor de \\(\\tau = 1, 2, 3, \\ldots\\)); Los resultados se mantienen aún y cuando exista una probable variable endógena en forma rezagada, y No depende del orden o la forma en que se acomoden las observaciones, algo que es muy probalble que ocurra en la econometría tradicional. El hecho de que los residuales no estén autocorrelacionados no implica que estos sean independientes y normalmente distribuidos. La ausencia de autocorrelación no implica una independencia estocástica si las variables son normalmente distribuidas. A menudo se asume que estos residuales están distribuidos normalmente, ya que la mayoría de las pruebas estadísticas tienen este supuesto detrás. No obstante, ello también depende de los otros momentos de la distribución, específicamente del tercer y cuarto momento. Los cuales se expresan como: \\[\\begin{equation*} \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^i] \\mbox{, } i = 3, 4 \\end{equation*}\\] El tercer momento es necesario para determinar el sesgo, el cual está dado como: \\[\\begin{equation} \\hat{S} = \\frac{1}{T} \\frac{\\sum_{t = 1}^{T} (X_t - \\hat{\\mu})^3}{\\sqrt{\\hat{\\gamma}(0)^3}} \\tag{3.24} \\end{equation}\\] Para distribuciones simétricas (como en el caso de la distribución normal) el valor teórico para el sesgo es cero. La curtosis, la cual está dada en función del cuarto momento, se puede expresar como: \\[\\begin{equation} \\hat{K} = \\frac{1}{T} \\frac{\\sum_{t = 1}^{T} (X_t - \\hat{\\mu})^4}{\\hat{\\gamma}(0)^2} \\tag{3.25} \\end{equation}\\] Para el caso de una distribución normal, esta estadística toma el valor de 3. Valores más grandes que 3 indican que la distribución tiene colas anchas. En tales casos se ubican los datos financieros. Usando el valor de las estadísticas para medir el sesgo y la curtosis, \\(S\\) y \\(K\\), respectivamente, Jarque y Bera (1980) propusieron una prueba de normalidad, la cual puede ser aplicada a series de tiempo en niveles o en diferencias indistintamente. Dicha prueba se expresa como: \\[\\begin{equation} JB = \\frac{T}{6} \\left(\\hat{S} + \\frac{1}{4} (\\hat{K} - 3)^2 \\right) \\tag{3.26} \\end{equation}\\] La cual tiene una distribución \\(\\chi^2\\) con \\(2\\) grados de libertad y donde \\(T\\) es el tamaño de la muestra. La hipótesis de que las observaciones están distribuidas de forma normal se rechaza si el valor de la estadística de prueba es más grande que los correspondientes valores críticos en tablas. . Veamos un ejemplo para ilustrar el uso de la función de autocorrelación. Tomemos como variable al número de pasajeros transportados por el sistema de transporte del metro de la CDMX.1 Los datos empleados fueron tomados del INEGI y son una serie de tiempo en el período que va de enero de 2000 a mayo de 2023, es decir, 281 observaciones. Como se puede apreciar en la Figura 3.2, el número de pasajeros por mes ha oscilado significativamente a lo largo del tiempo. Incluso podemos observar un cambio estructural de la serie entre 2011 y 2012. Asimismo, podemos ubicar una caída atípica que ocurrió en septiembre de 2017. Pero lo más relevante es la caída asociada a la pandemia de COVID-19 de 2020. library(ggplot2) library(dplyr) library(readxl) Datos &lt;- read_excel(&quot;BD/Base_Transporte.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) ggplot(data = Datos, aes(x = Periodo, y = Pax_Metro)) + geom_line(linewidth = 0.5, color = &quot;darkblue&quot;) + #geom_point(size = 1.0, color = &quot;darkblue&quot;) + #theme_bw() + xlab(&quot;Tiempo&quot;) + ylab(&quot;Millones de pasajeros&quot;) + theme(plot.title = element_text(size = 11, face = &quot;bold&quot;, hjust = 0)) + theme(plot.subtitle = element_text(size = 10, hjust = 0)) + theme(plot.caption = element_text(size = 10, hjust = 0)) + theme(plot.margin = unit(c(1,1,1,1), &quot;cm&quot;)) + labs( title = &quot;Pasajeros Transportados en el Metro de la CDMX&quot;, subtitle = &quot;(Ene-2000 a Jul-2021)&quot;, caption = &quot;Fuente: Elaboración propia con información del INEGI, \\nhttps://www.inegi.org.mx/app/indicadores/?tm=0&amp;t=1090&quot; ) Figure 3.2: Evolución del número de pasajeros en el Metro de la CDMX, enero 2000 a mayo 2023 # ggsave(&quot;Pax_Metro.png&quot;, width = 20, height = 15, units = &quot;cm&quot;) A esta serie de tiempo le calculamos los pincipales estadísticos hasta ahora estudiados y obtenemos el Cuadro 3.1. En dicho cuadro se destaca que se muestra la función de autocirrelación para los tres primeros rezagos. Para mayor detalle, en la Figura 3.3 se muestra la función de autocorrelación, en donde las bandas descritas por las líneas azules son el intervalo de confianza dentro de las cuales no se puede rechazar la hipotésis nula de que \\(H_0: \\hat{\\rho}(p) = 0\\), para todo \\(\\tau = 1, 2, \\ldots, T-1\\). Table 3.1: Estadísticas descriptivas del número de pasajeros en el Metro de la CDMX, enero de 2000 a junio de 2019 Estadística Valor \\(\\hat{\\mu} = \\frac{1}{T} \\sum^T_{t=1} X_t\\) 124.3000 \\(\\hat{\\gamma}(0) = \\frac{1}{T} \\sum^T_{t=1} (X_t - \\hat{\\mu})^2\\) 103.6400 \\(\\hat{\\gamma}(1) = \\frac{1}{T} \\sum^{T - 1}_{t=1} (X_t - \\hat{\\mu})(X_{t+1} - \\hat{\\mu})\\) 63.1100 \\(\\hat{\\gamma}(2) = \\frac{1}{T} \\sum^{T - 2}_{t=1} (X_t - \\hat{\\mu})(X_{t+2} - \\hat{\\mu})\\) 72.9100 \\(\\hat{\\gamma}(3) = \\frac{1}{T} \\sum^{T - 3}_{t=1} (X_t - \\hat{\\mu})(X_{t+3} - \\hat{\\mu})\\) 63.6900 \\(\\hat{\\rho}(1) = \\frac{\\sum^{T - 1}_{t=1} (X_t - \\hat{\\mu})(X_{t+1} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}\\) 0.6089 \\(\\hat{\\rho}(2) = \\frac{\\sum^{T - 2}_{t=1} (X_t - \\hat{\\mu})(X_{t+2} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(2)}{\\hat{\\gamma}(0)}\\) 0.7035 \\(\\hat{\\rho}(3) = \\frac{\\sum^{T - 3}_{t=1} (X_t - \\hat{\\mu})(X_{t+3} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(3)}{\\hat{\\gamma}(0)}\\) 0.6145 \\(Q^* = T \\sum_{j = 1}^{1} \\hat{\\rho} (j)^2\\) 86.7577 \\(Q^* = T \\sum_{j = 1}^{2} \\hat{\\rho} (j)^2\\) 290.9279 Pax_Metro &lt;- ts(Datos$Pax_Metro, start = 2000, freq = 12) acf(Pax_Metro, lag.max = 150, xlab = &#39;Resagos k en meses&#39;, main = &quot;Funcion de Autocorrelación del número de pasajeros del metro&quot;) Figure 3.3: Función de Autocorrelación: 150 rezagos del número de pasajeros en el Metro de la CDMX, enero de 2000 a mayo de 2023 3.3 Procesos estacionarios univariados En este capítulo analizaremos el método o metodología de análisis de series de tiempo propuesto por Box y Jenkins (1970). Los modelos propuestos dentro de esta metodología o conjunto de métodos se han vuelto indispensables para efectos de realizar pronósticos de corto plazo. En este sentido, se analizarán los métodos más importantes en series de tiempo: procesos autoregresivos (AR, por sus siglas en inglés) y procesos de medias móviles (MA, por sus siglas en inglés). Asimismo, se realizará un análisis de los procesos que resultan de la combinación de ambos, conocida como ARMA, los cuales son más comúnmente usados para realizar pronósticos. 3.4 Procesos Autoregresivos (AR) Los procesos autoregresivos tienen su origen en el trabajo de Cochrane y Orcutt de 1949, mediante el cual analizaron los residuales de una regresión clásica como un proceso autoregresivo. Puede consultarse el apéndice para la discusión del modelo de regresión clásica. 3.4.1 AR(1) Como primer caso analizaremos al proceso autoregresivo de primer orden, \\(AR(1)\\), el cual podemos definir como una Ecuación Lineal en Diferencia de Primer Orden Estocástica. Diremos que una Ecuación Lineal en Diferencia de Primer Orden es estocástica si en su representación analítica considera un componente estocástico como en la ecuación (3.27) descrita a continuación: \\[\\begin{equation} X_t = a_0 + a_1 X_{t-1} + U_t \\tag{3.27} \\end{equation}\\] Donde \\(a_0\\) es un término constante, \\(U_t\\) es un proceso estacionario, con media cero (0), una varianza finita y constante (\\(\\sigma^2\\)) y una covarianza que depende de la distancia entre \\(t\\) y cualquier \\(t-s\\) (\\(\\gamma_s\\))–que no depende de los valores pasados o futuros de la variable–, \\(X_0\\) es el valor inicial del proceso \\(X_t\\). No obstante, en ocasiones vamos a asumir que la covarianza será cero (0), por lo que en esos casos tendremos un proceso puramente aleatorio. Considerando la ecuación (3.27) y un proceso de sustitución sucesivo podemos establecer lo siguiente, empezando con \\(X_1\\): \\[\\begin{eqnarray*} X_{1} &amp; = &amp; a_0 + a_1 X_{0} + U_{1} \\end{eqnarray*}\\] Para \\(X_2\\): \\[\\begin{eqnarray*} X_{2} &amp; = &amp; a_0 + a_1 X_{1} + U_{2} \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_1 X_{0} + U_{1}) + U_{2} \\\\ &amp; = &amp; a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2} \\end{eqnarray*}\\] Para \\(X_3\\): \\[\\begin{eqnarray*} X_{3} &amp; = &amp; a_0 + \\alpha X_{2} + U_{3} \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}) + U_{3} \\\\ &amp; = &amp; a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 X_{0} + a_1^2 U_{1} + a_1 U_{2} + U_{3} \\end{eqnarray*}\\] Así, para cualquier \\(X_t\\), \\(t = 1, 2, 3, \\ldots\\), obtendríamos: \\[\\begin{eqnarray} X_{t} &amp; = &amp; a_0 + a_1 X_{t - 1} + U_{t} \\nonumber \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 a_0 + \\ldots + a_1^{t-2} a_0 + a_1^{t-1} X_{0} \\nonumber \\\\ &amp; &amp; + a_1^{t-2} U_{1} + \\ldots + a_1 U_{t - 2} + U_{t - 1}) + U_{t} \\nonumber \\\\ &amp; = &amp; a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 a_0 + \\ldots + a_1^{t-1} a_0 + a_1^{t} X_{0} \\nonumber \\\\ &amp; &amp; + a_1^{t-1} U_{1} + \\ldots a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t} \\nonumber \\\\ &amp; = &amp; (1 + a_1 + a_1^2 + a_1^3 + \\ldots + a_1^{t-1}) a_0 + a_1^{t} X_{0} \\nonumber \\\\ &amp; &amp; + a_1^{t-1} U_{1} + \\ldots + a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t} \\nonumber\\\\ &amp; = &amp; \\frac{1 - a_1^t}{1 - a_1} a_0 + a_1^{t} X_{0} + \\sum^{t-1}_{j = 0} a_1^{j} U_{t - j} \\tag{3.28} \\end{eqnarray}\\] De esta forma en la ecuación (3.28) observamos un proceso que es explicado por dos partes: una que depende del tiempo y otra que depende de un proceso estocástico. Asimismo, debe notarse que la condición de convergencia es idéntica al caso de ecuaciones en diferencia estudiadas al inicio del curso: $ |a_1| &lt; 1$, por lo que cuando \\(t \\to \\infty\\), la expresión (3.28) será la siguiente: \\[\\begin{equation} X_t = a_0 \\frac{1}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^{j} U_{t - j} \\tag{3.29} \\end{equation}\\] Así, desaparece la parte dependiente del tiempo y únicamente prevalece la parte que es dependiente del proceso estocástico. Esta es la solución de largo plazo del proceso \\(AR(1)\\), la cual depende del proceso estocástico. Notemos, además, que esta solución implica que la variable o la serie de tiempo \\(X_t\\) es también un proceso estocástico que hereda las propiedades de \\(U_t\\). Así, \\(X_t\\) es también un proceso estocástico estacionario, como demostraremos más adelante. Observemos que la ecuación (3.29) se puede reescribir si consideramos la formulación que en la literatura se denomina como la descomposición de Wold, en la cual se define que es posible asumir que \\(\\psi_j = a_1^j\\) y se considera el caso en el cual $ |a_1| &lt; 1 $. De esta forma tendremos que, por ejemplo, cuando: \\[\\begin{equation*} \\sum^{\\infty}_{j = 0} \\psi^2_j = \\sum^{\\infty}_{j = 0} a_1^{2j} = \\frac{1}{1 - a_1^2} \\end{equation*}\\] Alternativamente y de forma similar a las ecuaciones en diferencia estudiadas previamente, podemos escribir el proceso \\(AR(1)\\) mediante el uso del operador rezago como: \\[\\begin{eqnarray} X_t &amp; = &amp; a_0 + a_1 L X_t + U_t \\nonumber \\\\ X_t - a_1 L X_t &amp; = &amp; a_0 + U_t \\nonumber \\\\ (1 - a_1 L) X_t &amp; = &amp; a_0 + U_t \\nonumber \\\\ X_t &amp; = &amp; \\frac{a_0}{1 - a_1 L} + \\frac{1}{1 - a_1 L} U_t \\tag{3.30} \\end{eqnarray}\\] En esta última ecuación retomamos el siguiente término para reescribirlo como: \\[\\begin{equation} \\frac{1}{1 - a_1 L} = 1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots \\end{equation}\\] Tomando este resultado para sustituirlo en la ecuación (3.30), obtenemos la siguiente expresión: \\[\\begin{eqnarray} X_t &amp; = &amp; (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots) a_0 + (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots) U_t \\nonumber \\\\ &amp; = &amp; (1 + a_1 + a_1^2 + a_1^3 + \\ldots) a_0 + U_t + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \\ldots \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} \\tag{3.31} \\end{eqnarray}\\] Donde la condición de convergencia y estabilidad del proceso descrito en esta ecuación es que $ |a_1| &lt; 1 $. Por lo que hemos demostrado que, mediante el uso del operador de rezago, es posible llegar al mismo resultado que obtuvimos mediante el procedimiento de sustituciones iterativas. La ecuación (3.31) se puede interpretar como sigue. La solución o trayectoria de equilibrio de un AR(1) se divide en dos partes. La primera es una constante que depende de los valores de \\(a_0\\) y \\(a_1\\). La segunda parte es la suma ponderada de las desviaciones o errores observados y acumulados en el tiempo hasta el momento \\(t\\). Ahora obtendremos los momentos que describen a la serie de tiempo cuando se trata de un proceso \\(AR(1)\\). Para ello debemos obtener la media, la varianza y las covarianzas de \\(X_t\\). Para los siguientes resultados debemos recordar y tener en mente que si \\(U_t\\) es un proceso puramente aleatorio, entonces: \\(\\mathbb{E}[U_t] = 0\\) para todo \\(t\\) \\(Var[U_t] = \\sigma^2\\) para todo \\(t\\) \\(Cov[U_t, U_s] = 0\\) para todo \\(t \\neq s\\) Dicho lo anterior y partiendo de la ecuación (3.31), el primer momento o valor esperado de la serie de tiempo será el siguiente: \\[\\begin{eqnarray} \\mathbb{E}[X_t] &amp; = &amp; \\mathbb{E} \\left[ \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} \\right] \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j \\mathbb{E}[U_{t-j}] \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1} = \\mu \\tag{3.32} \\end{eqnarray}\\] Respecto de la varianza podemos escribir la siguiente expresión a partir de la ecuación (3.31): \\[\\begin{eqnarray} Var[X_t] &amp; = &amp; \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\ &amp; = &amp; \\mathbb{E} \\left[ \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} - \\frac{a_0}{1 - a_1} \\right)^2 \\right] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[(U_{t} + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \\ldots)^2] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U^2_{t} + a_1^2 U^2_{t-1} + a_1^4 U^2_{t-2} + a_1^6 U^2_{t-3} + \\ldots \\nonumber \\\\ &amp; &amp; + 2 a_1 U_t U_{t-1} + 2 a_1^2 U_t U_{t-2} + \\ldots] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U^2_{t}] + a_1^2 \\mathbb{E}[U^2_{t-1}] + a_1^4 \\mathbb{E}[U^2_{t-2}] + a_1^6 \\mathbb{E}[U^2_{t-3}] + \\ldots \\nonumber \\\\ &amp; = &amp; \\sigma^2 + a_1^2 \\sigma^2 + a_1^4 \\sigma^2 + a_1^6 \\sigma^2 + \\ldots \\nonumber \\\\ &amp; = &amp; \\sigma^2 (1 + a_1^2 + a_1^4 + a_1^6 + \\ldots) \\nonumber \\\\ &amp; = &amp; \\sigma^2 \\frac{1}{1 - a_1^2} = \\gamma(0) \\tag{3.33} \\end{eqnarray}\\] Previo a analizar la covarianza de la serie de tiempo, recordemos que para el proceso puramente aleatorio \\(U_t\\) su varianza y covarianza puede verse como \\(\\mathbb{E}[U_t, U_s] = \\sigma^2\\), para \\(t = s\\), y \\(\\mathbb{E}[U_t, U_s] = 0\\), para cualquier otro caso, respectivamente. Dicho lo anterior, partiendo de la ecuación (3.31) la covarianza de la serie estará dada por: \\[\\begin{eqnarray} Cov(X_t, X_{t-\\tau}) &amp; = &amp; \\mathbb{E}[(X_t - \\mu)(X_{t-\\tau} - \\mu)] \\nonumber \\\\ &amp; = &amp; \\mathbb{E} \\left[ \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} - \\frac{a_0}{1 - a_1} \\right) \\right. \\nonumber \\\\ &amp; &amp; \\left. \\times \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-\\tau-j} - \\frac{a_0}{1 - a_1} \\right) \\right] \\nonumber \\\\ &amp; = &amp; a_1^{\\tau} \\mathbb{E}[U^2_{t-\\tau} + a_1 U^2_{t-\\tau-1} + a_1^2 U^2_{t-\\tau-2} + a_1^3 U^2_{t-\\tau-3} + \\ldots] \\nonumber \\\\ &amp; = &amp; a_1^{\\tau} \\sigma^2 \\frac{1}{1 - a_1^2} = \\gamma(\\tau) \\tag{3.34} \\end{eqnarray}\\] Notése que con estos resultados en las ecuaciones (3.33) y (3.34) podemos construir la función de autocorrelación teórica como sigue: \\[\\begin{eqnarray} \\rho(\\tau) &amp; = &amp; \\frac{\\gamma(\\tau)}{\\gamma(0)} \\nonumber \\\\ &amp; = &amp; a_1^\\tau \\end{eqnarray}\\] Donde \\(\\tau = 1, 2, 3, \\ldots\\) y $ |a_1| &lt; 1 $. Este último resultado significa que cuando el proceso autoregresivo es de orden 1 (es decir, AR(1)) la función de autocorrelación teóricamente es igual al parámetro \\(a_1\\) elevado al número de rezagos considerados. No obstante, note que esto no significa que la autocorrelación observada sea como lo expresa en el planteamiento anterior. Por el contrario, una observación sencilla mostraría que la autocorrelación observada sería ligeramente distinta a la autocorrelación teórica. Ahora veamos algunos ejemplos. . En el primer ejemplo simularemos una serie y mostraremos el análisis de un proceso construido considerando un proceso puramente aleatorio como componente \\(U_t\\). Por su parte, en un segundo ejemplo aplicaremos el análisis a una serie de tiempo de una variable económica observada. Para el primer ejemplo, consideremos un proceso dado por la forma de un \\(AR(1)\\) como en la ecuación (3.30) cuya solución está dada por la ecuación (3.31). En específico, supongamos que el término o componente estocástico \\(U_t\\) es una serie generada a partir de números aleatorios de una función normal con media \\(0\\) y desviación estándar \\(4\\). Los detalles del proceso simulado se muestran en las siguientes gráficas. La Figura 3.4 ilustra el comportamiento que se debería observar en una serie considerando el procedimiento iterativo de construcción. Por su parte, la Figura 3.5 ilustra el proceso o trayectoria de la solución de la serie de tiempo. Finalmente, las Figuras 3.6 y 3.7 muestran el correlograma calculado considerando una función de autocorrelación aplicada al porceso real y una función de autocorrelación aplicada al proceso teórico, respectivamente. library(ggplot2) library(dplyr) library(latex2exp) a0 &lt;- 5; a1 &lt;- 0.9; X_0 &lt;- (a0/(1 - a1)); T &lt;- 1000 X_t &lt;- data.frame(Tiempo = c(0:T)) set.seed(12345) # Agregamos un término estocástico al data frame X_t$U_t &lt;- rnorm(T+1, mean = 0, sd = 4) # Agregamos columnas con NA&#39;s para un proceso teorico y uno real X_t$X_t &lt;- NA X_t$XR_t &lt;- NA # La serie teórica inicia en un valor inicial X_0 X_t$X_t[1] &lt;- X_0 # La serie real inicia en un valor inicial X_0 X_t$XR_t[1] &lt;- X_0 # Agregamos una columna para la función de Autocorrelación teórica: X_t$rho &lt;-NA for (i in 2:(T + 1)) { # Real: X_t$XR_t[i] = a0 + a1*X_t$XR_t[i-1] + X_t$U_t[i-1] # Teórico: X_t$X_t[i] = X_t$X_t[i-1] + (a1^(i-1))*X_t$U_t[i-1] # Autocorrelación: X_t$rho[i-1] = a1^(i-1) } ggplot(data = X_t, aes(x = Tiempo, y = XR_t)) + geom_line(size = 0.5, color = &quot;darkred&quot;) + #theme_bw() + xlab(&quot;Tiempo&quot;) + ylab(TeX(&quot;$X_t$&quot;)) + theme(plot.title = element_text(size = 11, face = &quot;bold&quot;, hjust = 0)) + theme(plot.subtitle = element_text(size = 10, hjust = 0)) + theme(plot.caption = element_text(size = 10, hjust = 0)) + theme(plot.margin = unit(c(1,1,1,1), &quot;cm&quot;)) + labs( title = &quot;Comportamiento del Proceso Real (&#39;Estimado&#39;)&quot;, subtitle = &quot;Con un error con Distribución Normal (media = 0, desviación estándar = 4)&quot;, caption = &quot;Fuente: Elaboración propia.&quot; ) Figure 3.4: Comportamiento del Proceso Real (‘Estimado’) ggsave(&quot;G_AR_1_Real.png&quot;, width = 20, height = 10, units = &quot;cm&quot;) ggplot(data = X_t, aes(x = Tiempo, y = X_t)) + geom_line(size = 0.5, color = &quot;darkblue&quot;) + #theme_bw() + xlab(&quot;Tiempo&quot;) + ylab(TeX(&quot;$X_t$&quot;)) + theme(plot.title = element_text(size = 11, face = &quot;bold&quot;, hjust = 0)) + theme(plot.subtitle = element_text(size = 10, hjust = 0)) + theme(plot.caption = element_text(size = 10, hjust = 0)) + theme(plot.margin = unit(c(1,1,1,1), &quot;cm&quot;)) + labs( title = &quot;Comportamiento del Proceso Teórico&quot;, subtitle = &quot;Con un error con Distribución Normal (media = 0, desviación estándar = 4)&quot;, caption = &quot;Fuente: Elaboración propia.&quot; ) Figure 3.5: Comportamiento del Proceso Teórico ggsave(&quot;G_AR_1_Teo.png&quot;, width = 20, height = 10, units = &quot;cm&quot;) acf(X_t$XR_t, lag.max = 30, col = &quot;blue&quot;, ylab = &quot;Autocorrelacion&quot;, xlab=&quot;Rezagos&quot;, main=&quot;Funcion de Autocorrelacion Real&quot;) Figure 3.6: Funcion de Autocorrelacion Real barplot(X_t$rho[1:30], names.arg = c(1:30), col = &quot;blue&quot;, border=&quot;blue&quot;, density = c(10,20), ylab = &quot;Autocorrelacion&quot;, xlab=&quot;Rezagos&quot;, main=&quot;Funcion de Autocorrelacion Teórica&quot;) Figure 3.7: Funcion de Autocorrelacion Teórica Recordemos que una trayectoria de equilibrio o solución de un \\(AR(1)\\) es como se muestra en la ecuación (3.31). Así, nuestra serie simulada cumple con la característica de que los errores son más relevantes cuando la serie es corta. Por el contrario, los errores son menos relevantes, cuando la serie es muy larga. La Figura 3.8 ilustra esta observación de la trayectoria de equilibrio. ggplot(data = X_t, aes(x = Tiempo)) + geom_line(aes(y = XR_t), size = 0.5, color = &quot;darkred&quot;) + geom_line(aes(y = X_t), size = 0.5, color = &quot;darkblue&quot;) + #theme_bw() + xlab(&quot;Tiempo&quot;) + ylab(TeX(&quot;$X_t$&quot;)) + theme(plot.title = element_text(size = 11, face = &quot;bold&quot;, hjust = 0)) + theme(plot.subtitle = element_text(size = 10, hjust = 0)) + theme(plot.caption = element_text(size = 10, hjust = 0)) + theme(plot.margin = unit(c(1,1,1,1), &quot;cm&quot;)) + labs( title = &quot;Comportamiento de los Procesos Real y Teórico&quot;, subtitle = &quot;Con un error con Distribución Normal (media = 0, desviación estándar = 4)&quot;, caption = &quot;Fuente: Elaboración propia.&quot; ) Figure 3.8: Comportamiento de los Procesos Real y Teórico ggsave(&quot;G_AR_1_Comb.png&quot;, width = 20, height = 10, units = &quot;cm&quot;) . Para el segundo ejemplo consideremos una aplicación a una serie de tiempo en especifico: Pasajeros transportados mensualmente en el Sistema de Transporte Colectivo Metro (pasajeros medidos en millones). A la serie se le aplicará una metodología de estimación dada por el método de Máxima Verosimilitud (ML, por sus siglas en inglés). Antes de realizar el proceso de estimación, consideremos una transformación de diferencias logarítmicas con el objeto de obtener una serie de tiempo expresada en tasas de crecimiento y con un comportamiento parecido a un proceso estacionario. Así, para cada una de las series que analicemos en diferencias logarítmicas respecto del momento \\(k\\) las expresaremos bajo la siguiente transformación: \\[\\begin{equation*} DLX_t = log(X_t) - log(X_{t-k}) \\end{equation*}\\] Donde \\(k = 1, 2, 3, \\ldots\\) y \\(log(.)\\) es la función logaritmo natural. Esta expresión se puede interpretar como una tasa de crecimiento, puesto que asumimos variaciones pequeñas para las cuales se cumple que: \\(log(X_t) - log(X_{t-k}) \\approx \\frac{X_t - X_{t-k}}{X_t}\\). Primero, para realizar el análisis de una serie de tiempo deberemos decidir si éste se realizará para la serie en niveles o en diferencias. Por convención, decimos que la serie está en niveles si ésta se analiza sin hacerle ninguna transformación o si se analiza aplicando solo logaritmos. Cuando la serie se analiza en diferencias significa que la diferencia se hace sin aplicar logaritmos o aplicando logaritmos. Sin embargo, la convención es hacer un análisis en diferencias logarítmicas. Para decidir cómo analizar la serie de pasajeros en el metro de la CDMX en la Figura 3.9 se muestra la gráfica de la serie en niveles (sin transformación logaritmica y con transformación logarítmica) y en diferencias logarítmicas mensuales (es decir, con \\(k = 1\\)). library(ggplot2) library(dplyr) library(readxl) library(stats) Datos &lt;- read_excel(&quot;BD/Base_Transporte.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) # En Niveles Pax_Metro &lt;- ts(Datos$Pax_Metro, start = c(2000, 1), freq = 12) # En Logaritmos: Pax_LMetro &lt;- ts(log(Datos$Pax_Metro), start = c(2000, 1), freq = 12) # Diferencias mensuales: Pax_DLMetro &lt;- ts( log(Datos$Pax_Metro) - lag( log(Datos$Pax_Metro), 1 ), start = c(2000, 1), freq = 12) # par(mfrow = c(3,1)) plot(Pax_Metro, xlab = &quot;Tiempo&quot;, main = &quot;Pasajeros transportados (Millones) en el SCM&quot;, col = &quot;darkgreen&quot;) plot(Pax_LMetro, xlab = &quot;Tiempo&quot;, main = &quot;LN Pasajeros transportados (Millones) en el SCM&quot;, col = &quot;darkblue&quot;) plot(Pax_DLMetro, xlab = &quot;Tiempo&quot;, main = &quot;Diff LN Pasajeros transportados (Millones) en el SCM&quot;, col = &quot;darkred&quot;) Figure 3.9: Pasajeros transportados (Millones) en el metro de la CDM en niveles y en diferencias logaritmicas par(mfrow=c(1,1)) A continuación, estimaremos una \\(AR(1)\\) para la serie en niveles bajo la transformación logarítmica (\\(PaxLMetro_t\\)) y en diferencias logarítmicas (\\(PaxDLMetro_t\\)). Para el primer caso, obtenemos el siguiente resultado: Table 3.2: AR(1) para la variable \\(PaxLMetro_t\\). \\(PaxLMetro_t\\) \\(=\\) \\(4.7419\\) \\(+\\) \\(0.5916\\) \\(PaxLMetro_{t-1}\\) \\((0.0105)\\) \\((0.0526)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.004335\\) \\(AIC\\) \\(=\\) \\(-602.73\\) Para el segundo caso, obtenemos el siguiente resultado: Table 3.3: AR(1) para la variable \\(PaxDLMetro_t\\). \\(PaxDLMetro_t\\) \\(=\\) \\(0.0007\\) \\(-\\) \\(0.6194\\) \\(PaxDLMetro_{t-1}\\) \\((0.0023)\\) \\((0.0511)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.003344\\) \\(AIC\\) \\(=\\) \\(-660.53\\) En ambos casos observamos que el parámetro asociado al componente AR es significativo y cumple con la restricción de ser en valor absoluto menor a 1, por lo que la solución asociada al proceso será convergente. También en ambos casos se reporta la estadística o Criterio de Información de Akaike (AIC, por sus siglas en inglés), misma que más adelante discutiremos su importancia y aplicación. 3.4.2 AR(2) Una vez analizado el caso de \\(AR(1)\\) analizaremos el caso del \\(AR(2)\\). La ecuación generalizada del proceso autoregresivo de orden 2 (denotado como \\(AR(2)\\)) puede ser escrita como: \\[\\begin{equation} X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + U_t \\tag{3.35} \\end{equation}\\] Donde \\(U_t\\) denota un proceso puramente aleatorio con media cero (\\(0\\)), varianza constante (\\(\\sigma^2\\)) y autocovarianza cero (\\(Cov(U_t, U_s) = 0\\), con \\(t \\neq s\\)), y un parámetro \\(a_2 \\neq 0\\). Así, utilizando el operador rezago podemos reescribir la ecuación (3.35) como: \\[\\begin{eqnarray*} X_t - a_1 X_{t-1} - a_2 X_{t-2} &amp; = &amp; a_0 + U_t \\\\ (1 - a_1 L^1 - a_2 L^2) X_t &amp; = &amp; a_0 + U_t \\end{eqnarray*}\\] Donde, vamos a denotar a \\(\\alpha (L) = (1 - a_1 L^1 - a_2 L^2)\\), y lo denotaremos como un polinomio que depende del operador rezago y que es distinto de cero. De esta forma podemos reescribir la ecuación (3.35) como: \\[\\begin{equation} \\alpha(L) X_t = a_0 + U_t \\end{equation}\\] Ahora, supongamos que existe el inverso multiplicativo del polinomio \\(\\alpha(L)\\), el cual será denotado como: \\(\\alpha^{-1}(L)\\) y cumple con que: \\[\\begin{equation} \\alpha^{-1}(L) \\alpha(L) = 1 \\end{equation}\\] Así, podemos escribir la solución a la ecuación (3.35) como: \\[\\begin{equation*} X_t = \\alpha^{-1}(L) \\delta + \\alpha^{-1}(L) U_t \\end{equation*}\\] Si utilizamos el hecho que \\(\\alpha^{-1}(L)\\) se puede descomponer a través del procedimiento de Wold en un polinomio de forma similar al caso de \\(AR(1)\\), tenemos que: \\[\\begin{equation} \\alpha^{-1}(L) = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\ldots \\end{equation}\\] Por lo tanto, el inverso multiplicativo \\(\\alpha^{-1}(L)\\) se puede ver como: \\[\\begin{equation} 1 = (1 - a_1 L^1 - a_2 L^2) (\\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\ldots) \\tag{3.36} \\end{equation}\\] Desarrollando la ecuación (3.36) tenemos la siguiente expresión: \\[\\begin{eqnarray*} 1 &amp; = &amp; \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots \\\\ &amp; &amp; - a_1 \\psi_0 L - a_1 \\psi_1 L^2 - a_1 \\psi_2 L^3 - \\ldots \\\\ &amp; &amp; - a_2 \\psi_0 L^2 - a_2 \\psi_1 L^3 - \\ldots \\end{eqnarray*}\\] Ahora, podemos agrupar todos los términos en función del exponente asociado al operador rezago \\(L\\). La siguiente es una solución partícular y es una de las múltiples que podrían existir que cumpla con la ecuación (3.36). Sin embargo, para efectos del análisis, sólo necesitamos una de esas soluciones. Utilizaremos las siguientes condiciones que deben cumplirse en una de las posibles soluciones: \\[\\begin{eqnarray*} L^0 &amp; : &amp; \\Rightarrow \\psi_0 = 1 \\\\ L^1 &amp; : &amp; \\psi_1 - a_1 \\psi_0 = 0 \\Rightarrow \\psi_1 = a_1 \\\\ L^2 &amp; : &amp; \\psi_2 - a_1 \\psi_1 - a_2 \\psi_0 = 0 \\Rightarrow \\psi_2 = a^2_1 + a_2 \\\\ L^3 &amp; : &amp; \\psi_3 - a_1 \\psi_2 - a_2 \\psi_1 = 0 \\Rightarrow \\psi_3 = a^3_1 + 2 a_1 a_2 \\\\ &amp; \\vdots &amp; \\end{eqnarray*}\\] De esta forma podemos observar que en el límite siempre obtendremos una ecuación del tipo \\(\\psi_j - a_1 \\psi_{j-1} - a_2 \\psi_{j-2} = 0\\) asociada a cada uno de los casos en que exista un \\(L^j\\), donde \\(j \\neq 0, 1\\), y la cual siempre podremos resolver conociendo que las condiciones iniciales son: \\(\\psi_0 = 1\\) y \\(\\psi_1 = a_1\\). Así, de las relaciones antes mencionadas y considerando que \\(\\alpha^{-1} (L)\\) aplicada a una constante como \\(a_0\\), tendrá como resultado otra constante. De esta forma podemos escribir que la solución del proceso AR(2) en la ecuación (3.35) será dada por una expresión como sigue: \\[\\begin{equation} X_t = \\frac{a_0}{1 - a_1 - a_2} + \\sum^{\\infty}_{j = 0} \\psi_{t - j} U_{t - j} \\tag{3.37} \\end{equation}\\] Donde todos los parámetros \\(\\psi_i\\) están determinados por los parámetros \\(a_0\\), \\(a_1\\) y \\(a_2\\). En particular, \\(\\psi_0 = 1\\) y \\(\\psi_1 = a_1\\), como describimos anteriormente. Al igual que en el caso del \\(AR(1)\\), en la ecuación (3.37) las condiciones de estabilidad estarán dadas por las soluciones del siguiente polinomio característico: \\[\\begin{equation} \\lambda^2 - \\lambda a_1 - a_2 = 0 \\end{equation}\\] Así, la condición de estabilidad de la trayectoria es que $ | _i | &lt; 1 $, para \\(i = 1, 2\\). Es decir, es necesario que cada una de las raíces sea, en valor absoluto, siempre menor que la unidad. Estas son las condiciones de estabilidad para el proceso \\(AR(2)\\). Finalmente, al igual que en un \\(AR(1)\\), a continuación determinamos los momentos de una serie que sigue un proceso \\(AR(2)\\). Iniciamos con la determinación de la media de la serie: \\[\\begin{equation} \\mathbb{E}[X_t] = \\mu = \\frac{a_0}{1 - a_1 - a_2} \\end{equation}\\] Lo anterior es cierto puesto que \\(\\mathbb{E}[U_{t - i}] = 0\\), para todo \\(i = 0, 1, 2, \\ldots\\). Para determinar la varianza utilizaremos las siguientes relaciones basadas en el uso del valor esperado, varianza y covarianza de la serie. Adicionalmente, para simplificar el trabajo asumamos que \\(a_0 = 0\\), lo cual implica que \\(\\mu = 0\\). Dicho lo anterior, partamos de: \\[\\begin{eqnarray*} \\mathbb{E}[X_t X_{t - \\tau}] &amp; = &amp; \\mathbb{E}[(a_1 X_{t-1} + a_2 X_{t-2} + U_t) X_{t - \\tau}]\\\\ &amp; = &amp; a_1 \\mathbb{E}[X_{t - 1} X_{t - \\tau}] + a_2 \\mathbb{E}[X_{t - 2} X_{t - \\tau}] + \\mathbb{E}[U_{t} X_{t - \\tau}] \\end{eqnarray*}\\] Donde \\(\\tau = 0, 1, 2, 3, \\ldots\\) y \\(\\mathbb{E}[U_{t} X_{t - \\tau}] = 0\\) para todo \\(\\tau \\neq 0\\). Dicho esto, podemos derivar el valor del valor esperado para diferentes valores de \\(\\tau\\): \\[\\begin{eqnarray*} \\tau = 0 &amp; : &amp; \\gamma(0) = \\alpha_1 \\gamma(1) + \\alpha_2 \\gamma(2) + \\sigma^2 \\\\ \\tau = 1 &amp; : &amp; \\gamma(1) = \\alpha_1 \\gamma(0) + \\alpha_2 \\gamma(1) \\\\ \\tau = 2 &amp; : &amp; \\gamma(2) = \\alpha_1 \\gamma(1) + \\alpha_2 \\gamma(0) \\\\ &amp; \\vdots &amp; \\end{eqnarray*}\\] Donde debe ser claro que \\(\\mathbb{E}[(X_{t} - \\mu)(X_{t - \\tau} - \\mu)] = \\mathbb{E}[X_{t} X_{t - \\tau}] = \\gamma(\\tau)\\). Así, en general cuando \\(\\tau \\neq 0\\): \\[\\begin{equation} \\gamma(\\tau) = a_1 \\gamma(\\tau - 1) + a_2 \\gamma(\\tau - 2) \\end{equation}\\] Realizando la sustitución recursiva y solucionando el sistema respectivo obtenemos que la varianza y las covarianzas estarán determinadas por: \\[\\begin{equation} Var[X_t] = \\gamma(0) = \\frac{1 - a_2}{(1 + a_2)[(1 - a_2)^2 - a^2_1]} \\sigma^2 \\end{equation}\\] \\[\\begin{equation} \\gamma(1) = \\frac{a_1}{(1 + a_2)[(1 - a_2)^2 - a^2_1]} \\sigma^2 \\end{equation}\\] \\[\\begin{equation} \\gamma(2) = \\frac{a^2_1 + a_2 - a^2_2}{(1 + a_2)[(1 - a_2)^2 - a^2_1]} \\sigma^2 \\end{equation}\\] Recordemos que las funciones de autocorrelación se obtienen de la división de cada una de las funciones de covarianza (\\(\\gamma(\\tau)\\)) por la varianza (\\(\\gamma(0)\\)). Así, podemos construir la siguiente expresión: \\[\\begin{equation} \\rho(\\tau) - a_1 \\rho(\\tau - 1) - a_2 \\rho(\\tau - 2) = 0 \\end{equation}\\] . Utilizaremos la serie de Pasajeros en vuelos nacionales (en vuelos de salidas) para estimar un \\(AR(2)\\) mediante el método de máxima verosimilitud (ML, por sus siglas en inglés). Antes de realizar el proceso de estimación, consideremos una transformación de la serie en logaritmos y una más en diferencias logarítmicas; lo anterior con el objeto de obtener una serie de tiempo suavizada y expresada en tasas de crecimiento, con un comportamiento parecido a un proceso estacionario. Así, para cada una de las series que analicemos en diferencias logarítmicas, las expresaremos bajo la siguiente transformación: \\[\\begin{equation*} DLX_t = log(X_t) - log(X_{t-k}) \\end{equation*}\\] Donde \\(k = 1, 2, 3, \\ldots\\) y \\(log(.)\\) es la función logaritmo natural. Por convención, decimos que la serie está en niveles si esta se analiza sin hacerle ninguna transformación o se analiza en logaritmos. Cuando la serie se analiza en diferencias significa que la diferencia se hace sin aplicar logaritmos. Y cuando la serie analizada está en diferencias logarítmicas también diremos que está en diferencias. Sin embargo, lo común es hacer un análisis en logaritmos y en diferencias logarítmicas. Primero, para decidir si se realizará un AR(2) para la serie en niveles o en diferencias, analizaremos su gráfica. La serie en niveles, en niveles bajo una transformación logarítmica y en diferencias logarítmicas mensuales de los pasajeros en vuelos nacionales se muestra en la Figura 3.10. library(ggplot2) library(dplyr) library(readxl) library(stats) Datos &lt;- read_excel(&quot;BD/Base_Transporte.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) # En Niveles Pax_Nal &lt;- ts(Datos$Pax_Nal, start = c(2000, 1), freq = 12) # Logaritmos: LPax_Nal &lt;- ts(log(Datos$Pax_Nal), start = c(2000, 1), freq = 12) # Diferencias mensuales: DLPax_Nal &lt;- ts(log(Datos$Pax_Nal) - lag(log(Datos$Pax_Nal), 1), start = c(2000, 1), freq = 12) # par(mfrow=c(3,1)) plot(Pax_Nal, xlab = &quot;Tiempo&quot;, ylab = &quot;Pasajeros&quot;, main = &quot;Pasajeros en vuelos nacionales de salida&quot;, col = &quot;darkgreen&quot;) plot(LPax_Nal, xlab = &quot;Tiempo&quot;, ylab = &quot;LN Pasajeros&quot;, main = &quot;LN Pasajeros en vuelos nacionales de salida&quot;, col = &quot;darkblue&quot;) plot(DLPax_Nal, xlab = &quot;Tiempo&quot;, ylab = &quot;DLN Pasajeros&quot;, main = &quot;Diff LN Pasajeros en vuelos nacionales de salida&quot;, col = &quot;darkred&quot;) Figure 3.10: Pasajeros en vuelos de salidas nacionales en niveles y en diferencias logaritmicas par(mfrow=c(1,1)) A continuación, estimaremos un \\(AR(2)\\) para la serie en niveles bajo una transformación logarítmica (\\(LPaxNal_t\\)) y en diferencias logaritmitcas (\\(DLPax_Nal_t\\)). Para el primer caso obtenemos el siguiente resultado: Table 3.4: AR(2) para la variable \\(LPaxNal_t\\). \\(LPaxNal_t\\) \\(=\\) \\(14.6267\\) \\(+\\) \\(0.7637\\) \\(LPaxNal_{t-1}\\) \\((0.1816)\\) \\((0.0637)\\) \\(0.2025\\) \\(LPaxNal_{t-2}\\) \\((0.0646)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.01138\\) \\(AIC\\) \\(=\\) \\(-372.64\\) Para el segundo caso obtenemos el siguiente resultado: Table 3.5: AR(2) para la variable \\(DLPaxNal_t\\). \\(DLPaxNal_t\\) \\(=\\) \\(0.0050\\) \\(+\\) \\(0.3205\\) \\(DLPaxNal_{t-1}\\) \\((0.0036)\\) \\((0.0592)\\) \\(0.4242\\) \\(DLPaxNal_{t-2}\\) \\((0.0591)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.009378\\) \\(AIC\\) \\(=\\) \\(-418.3\\) Para ambos casos, entre paréntesis indicamos los errores estándar y reportamos el estadístico de Akaike, AIC. Finalmente, podemos determinar si las soluciones serán convergentes para ello en la Figura 3.11 mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual, podemos concluir que ambas propuestas de AR(2) representan una solución convergente y estable. source(&quot;arroots.R&quot;) source(&quot;plot.armaroots.R&quot;) AR_LPax_Nal &lt;- arima(LPax_Nal, order = c(2, 0, 0), method = &quot;ML&quot;) AR_DLPax_Nal &lt;- arima(DLPax_Nal, order = c(2, 0, 0), method = &quot;ML&quot;) par(mfrow=c(1,2)) plot.armaroots(arroots(AR_LPax_Nal), main=&quot;Inverse AR roots of \\nAR(2): LN Pax Nal&quot;) # plot.armaroots(arroots(AR_DLPax_Nal), main=&quot;Inverse AR roots of \\nAR(2): Diff LN Pax Nal&quot;) Figure 3.11: Inveso de las Raíces del polinomio característico par(mfrow=c(1,1)) 3.4.3 AR(p) Veremos ahora una generalización de los procesos autoregresivos (AR). Esta generalización es conocida como un proceso \\(AR(p)\\) y que puede ser descrito por la siguiente ecuación en diferencia estocástica: \\[\\begin{equation} X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \\ldots + a_p X_{t-p} + U_t \\tag{3.38} \\end{equation}\\] Donde \\(a_p \\neq 0\\), y \\(U_t\\) es un proceso puramente aleatorio con media cero (0), varianza constante (\\(\\sigma^2\\)) y covarianza cero (0). Usando el operador rezago, \\(L^k\\), para \\(k = 0, 1, 2, \\ldots, p\\), obtenemos la siguiente expresión de la ecuación (3.38): \\[\\begin{equation} (1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p) X_t = a_0 + U_t \\end{equation}\\] Definamos el polinomio \\(\\alpha(L)\\) como: \\[\\begin{equation} \\alpha(L) = 1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p \\tag{3.39} \\end{equation}\\] De forma similar que en los procesos \\(AR(1)\\) y \\(AR(2)\\), las condiciones de estabilidad del proceso \\(AR(p)\\) estarán dadas por la solución de la ecuación característica: \\[\\begin{equation} \\lambda^p - a_1 \\lambda^{p-1} - a_2 \\lambda^{p-2} - a_3 \\lambda^{p-3} - \\ldots - a_p = 0 \\end{equation}\\] Así, solo si el polinomio anterior tiene raíces cuyo valor absoluto sea menor a uno ($ | _i | &lt; 1 $) y si \\(1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p &lt; 1\\) podremos decir que el proceso es convergente y estable. Lo anterior significa que la ecuación (3.39) puede expresarse en términos de la descomposición de Wold o como la suma infinita de términos como: \\[\\begin{equation} \\frac{1}{1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p} = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots \\end{equation}\\] Donde, por construcción de \\(\\alpha(L) \\alpha^{-1}(L) = 1\\) implica que \\(\\psi_0 = 1\\). De forma similar a los procesos AR(1) y AR(2), es posible determinar el valor de los coeficientes \\(\\psi_j\\) en términos de los coeficientes \\(a_i\\). Así, la solución del proceso \\(AR(p)\\) estará dada por: \\[\\begin{equation} X_t = \\frac{a_0}{1 - a_1 - a_2 - a_3 - \\ldots - a_p} + \\sum^{\\infty}_{j = 0} \\psi_j U_{t-j} \\tag{3.40} \\end{equation}\\] Considerando la solución de la ecuación (3.38) expresada en la ecuación (3.40) podemos determinar los momentos del proceso y que estarán dados por una media como: \\[\\begin{equation} \\mathbb{E}[X_t] = \\mu = \\frac{a_o}{1 - a_1 - a_2 - a_3 - \\ldots - a_p} \\end{equation}\\] Lo anterior, considerado que \\(\\mathbb{E}[U_t] = 0\\), para todo \\(t\\). Para determinar la varianza del proceso, sin pérdida de generalidad, podemos definir una ecuación: \\(\\gamma(\\tau) = \\mathbb{E}[X_{t - \\tau} X_t]\\), la cual (omitiendo la constante, ya que la correlación de una constante con cuaquier variable aleatoria que depende del tiempo es cero (0)) puede ser escrita como: \\[\\begin{equation} \\gamma(\\tau) = \\mathbb{E}[(X_{t - \\tau}) \\cdot (a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \\ldots + + a_p X_{t-p} + U_t)] \\end{equation}\\] Donde \\(\\tau = 0, 1, 2, \\ldots, p\\) y \\(a_0 = 0\\), lo que implica que \\(\\mu = 0\\). De lo anterior obtenemos el siguiente conjunto de ecuaciones mediante sustituciones de los valores de \\(\\tau\\): \\[\\begin{eqnarray} \\gamma(0) &amp; = &amp; a_1 \\gamma(1) + a_2 \\gamma(2) + \\ldots + a_p \\gamma(p) + \\sigma^2 \\nonumber \\\\ \\gamma(1) &amp; = &amp; a_1 \\gamma(0) + a_2 \\gamma(1) + \\ldots + a_p \\gamma(p-1) \\nonumber \\\\ \\vdots \\nonumber \\\\ \\gamma(p) &amp; = &amp; a_1 \\gamma(p-1) + a_2 \\gamma(p-2) + \\ldots + a_p \\gamma(0) \\nonumber \\end{eqnarray}\\] De esta forma, es fácil observar que la ecuación general para \\(p &gt; 0\\) estará dada por: \\[\\begin{equation} \\gamma(p) - a_1 \\gamma(\\tau - 1) - a_2 \\gamma(\\tau - 2) - \\ldots - a_p \\gamma(\\tau - p) = 0 \\tag{3.41} \\end{equation}\\] Dividiendo la ecuación (3.41) por \\(\\gamma(0)\\), se obtiene la siguiente ecuación: \\[\\begin{equation} \\rho(p) - a_1 \\rho(\\tau - 1) + a_2 \\rho(\\tau - 2) + \\ldots + a_p \\rho(\\tau - p) = 0 \\end{equation}\\] Así, podemos escribir el siguiente sistema de ecuaciones: \\[\\begin{eqnarray} \\rho(1) &amp; = &amp; a_1 + a_2 \\rho(1) + a_3 \\rho(2) + \\ldots + a_p \\rho(p-1) \\nonumber \\\\ \\rho(2) &amp; = &amp; a_1 \\rho(1) + a_2 + a_3 \\rho(1) + \\ldots + a_p \\rho(p-2) \\nonumber \\\\ &amp; \\vdots &amp; \\nonumber \\\\ \\rho(p) &amp; = &amp; a_1 \\rho(p-1) + a_2 \\rho(p-2) + \\ldots + a_p \\nonumber \\end{eqnarray}\\] Lo anterior se puede expresar como un conjunto de vectores y matrices de la siguiente forma: \\[\\begin{equation} \\left[ \\begin{array}{c} \\rho(1) \\\\ \\rho(2) \\\\ \\vdots \\\\ \\rho(p) \\end{array} \\right] = \\left[ \\begin{array}{c c c c} 1 &amp; \\rho(1) &amp; \\ldots &amp; \\rho(p - 1) \\\\ \\rho(1) &amp; 1 &amp; \\ldots &amp; \\rho(p - 2) \\\\ \\rho(2) &amp; \\rho(1) &amp; \\ldots &amp; \\rho(p - 3) \\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ \\rho(p - 1) &amp; \\rho(p - 2) &amp; \\ldots &amp; 1 \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_p \\\\ \\end{array} \\right] \\end{equation}\\] De lo anterior podemos escribir la siguiente ecuación que es una forma alternativa para expresar los valores de los coeficientes \\(a_i\\) de la solución del proceso \\(AR(p)\\): \\[\\begin{equation} \\boldsymbol{\\rho} = \\mathbf{R} \\mathbf{a} \\end{equation}\\] Es decir, podemos obtener la siguiente expresión: \\[\\begin{equation} \\mathbf{a} = \\mathbf{R}^{-1} \\boldsymbol{\\rho} \\end{equation}\\] . Utilizaremos la serie de Pasajeros en vuelos internacionales de salida para estimar un \\(AR(p)\\) mediante el método de máxima verosimilitud (ML). Antes de realizar el proceso de estimación, consideremos una transformación de la serie en logaritmos y una más en diferencias logarítmicas; lo anterior con el objeto de obtener una serie de tiempo suavizada y expresada en tasas de crecimiento, con un comportamiento parecido a un proceso estacionario. Primero, para decidir si se realizará un \\(AR(p)\\) para la serie en niveles o en diferencias, analizaremos su gráfica. La serie de Pasajeros en vuelos internacionales de salidas se muestra en la Figura 3.12. En esta se muestra la gráfica de la serie en niveles (sin transformación logarítmica y con transformación logarítmica) y en diferencias logarítmicas mensuales (es decir, con diferencia respecto del mes inmediato anterior). library(ggplot2) library(dplyr) library(readxl) library(stats) Datos &lt;- read_excel(&quot;BD/Base_Transporte.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) # En Niveles Pax_Int &lt;- ts(Datos$Pax_Int, start = c(2000, 1), freq = 12) # Logaritmos: LPax_Int &lt;- ts(log(Datos$Pax_Int), start = c(2000, 1), freq = 12) # Diferencias mensuales: DLPax_Int &lt;- ts(log(Datos$Pax_Int) - lag(log(Datos$Pax_Int), 1), start = c(2000, 1), freq = 12) # par(mfrow=c(3,1)) plot(Pax_Int, xlab = &quot;Tiempo&quot;, ylab = &quot;Pasajeros&quot;, main = &quot;Pasajeros en vuelos internacionales de salida&quot;, col = &quot;darkgreen&quot;) plot(LPax_Int, xlab = &quot;Tiempo&quot;, ylab = &quot;LN Pasajeros&quot;, main = &quot;LN Pasajeros en vuelos internacionales de salida&quot;, col = &quot;darkblue&quot;) plot(DLPax_Int, xlab = &quot;Tiempo&quot;, ylab = &quot;DLN Pasajeros&quot;, main = &quot;Diff LN Pasajeros en vuelos internacionales de salia&quot;, col = &quot;darkred&quot;) Figure 3.12: Pasajeros en vuelos internacionales de salida en niveles y en diferencias logaritmicas par(mfrow=c(1,1)) De la gráfica en la Figura 3.12 observamos que quizá la mejor forma de estimar un \\(AR(p)\\) es mediante la serie en diferencias, ya que ésta es la que parece ser una serie estacionaria. A continuación, estimaremos una AR(4) para la serie en diferencias logarítmicas (\\(DLPaxInt_t\\)): Table 3.6: AR(4) para la variable \\(DLPaxInt_t\\). \\(DLPaxInt_t\\) \\(=\\) \\(0.0050\\) \\(+\\) \\(0.2701\\) \\(DLPaxInt_{t-1}\\) \\((0.0052)\\) \\((0.0655)\\) \\(-0.4326\\) \\(DLPaxNal_{t-2}\\) \\((0.0664)\\) \\(-0.1956\\) \\(DLPaxNal_{t-3}\\) \\((0.0664)\\) \\(-0.0316\\) \\(DLPaxNal_{t-4}\\) \\((0.0653)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.02371\\) \\(AIC\\) \\(=\\) \\(-198.16\\) Entre paréntesis indicamos los errores estándar y reportamos el estadístico de Akaike, AIC. Finalmente, podemos determinar si las soluciones serán convergentes, para ello en la Figura 3.13 mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual podemos concluir que el AR(4) representan una solución convergente y estable. source(&quot;arroots.R&quot;) source(&quot;plot.armaroots.R&quot;) AR_LPax_Int &lt;- arima(LPax_Int, order = c(4, 0, 0), method = &quot;ML&quot;) AR_DLPax_Int &lt;- arima(DLPax_Int, order = c(4, 0, 0), method = &quot;ML&quot;) par(mfrow=c(1,2)) plot.armaroots(arroots(AR_LPax_Nal), main=&quot;Inverse AR roots of \\nAR(2): LN Pax Int&quot;) # plot.armaroots(arroots(AR_DLPax_Nal), main=&quot;Inverse AR roots of \\nAR(2): Diff LN Pax Int&quot;) Figure 3.13: Inveso de las Raíces del polinomio característico par(mfrow=c(1,1)) 3.5 Procesos de Medias Móviles (MA) 3.5.1 MA(1) Una vez planteado el proceso generalizado de \\(AR(p)\\), iniciamos el planteamiento de los procesos de medias móviles, denotados como \\(MA(q)\\). Iniciemos con el planteamiento del proceso \\(MA(1)\\), que se puede escribir como una ecuación como la siguiente: \\[\\begin{equation} X_t = \\mu + U_t - b_1 U_{t-1} \\tag{3.42} \\end{equation}\\] O como: \\[\\begin{equation} X_t - \\mu = (1 - b_1 L) U_{t} \\end{equation}\\] Donde \\(U_t\\) es un proceso puramente aleatorio, es decir, con \\(\\mathbb{E}[U_t] = 0\\), \\(Var[U_t] = \\sigma^2\\), y \\(Cov[U_t, U_s] = 0\\). Así, un proceso \\(MA(1)\\) puede verse como un proceso AR con una descomposición de Wold en la que \\(\\psi_0 = 1\\), \\(\\psi_1 = - b_1\\) y \\(\\psi_j = 0\\) para todo \\(j &gt; 1\\). Al igual que los procesos autoregresivos, determinaremos los momentos de un proceso \\(MA(1)\\). En el caso de la media, observamos que será: \\[\\begin{eqnarray} \\mathbb{E}[X_t] &amp; = &amp; \\mu + \\mathbb{E}[U_t] - b_1 \\mathbb{E}[U_{t - 1}] \\nonumber \\\\ &amp; = &amp; \\mu \\end{eqnarray}\\] Por su parte, la varianza estará dada por: \\[\\begin{eqnarray} Var[X_t] &amp; = &amp; \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[(U_t - b_1 U_{t-1})^2] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U_t^2 - 2 b_1 U_t U_{t-1} + b_1^2 U_{t - 1}^2] \\nonumber \\\\ &amp; = &amp;\\mathbb{E}[U_t^2] - 2 b_1 \\mathbb{E}[U_t U_{t-1}] + b_1^2 \\mathbb{E}[U_{t - 1}^2]] \\nonumber \\\\ &amp; = &amp; \\sigma^2 + b_1^2 \\sigma^2 \\nonumber \\\\ &amp; = &amp; (1 + b_1^2) \\sigma^2 = \\gamma(0) \\end{eqnarray}\\] De esta forma, la varianza del proceso es constante en cualquier periodo \\(t\\). Para determinar la covarianza utilizaremos la siguiente ecuación: \\[\\begin{eqnarray} \\mathbb{E}[(x_t - \\mu)(x_{t + \\tau} - \\mu)] &amp; = &amp; \\mathbb{E}[(U_t - b_1 U_{t-1})(U_{t + \\tau} - b_1 U_{t + \\tau - 1})] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U_t U_{t + \\tau} - b_1 U_t U_{t + \\tau - 1} - b_1 U_{t - 1} U_{t + \\tau} \\nonumber \\\\ &amp; &amp; + b_1^2 U_{t - 1} U_{t + \\tau - 1}] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U_t U_{t + \\tau}] - b_1 \\mathbb{E}[U_t U_{t + \\tau - 1}] \\nonumber \\\\ &amp; &amp; - b_1 \\mathbb{E}[U_{t - 1} U_{t + \\tau}] + b_1^2 \\mathbb{E}[U_{t - 1} U_{t + \\tau - 1}] \\tag{3.43} \\end{eqnarray}\\] Si hacemos sustituciones de diferentes valores de \\(\\tau\\) en la ecuación (3.43) notaremos que la covarianza será distinta de cero únicamente para el caso de \\(\\tau = 1, -1\\). En ambos casos tendremos como resultado: \\[\\begin{eqnarray} \\mathbb{E}[(x_t - \\mu)(x_{t + 1} - \\mu)] &amp; = &amp; \\mathbb{E}[(x_t - \\mu)(x_{t - 1} - \\mu)] \\nonumber \\\\ &amp; = &amp; - b_1 \\mathbb{E}[U_t U_{t}] \\nonumber \\\\ &amp; = &amp; - b_1 \\mathbb{E}[U_{t - 1} U_{t - 1}] \\nonumber \\\\ &amp; = &amp; - b_1^2 \\sigma^2 = \\gamma(1) \\end{eqnarray}\\] De esta forma tendremos que las funciones de autocorrelación estarán dadas por los siguientes casos: \\[\\begin{eqnarray} \\rho(0) &amp; = &amp; 1 \\nonumber \\\\ \\rho(1) &amp; = &amp; \\frac{- b_1}{1 + b_1^2} \\nonumber \\\\ \\rho(\\tau) &amp; = &amp; 0 \\text{ para todo } \\tau &gt; 1 \\nonumber \\end{eqnarray}\\] Ahora regresando a la ecuación (3.42), su solución la podemos expresar como: \\[\\begin{eqnarray} U_ t &amp; = &amp; - \\frac{\\mu}{1 - b_1} + \\frac{1}{1 - b_1 L} X_t \\nonumber \\\\ &amp; = &amp; - \\frac{\\mu}{1 - b_1} + X_t + b_1 X_{t-1} + b_1^2 X_{t-2} + \\ldots \\nonumber \\end{eqnarray}\\] Donde la condición para que se cumpla esta ecuación es que $ | b_1 | &lt; 1 $. La manera de interpretar esta condición es como una condición de estabilidad de la solución y cómo una condición de invertibilidad. Notemos que un \\(MA(1)\\) (y en general un \\(MA(q)\\)) es equivalente a un \\(AR(\\infty)\\), es decir, cuando se invierte un MA se genera un AR con infinitos rezagos. En esta sección no desarrollaremos un ejemplo, primero explicaremos en qué consiste una modelación del tipo \\(MA(q)\\) y después plantearemos un ejemplo en concreto. 3.5.2 MA(q) En general, el proceso de medias móviles de orden \\(q\\), \\(MA(q)\\), puede ser escrito como: \\[\\begin{equation} X_t = \\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q} \\tag{3.44} \\end{equation}\\] Podemos reescribir la ecuación (3.44) utilizando el operador rezago. Así, tendremos el proceso de \\(MA(q)\\) como: \\[\\begin{eqnarray} X_t - \\mu &amp; = &amp; (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q) U_{t} \\nonumber \\\\ X_t - \\mu &amp; = &amp; \\beta(L) U_t \\tag{3.45} \\end{eqnarray}\\] Donde \\(U_t\\) es un proceso puramente aleatorio con \\(\\mathbb{E}[U_t] = 0\\), \\(Var[U_t] = \\mathbb{E}[U_t^2] = 0\\) y \\(Cov[U_t, U_s] = \\mathbb{E}[U_t, U_s] = 0\\), y \\(\\beta(L) = 1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q\\) es un polinomio del operador rezago \\(L\\). La ecuación (3.45) puede ser interpretada como un proceso \\(AR(q)\\) sobre la serie \\(U_t\\). Ahora determinemos los momentos de un proceso \\(MA(q)\\): \\[\\begin{eqnarray} \\mathbb{E}[X_t] &amp; = &amp; \\mathbb{E}[\\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}] \\nonumber \\\\ &amp; = &amp; \\mu + \\mathbb{E}[U_t] - b_1 \\mathbb{E}[U_{t-1}] - b_2 \\mathbb{E}[U_{t-2}] - \\ldots - b_q \\mathbb{E}[U_{t-q}] \\nonumber \\\\ &amp; = &amp; \\mu \\end{eqnarray}\\] En el caso de la varianza tenemos que se puede expresar como: \\[\\begin{eqnarray} Var[X_t] &amp; = &amp; \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q})^2] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U_t^2 + b_1^2 U_{t-1}^2 + b_2^2 U_{t-2}^2 + \\ldots + b_q^2 U_{t-q}^2 \\nonumber \\\\ &amp; &amp; - 2 b_1 U_t U_{t - 1} - \\ldots - 2 b_{q - 1} b_q U_{t - q + 1} U_{t - q}] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[U_t^2] + b_1^2 \\mathbb{E}[U_{t-1}^2] + b_2^2 \\mathbb{E}[U_{t-2}^2] + \\ldots + b_q^2 \\mathbb{E}[U_{t-q}^2] \\nonumber \\\\ &amp; &amp; - 2 b_1 \\mathbb{E}[U_t U_{t - 1}] - \\ldots - 2 b_{q - 1} b_q \\mathbb{E}[U_{t - q + 1} U_{t - q}] \\nonumber \\\\ &amp; = &amp; \\sigma^2 + b^2_1 \\sigma^2 + b^2_2 \\sigma^2 + \\ldots + b^2_q \\sigma^2 \\nonumber \\\\ &amp; = &amp; (1 + b^2_1 + b^2_2 + \\ldots + b^2_q) \\sigma^2 \\end{eqnarray}\\] En el caso de las covarianzas podemos utilizar una idea similar al caso del \\(AR(p)\\), construir una expresión general para cualquier rezago \\(\\tau\\): \\[\\begin{eqnarray} Cov[X_t, X_{t + \\tau}] &amp; = &amp; \\mathbb{E}[(X_t - \\mu)(X_{t + \\tau} - \\mu)] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}) \\nonumber \\\\ &amp; &amp; (U_{t + \\tau} - b_1 U_{t + \\tau -1} - b_2 U_{t + \\tau -2} - \\ldots - b_q U_{t + \\tau - q})] \\nonumber \\end{eqnarray}\\] La expresión anterior se puede desarrollar para múltiples casos de \\(\\tau = 1, 2, \\ldots, q\\). De esta forma tenemos el siguiente sistema: \\[\\begin{eqnarray} \\tau = 1 &amp; : &amp; \\gamma(1) = (- b_1 + b_1 b_2 + \\ldots + b_{q-1} b_q) \\sigma^2 \\nonumber \\\\ \\tau = 2 &amp; : &amp; \\gamma(2) = (- b_2 + b_1 b_3 + \\ldots + b_{q-2} b_q) \\sigma^2 \\nonumber \\\\ &amp; \\vdots &amp; \\nonumber \\\\ \\tau = q &amp; : &amp; \\gamma(q) = b_q \\sigma^2 \\nonumber \\end{eqnarray}\\] Donde \\(\\gamma(\\tau) = 0\\) para todo \\(\\tau &gt; q\\). Es decir, todas las autocovarianzas y autocorrelaciones con órdenes superiores a \\(q\\) son cero (0). De esta forma, esta característica teórica permite identificar el orden de \\(MA(q)\\) visualizando la función de autocorrelación y verificando a partir de cuál valor de rezago la autocorrelación es no significativa. Regresando al problema original que es el de determinar una solución para la ecuación (3.44), tenemos que dicha solución estará dada por un \\(AR(\\infty)\\) en términos de \\(U_t\\): \\[\\begin{eqnarray} U_t &amp; = &amp; - \\frac{\\mu}{1 - b_1 - b_2 - \\ldots - b_q} + \\beta(L)^{-1} X_t \\nonumber \\\\ &amp; &amp; - \\frac{\\mu}{1 - b_1 - b_2 - \\ldots - b_q} + \\sum_{j = 0}^{\\infty} c_j X_{t-j} \\tag{3.46} \\end{eqnarray}\\] Donde se cumple que: \\(1 = (1 - b_1 L^1 - b_2 L^2 - \\ldots - b_q L^q)(1 - c_1 L - c_2 L^2 - \\ldots)\\) y los coeficientes \\(c_j\\) se pueden determinar por un método de coeficientes indeterminados y en términos de los valores \\(b_i\\). De igual forma que en el caso de la ecuación (3.38), en la ecuación (3.46) se deben cumplir condiciones de estabilidad asociadas con las raíces del polinomio característico dado por: \\[\\begin{equation} 1 - b_1 x - b_2 x^2 - \\ldots b_q x^q = 0 \\end{equation}\\] El cual debe cumplir que $ | x_i | &lt; 1 $ y que \\(1 - b_1 - b_2 - \\ldots b_q &lt; 1\\). . Ahora veamos un ejemplo del proceso \\(MA(q)\\), para lo cual retomaremos la serie de Pasajeros transportados en el metro de la CDMX (\\(PaxMetro\\)). Estimaremos el \\(MA(q)\\) mediante el método de máxima verosimilitud (ML). Antes de realizar el proceso de estimación, consideremos una transformación de la serie en logaritmos y una más en diferencias logarítmicas; lo anterior con el objeto de obtener una serie de tiempo suavizada y expresada en tasas de crecimiento, con un comportamiento parecido a un proceso estacionario. La serie de Pasajeros transportados en el metro de la CDMX se muestra en la Figura 3.9. En esta, se muestra la gráfica de la serie en niveles (sin transformación logarítmica y con transformación logarítmica) y en diferencias logarítmicas mensuales (es decir, con una diferencia respecto del mes inmediato anterior). Utilizaremos la serie en diferencias, ya que es la que parece ser estacionaria. Esta serie tiene la peculiaridad de que tiene un salto a la baja y uno al alza entre septiembre de 2017 y octubre de 2017. Para controlar ese efecto, en nuestro modelo \\(MA(q)\\) incluiremos dos variables dummies para dichos meses. A continuación, estimaremos una \\(MA(4)\\) para la serie en diferencias: Table 3.7: MA(4) para la variable \\(DLPaxMetro_t\\). \\(DLPaxMetro_t\\) \\(=\\) \\(0.0000\\) \\(-\\) \\(0.7804\\) \\(U_{t-1}\\) \\((0.0000)\\) \\((0.0661)\\) \\(+\\) \\(0.3591\\) \\(U_{t-2}\\) \\((0.0826)\\) \\(-\\) \\(0.2775\\) \\(U_{t-3}\\) \\((0.0908)\\) \\(-\\) \\(0.1120\\) \\(U_{t-4}\\) \\((0.0769)\\) \\(-\\) \\(0.3789\\) \\(DSep2017\\) \\((0.0436)\\) \\(+\\) \\(0.3695\\) \\(DOct2017\\) \\((0.0434)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.002176\\) \\(AIC\\) \\(=\\) \\(-749.94\\) Entre parentésis indicamos los errores estándar y al final reportamos el estadístico de Akaike, AIC. Finalmente, podemos determinar si la solución serán convergente, para ello en la Figura 3.14 mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual, podemos concluir que ambas propuestas de MA(4) representan una solución convergente y estable. library(ggplot2) library(dplyr) library(readxl) library(stats) Datos &lt;- read_excel(&quot;BD/Base_Transporte_ARIMA.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) source(&quot;maroots.R&quot;) source(&quot;plot.armaroots.R&quot;) Pax_Metro &lt;- ts(Datos$Pax_Metro, start = c(2000, 1), freq = 12) LPax_Metro &lt;- ts(log(Datos$Pax_Metro), start = c(2000, 1), freq = 12) DLPax_Metro &lt;- ts(log(Datos$Pax_Metro) - lag(log(Datos$Pax_Metro), 1), start = c(2000, 1), freq = 12) D_Sep2017 &lt;- ts(Datos$D_Sep2017, start = c(2000, 1), freq = 12) D_Abr2020 &lt;- ts(Datos$D_Abr2020, start = c(2000, 1), freq = 12) D_May2020 &lt;- ts(Datos$D_May2020, start = c(2000, 1), freq = 12) MA_LPax_Metro &lt;- arima(LPax_Metro, order = c(0, 0, 4), method = &quot;ML&quot;) MA_LPax_Metro_2 &lt;- arima(LPax_Metro, order = c(0, 0, 4), xreg = cbind(D_Abr2020, D_Sep2017, D_May2020), method = &quot;ML&quot;) par(mfrow=c(1,2)) plot.armaroots(maroots(MA_LPax_Metro), main=&quot;Inverse MA roots of \\nMA(p): LN PAx Metro&quot;) plot.armaroots(maroots(MA_LPax_Metro_2), main=&quot;Inverse MA roots of \\nMA(p): LN PAx Metro con Dummy&quot;) Figure 3.14: Inveso de las Raíces del polinomio característico par(mfrow=c(1,1)) 3.6 Procesos ARMA(p, q) y ARIMA(p, d, q) Hemos establecido algunas relaciones de los procesos AR y los procesos MA, es decir, cómo un \\(MA(q)\\) de la serie \\(X_t\\) puede ser reexpresada como un \\(AR(\\infty)\\) de la serie \\(U_t\\), y viceversa un \\(AR(p)\\) de la serie \\(X_t\\) puede ser reeexpresada como un \\(MA(\\infty)\\). En este sentido, para cerrar esta sección veamos el caso de la especificación que conjunta ambos modelos en un modelo general conocido como \\(ARMA(p, q)\\) o \\(ARIMA(p, d, q)\\). La diferencia entre el primero y el segundo es las veces que se tuvo que diferenciar la serie analizada, registro que se lleva en el índice \\(d\\) de los parámetros dentro del concepto \\(ARIMA(p, d, q)\\). No obstante, en general nos referiremos al modelo como \\(ARMA(p, q)\\) y dependerá del analista si modela la serie en niveles (por ejemplo, en logaritmos) o en diferencias logarítmicas (o diferencias sin logaritmos). 3.6.1 ARMA(1, 1) Dicho lo anterior, vamos a empezar con el análisis de un \\(ARMA(1, 1)\\). Un proceso \\(ARMA(1, 1)\\) puede verse como: \\[\\begin{equation} X_t = \\delta + a_1 X_{t - 1} + U_t - b_1 U_{t - 1} \\tag{3.47} \\end{equation}\\] Aplicando el operado rezago podemos rescribir la ecuación (3.47) como: \\[\\begin{equation} (1 - a_1 L) X_t = \\delta + (1 - b_1 L) U_t \\end{equation}\\] Donde \\(U_t\\) es un proceso puramente aleatorio como en los casos de \\(AR(p)\\) y \\(MA(q)\\), y \\(X_t\\) puede ser una serie en niveles o en diferencias (ambas, en términos logarítmicos). Así, el modelo \\(ARIMA (p, q)\\) también tiene una representación de Wold que estará dada por las siguientes expresiones: \\[\\begin{equation} X_t = \\frac{\\delta}{1 - a_1} + \\frac{1 - b_1 L}{1 - a_1 L} U_t \\tag{3.48} \\end{equation}\\] Donde \\(a_1 \\neq b_1\\), puesto que en caso contrario \\(X_t\\) sería un proceso puramente aleatorio con una media \\(\\mu = \\frac{\\delta}{1 - a_1}\\). Así, podemos reescribir la descomposición de Wold a partir del componente de la ecuación (3.48): \\[\\begin{equation} \\frac{1 - b_1 L}{1 - a_1 L} = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots \\tag{3.49} \\end{equation}\\] Está ecuación es equivalente a la expresión: \\[\\begin{eqnarray} (1 - b_1 L) &amp; = &amp; (1 - a_1 L)(\\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots) \\nonumber \\\\ &amp; = &amp; \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots \\nonumber \\\\ &amp; &amp; - a_1 \\psi_0 L - a_1 \\psi_1 L^2 - a_2 \\psi_2 L^3 - a_1 \\psi_3 L^4 - \\ldots \\nonumber \\end{eqnarray}\\] De esta forma podemos establecer el siguiente sistema de coeficientes indeterminados: \\[\\begin{eqnarray*} L^0 &amp; : &amp; \\Rightarrow \\psi_0 = 1 \\\\ L^1 &amp; : &amp; \\psi_1 - a_1 \\psi_0 = - b_1 \\Rightarrow \\psi_1 = a_1 - b_1 \\\\ L^2 &amp; : &amp; \\psi_2 - a_1 \\psi_1 = 0 \\Rightarrow \\psi_2 = a_1(a_1 - b_1) \\\\ L^3 &amp; : &amp; \\psi_3 - a_1 \\psi_2 = 0 \\Rightarrow \\psi_3 = a^2_1(a_1 - b_1) \\\\ &amp; \\vdots &amp; \\\\ L^j &amp; : &amp; \\psi_j - a_1 \\psi_{j - 1} = 0 \\Rightarrow \\psi_j = a^{j - 1}_1(a_1 - b_1) \\end{eqnarray*}\\] Así, la solución a la ecuación (3.47) estará dada por la siguiente generalización: \\[\\begin{equation} X_t = \\frac{\\delta}{1 - a_1} + U_t + (a_1 - b_1) U_{t - 1} + a_1(a_1 - b_1) U_{t - 2} + a_1^2(a_1 - b_1) U_{t - 3} + \\ldots \\tag{3.50} \\end{equation}\\] En la ecuación (3.50) las condiciones de estabilidad y de invertibilidad del sistema (de un MA a un AR, y viceversa) estarán dadas por: $ | a_1 | &lt; 1 $ y $ | b_1 | &lt; 1 $. Adicionalmente, la ecuación (3.50) expresa cómo una serie que tiene un comportamiento \\(ARMA(1, 1)\\) es equivalente a una serie modelada bajo un \\(MA(\\infty)\\). Al igual que en los demás modelos, ahora determinaremos los momentos del proceso \\(ARMA(1, 1)\\). La media estará dada por: \\[\\begin{eqnarray} \\mathbb{E}[X_t] &amp; = &amp; \\mathbb{E}[\\delta + a_1 X_{t-1} + U_t - b_1 U_{t-1}] \\nonumber \\\\ &amp; = &amp; \\delta + a_1 \\mathbb{E}[X_{t-1}] \\nonumber \\\\ &amp; = &amp; \\frac{\\delta}{1 - a_1} \\nonumber \\\\ &amp; = &amp; \\mu \\end{eqnarray}\\] Donde hemos utilizado que \\(\\mathbb{E}[X_t] = \\mathbb{E}[X_{t-1}] = \\mu\\). Es decir, la media de un \\(ARMA(1, 1)\\) es idéntica a la de un \\(AR(1)\\). Para determinar la varianza tomaremos una estrategia similar a los casos de \\(AR(p)\\) y \\(MA(q)\\). Por lo que para todo \\(\\tau \\geq 0\\), y suponiendo por simplicidad que \\(\\delta = 0\\) (lo que implica que \\(\\mu = 0\\)) tendremos: \\[\\begin{eqnarray} \\mathbb{E}[X_{t-\\tau} X_t] &amp; = &amp; \\mathbb{E}[(X_{t-\\tau}) \\cdot (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \\nonumber \\\\ &amp; = &amp; a_1 \\mathbb{E}[X_{t-\\tau} X_{t-1}] + \\mathbb{E}[X_{t-\\tau} U_t] - b_1 \\mathbb{E}[X_{t-\\tau} U_{t-1}] \\tag{3.51} \\end{eqnarray}\\] De la ecuación (3.51) podemos determinar una expresión para el caso de \\(\\tau = 0\\): \\[\\begin{eqnarray} \\mathbb{E}[X_{t} X_t] &amp; = &amp; \\gamma(0) \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(1) + \\mathbb{E}[U_t X_t] - b_1 \\mathbb{E}[X_t U_{t-1}] \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(1) + \\sigma^2 + b_1 \\mathbb{E}[U_{t-1} (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(1) + \\sigma^2 - b_1 a_1 \\sigma^2 + b_1 \\sigma^2 \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(1) + (1 - b_1 (a_1 - b_1)) \\sigma^2 \\end{eqnarray}\\] Para el caso en que \\(\\tau = 1\\): \\[\\begin{eqnarray} \\mathbb{E}[X_{t-1} X_t] &amp; = &amp; \\gamma(1) \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(0) + \\mathbb{E}[X_{t-1} U_t] - b_1 \\mathbb{E}[X_{t-1} U_{t-1}] \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(0) - b_1 \\sigma^2 \\end{eqnarray}\\] Estas últimas expresiones podemos resolverlas como sistema para determinar los siguientes valores: \\[\\begin{eqnarray} \\gamma(0) &amp; = &amp; \\frac{1 + b_1^2 - 2 a_1 b_1}{1 - a_1^2} \\sigma^2 \\\\ \\gamma(1) &amp; = &amp; \\frac{(a_1 - b_1)(1 - a_1 b_1)}{1 - a_1^2} \\sigma^2 \\end{eqnarray}\\] En general, para cualquier valor \\(\\tau \\geq 2\\) tenemos que la autocovarianza y la función de autocorrelación serán: \\[\\begin{eqnarray} \\gamma(\\tau) = a_1 \\gamma(\\tau - 1) \\\\ \\rho(\\tau) = a_1 \\rho(\\tau - 1) \\end{eqnarray}\\] Por ejemplo, para el caso de \\(\\tau = 1\\) tendríamos: \\[\\begin{equation} \\rho(1) = \\frac{(a_1 - b_1)(1 - a_1 b_1)}{1 + b_1^2 - 2 a_1 b_1} \\end{equation}\\] De esta forma, la función de autocorrelación oscilará en razón de los valores que tome \\(a_1\\) y \\(b_1\\). 3.6.2 ARMA(p, q) La especificación general de un \\(ARMA(p, q)\\) (donde \\(p, q \\in \\mathbb{N}\\)) puede ser descrita por la siguiente ecuación: \\[\\begin{eqnarray} X_t &amp; = &amp; \\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \\ldots + a_p X_{t - p} \\nonumber \\\\ &amp; &amp; + U_t - b_1 U_{t - 1} - b_2 U_{t - 2} - \\ldots - b_q U_{t - q} \\tag{3.52} \\end{eqnarray}\\] Donde \\(U_t\\) es un proceso puramente aleatorio, y \\(X_t\\) puede ser modelada en niveles o en diferencias (ya sea en logaritmos o sin transformación logarítmica). Mediante el uso del operador rezago se puede escribir la ecuación (3.52) como: \\[\\begin{equation} (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p) X_t = \\delta + (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q) U_t \\tag{3.53} \\end{equation}\\] En la ecuación (3.53) definamos dos polinomios: \\(\\alpha(L) = (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p)\\) y \\(\\beta(L) = (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q)\\). Así, podemos reescribir la ecuación (3.53) como: \\[\\begin{equation} \\alpha(L) X_t = \\delta + \\beta(L) U_t \\end{equation}\\] Asumiendo que existe el polinomio inverso tal que: \\(\\alpha(L)^{-1}\\alpha(L) = 1\\).La solución entonces puede ser escrita como: \\[\\begin{eqnarray} X_t &amp; = &amp; \\alpha(L)^{-1} \\delta + \\alpha(L)^{-1} \\beta(L) U_t \\nonumber \\\\ &amp; = &amp; \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + \\frac{\\beta(L)}{\\alpha(L)} U_t \\nonumber \\\\ &amp; = &amp; \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + U_t + \\psi_1 L U_t + \\psi_2 L^2 U_t + \\ldots \\tag{3.54} \\end{eqnarray}\\] Donde la ecuación (3.54) nos permite interpretar que un ARMA(p, q) se puede reexpresar e interpretar como un \\(MA(\\infty)\\) y donde las condiciones para la estabilidad de la solución y la invertibilidad son que las raíces de los polinomios característicos \\(\\alpha(L)\\) y \\(\\beta(L)\\) son en valor absoluto menores a 1. Adicionalmente, la fracción en la ecuación (3.54) se puede descomponer como en la forma de Wold: \\[\\begin{equation} \\frac{\\beta(L)}{\\alpha(L)} = 1 + \\psi_1 L + \\psi_2 L^2 + \\ldots \\end{equation}\\] Bajo los supuestos de estacionariedad del componente \\(U_t\\), los valores de la media y varianza de un proceso \\(ARMA(p, q)\\) serán como describimos ahora. Para el caso de la media, podemos partir de la ecuación (3.54) para generar: \\[\\begin{eqnarray} \\mathbb{E}[X_t] &amp; = &amp; \\mathbb{E}\\left[ \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + U_t + \\psi_1 U_{t-1} + \\psi_2 U_{t-2} + \\ldots \\right] \\nonumber \\\\ &amp; = &amp; \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} \\nonumber \\\\ &amp; = &amp; \\mu \\end{eqnarray}\\] Esta expresión indica que en general un proceso \\(ARMA(p, q)\\) converge a una media idéntica a la de un proceso \\(AR(p)\\). Para determinar la varianza utilizaremos la misma estrategia que hemos utilizado para otros modelos \\(AR(p)\\) y \\(MA(q)\\). Sin pérdida de generalidad podemos asumir que \\(\\delta = 0\\), lo que implica que \\(\\mu = 0\\), de lo que podemos establecer una expresión de autocovarianzas para cualquier valor \\(\\tau = 0, 1, 2, \\ldots\\): \\[\\begin{eqnarray} \\gamma(\\tau) &amp; = &amp; \\mathbb{E}[X_{t-\\tau} X_t] \\nonumber \\\\ &amp; = &amp; \\mathbb{E}[X_{t-\\tau} (\\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \\ldots + a_p X_{t - p} \\nonumber \\\\ &amp; &amp; + U_t - b_1 U_{t - 1} - b_2 U_{t - 2} - \\ldots - b_q U_{t - q})] \\nonumber \\\\ &amp; = &amp; a_1 \\gamma(\\tau - 1) + a_2 \\gamma(\\tau - 2) + \\ldots + a_p \\gamma(\\tau - p) \\nonumber \\\\ &amp; &amp; + \\mathbb{E}[X_{t-\\tau} U_{t}] - b_1 \\mathbb{E}[X_{t-\\tau} U_{t-1}] - \\ldots - b_q \\mathbb{E}[X_{t-\\tau} U_{t-q}] \\end{eqnarray}\\] Ahora veámos un ejemplo. Utilizaremos la serie de Pasajeros en vuelos nacionales de salida para estimar un \\(ARMA(p, q)\\) mediante el método de máxima verosimilitud (ML). Antes de realizar el proceso de estimación, consideremos una transformación de la serie en diferencias logarítmicas, ya que, según la gráfica en la Figura 3.10, esa es la que puede ser estacionaria. A continuación, estimaremos una \\(ARMA(1, 1)\\) para la serie en diferencias logarítmicas (\\(DLPaxNal_t\\)). También incorporaremos al análisis variables exógenas tales como dummies de estacionalidad. En particular, utilizaremos los meses de enero, febrero, julio y diciembre. No debe pasar desapercibido que un análisis de estacionalidad más formal debería considerar todos los meses para separar del término de error la parte que puede ser explicada por los ciclos estacionales. Así obtenemos el siguiente resultado: Table 3.8: ARMA(1, 1) para la variable \\(DLPaxNal_t\\). \\(DLPaxNal_t\\) \\(=\\) \\(-0.0025\\) \\(-\\) \\(0.6018\\) \\(DLPaxNal_{t-1}\\) \\((0.0045)\\) \\((0.0964)\\) \\(-\\) \\(0.9064\\) \\(U_{t-1}\\) \\((0.0397)\\) \\(-\\) \\(0.0867\\) \\(DEne_t\\) \\((0.0230)\\) \\(-\\) \\(0.0409\\) \\(DFeb_t\\) \\((0.0263)\\) \\(+\\) \\(0.1529\\) \\(DJul_t\\) \\((0.0245)\\) \\(+\\) \\(0.0628\\) \\(DDic_t\\) \\((0.0223)\\) \\(\\hat{\\sigma}^2\\) \\(=\\) \\(0.007096\\) \\(AIC\\) \\(=\\) \\(-475.12\\) Donde entre paréntesis indicamos los errores estándar. Adicionalmente, reportamos el estadístico de Akaike (AIC). Finalmente, podemos determinar si las soluciones serán convergentes, para ello en la Figura 3.15 mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual, podemos concluir que tenemos una solución convergente y estable. Por su parte, la Figura 3.16 muestra los residuales de la estimación del \\(ARMA(1, 1)\\). knitr::include_graphics(&quot;Plots/G_Roots_ARMA11.png&quot;) Figure 3.15: Inveso de las Raíces del polinomio característico de un ARMA(1,1) knitr::include_graphics(&quot;Plots/G_Residuals_ARMA11.png&quot;) Figure 3.16: Residuales de un \\(ARMA(1, 1)\\) de la serie \\(DLPaxNal_t\\) En lo que resta de este capítulo, utilizaremos la serie en diferencias logarítmicas de los pasajeros en vuelos nacionales de salida, \\(DLPaxNal_t\\), para discutir los ejemplos que ilustran cada uno de los puntos teóricos que a continuación exponemos. 3.7 Función de Autocorrelación Parcial Ahora introduciremos el concepto de Función de Autocorrelación Parcial (PACF, por sus siglas en inglés). Primero, dadas las condiciones de estabilidad y de convergencia, si suponemos que los procesos AR, MA, ARMA o ARIMA tienen toda la información de los rezagos de la serie en conjunto y toda la información de los promedios móviles del término de error, resulta importante construir una métrica para distinguir el efecto de \\(X_{t - \\tau}\\) o el efecto de \\(U_{t - \\tau}\\) (para cualquier \\(\\tau\\)) sobre \\(X_t\\) de forma individual. La idea es construir una métrica de la correlación que existe entre las diferentes variables aleatorias, si para tal efecto se ha controlado el efecto del resto de la información. Así, podemos definir la ecuación que puede responder a este planteamiento como: \\[\\begin{equation} X_t = \\phi_{k1} X_{t-1} + \\phi_{k2} X_{t-2} + \\ldots + \\phi_{kk} X_{t-k} + U_t \\tag{3.55} \\end{equation}\\] Donde \\(\\phi_{ki}\\) es el coeficiente de la variable dada con el rezago \\(i\\) si el proceso tiene un órden \\(k\\). Así, los coeficientes \\(\\phi_{kk}\\) son los coeficientes de la autocorrelación parcial (considerando un proceso AR(k)). Observemos que la autocorrelación parcial mide la correlación entre \\(X_t\\) y \\(X_{t-k}\\) que se mantiene cuando el efecto de las variables \\(X_{t-1}\\), \\(X_{t-2}\\), \\(\\ldots\\) y \\(X_{t-k-1}\\) en \\(X_{t}\\) y \\(X_{t-k}\\) ha sido eliminado. Dada la expresión considerada en la ecuación (3.55), podemos resolver el problema de establecer el valor de cada \\(\\phi_{ki}\\) mediante la solución del sistema que se representa en lo siguiente: \\[\\begin{equation} \\left[ \\begin{array}{c} \\rho(1) \\\\ \\rho(2) \\\\ \\vdots \\\\ \\rho(k) \\end{array} \\right] = \\left[ \\begin{array}{c c c c} 1 &amp; \\rho(1) &amp; \\ldots &amp; \\rho(k - 1)\\\\ \\rho(1) &amp; 1 &amp; \\ldots &amp; \\rho(k - 2)\\\\ \\rho(2) &amp; \\rho(1) &amp; \\ldots &amp; \\rho(k - 3)\\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots\\\\ \\rho(k - 1) &amp; \\rho(k - 2) &amp; \\ldots &amp; 1\\\\ \\end{array} \\right] \\left[ \\begin{array}{c} \\phi_{k1} \\\\ \\phi_{k2} \\\\ \\phi_{k3} \\\\ \\vdots \\\\ \\phi_{kk} \\\\ \\end{array} \\right] \\end{equation}\\] Del cual se puede derivar una solución, resolviendo por cualquier método que consideremos y que permita calcular la solución de sistemas de ecuaciones. Posterior al planteamiento analítico, plantearemos un enfoque para interpretar las funciones de autocorrelación y autocorrelación parcial. Este enfoque pretende aportar al principio de parsimonia, en el cual podemos identificar el número de parámetros que posiblemente puede describir mejor a la serie en un modelo ARMA(p, q). En el siguiente cuadro mostramos un resumen de las características que debemos observar para determinar el número de parámetros de cada uno de los componentes AR y MA. Lo anterior por observación de las funciones de autocorrelación y autocorrelación parcial. Este enfoque no es el más formal, más adelante implementaremos uno más formal y que puede ser más claro de cómo determinar el número de parámetros. Table 3.9: Relación entre la Función de Autocorrelación y la Función de Autocorrelación Parcial de una serie \\(X_t\\). Modelo Función de Autocorrelación Función de Autocorrelación Parcial MA(q) Rompimienttos en la función No hay rompimientos AR(p) No hay rompimientos Rompimienttos en la función . Continuando con el ejemplo en la Figura 3.17 mostramos tanto la Función de Autocorrelación como la Función de Autocorrelación Parcial. En esta identificamos que ambas gráficas muestran que el modelo que explica a la variable \\(DLPaxNal_t\\) tiene tanto componentes AR como MA. Sin embargo, dado lo errático del comportamiento de ambas funciones, resulta complicado determinar cuál sería un buen número de parámetros \\(p\\) y \\(q\\) a considerar en el \\(ARMA(p,q)\\). Por esta razón, a continuación, plantearemos algunas pruebas más formales para determinar dichos parámetros. knitr::include_graphics(&quot;Plots/G_ACF_PACF.png&quot;) Figure 3.17: Función de Autocorrelación y la Función de Autocorrelación Parcial de una serie \\(DLPaxNal_t\\) 3.8 Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q) Respecto de cómo estimar un proceso ARMA(p, q) –en general utilizaremos este modelo para discutir, pero lo planteado en esta sección es igualmente aplicable en cualquier otro caso como aquellos modelos que incluyen variables exógenas– existen diversas formas de estimar los parámetros \\(a_i\\) y \\(b_i\\): i) por máxima verosimilitud y ii) por mínimos cuadrados ordinarios. El primer caso requiere que conozcamos la distribución del proceso aleatorio \\(U_t\\). El segundo, por el contrario, no requiere el mismo supuesto. No obstante, para el curso utilizaremos el método de máxima verosimilitud. Otra duda que debe quedar hasta el momento es ¿cómo determinar el orden \\(p\\) y \\(q\\) del proceso ARMA(p, q)? La manera más convencional y formal que existe para tal efecto es utilizar los criterios de información. Así, el orden se elige de acuerdo a aquel criterio de información que resulta ser el mínimo. En el caso de \\(d\\) se selecciona revisando la gráfica que parezca más estacionaria–más adelante mostraremos un proceso más formal para su selección. Los criterios de información más comunes son los siguientes: FPE (Final Prediction Error): \\[\\begin{equation} FPE = \\frac{T+m}{T-m}\\frac{1}{T}\\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\end{equation}\\] Akaike: \\[\\begin{equation} AIC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2}{T} \\end{equation}\\] Schwarz: \\[\\begin{equation} SC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{ln(T)}{T} \\end{equation}\\] Hannan - Quinn: \\[\\begin{equation} HQ = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2 ln(ln(T))}{T} \\end{equation}\\] Donde \\(\\hat{U}_t^{(p)}\\) son los residuales estimados mediante un proceso ARIMA y \\(m\\) es el número de parametros estimados: \\(m = p + q + 0 + 1\\) (ya que asumimos que \\(d = 0\\)). Una propiedad que no se debe perder de vista es que los criterios de información cumplen la siguiente relación: \\[\\begin{equation} orden(SC) \\leq orden(HQ) \\leq orden(AIC) \\end{equation}\\] Por esta razón, durante el curso solo utilizaremos el criterio de Akaike para determinar el orden óptimo del proceso ARMA, ya que ello garantiza el orden más grande posible. . Ahora veamos un ejemplo de estimación del número de rezagos óptimo de un \\(ARMA(p, q)\\). Retomemos la serie en diferencias logarítmicas de los pasajeros en vuelos nacionales de salidas, pero ahora incluiremos las variables exógenas de dummies estacionales: enero, febrero, julio y diciembre. Como mencionamos, las gráficas de las funciones de autocorrelación permiten observar el valor de la correlación existente entre la variable en el momento \\(t\\) con cada uno de los rezagos. Incluso la Función de Autocorrelación Parcial puede ayudar a determinar el número máximo de rezagos que se debe incluir en el proceso \\(AR(p)\\). No obstante, una métrica más formal es el uso de los criterios de información. En nuestro caso, dado lo discutido, sólo utilizaremos el criterio de Akaike. Al respecto, en el siguiente cuadro reportamos el criterio de Akaike que resulta de aplicar dicho criterio a los residuales resultantes de cada combinación de procesos \\(ARMA(p, q)\\). La forma de escoger será aquel modelo que reporta el criterio de Akaike menor. En la cuarta columna de la tabla se señala el valor del criterio de información que resulta ser el mínimo de todos los posibles. Table 3.10: Criterio de Akike para diferentes modelos \\(ARMA(p, q)\\) de la serie \\(DLPaxNal_t\\). Modelo AR(p) MA(p) Akaike (AIC) Óptimo 1 1 1 -475.1190 2 1 2 -473.4397 3 1 3 -483.1239 4 1 4 -482.4932 5 1 5 -506.9880 6 1 6 -533.9411 7 2 1 -473.4758 \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) 24 4 6 -752.8996 * \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) 36 6 6 -752.2632 El cuadro anterior reporta un resumen de los resultados para 36 diferentes modelos, todos incluyen variables exógenas. Como resultado del análisis concluimos que el modelo más adecuado es el 24, el cual considera un \\(ARMA(4, 6)\\), con variables dummies para controlar la estacionalidad de los meses de enero, febrero, julio y diciembre. Más adelante mostramos los resultados del modelo. No obstante, una inspección de los residuales del modelo nos permite sospechar que requiere de incluir un par de dummies más. Ambas, asociadas con la caída del transporte aéreo en 2009, principalmente asociadas con la crisis mundial de ese año. La Figura 3.18 muestra los residuales mencionados. knitr::include_graphics(&quot;Plots/G_Residuals_ARMA46.png&quot;) Figure 3.18: Residuales del ARMA(4, 6) de una serie \\(DLPaxNal_t\\) Una vez incluidas dos dummies más para mayo y junio de 2009, analizamos un total de 36 modelos ARMA y determinamos que el orden que minimiza el criterio de Akaike es un \\(ARMA(4, 6)\\). El siguiente cuadro muestra los resultados para este nuevo modelo. No lo motramos en esta sección, pero ambos modelos reportados tienen raices de sus respectivos polinomios característicos menores a 1 en valor absoluto. En la Figura 3.19 mostramos los residuales ahora ajustados por las dummies de mayo y junio de 2009. Table 3.11: Modelo \\(ARMA(p, q)\\) de la serie \\(DLPaxNal_t\\). Variable Coeficiente Error Est. Coeficiente Error Est. Modelo 1 Modelo 2 Constante 0.0134 0.0033 0.0145 0.0029 \\(DLPaxNal_{t-1}\\) 0.0004 0.0011 0.0014 0.0016 \\(DLPaxNal_{t-2}\\) 0.9997 0.0019 0.9988 0.0024 \\(DLPaxNal_{t-3}\\) 0.0001 0.0012 0.0002 0.0014 \\(DLPaxNal_{t-4}\\) -0.9997 0.0004 -0.9993 0.0009 \\(\\hat{U}_{t-1}\\) -0.2877 0.0670 -0.1472 0.0633 \\(\\hat{U}_{t-2}\\) -1.2028 0.0608 -1.3323 0.0625 \\(\\hat{U}_{t-3}\\) 0.2302 0.0765 0.1201 0.0756 \\(\\hat{U}_{t-4}\\) 1.2085 0.0645 1.3142 0.0642 \\(\\hat{U}_{t-5}\\) -0.2634 0.0708 -0.1034 0.0717 \\(\\hat{U}_{t-6}\\) -0.2171 0.0606 -0.3864 0.0657 \\(DEne_{t}\\) -0.2904 0.0172 -0.2865 0.0156 \\(DFeb_{t}\\) -0.1312 0.0170 -0.1273 0.0140 \\(DJul_{t}\\) 0.3303 0.0173 0.3164 0.0156 \\(Dic_{t}\\) -0.0118 0.0169 -0.0138 0.0137 \\(DMay2009_{t}\\) -0.3378 0.0360 \\(DJun2009_{t}\\) 0.2371 0.0359 \\(\\hat{\\sigma}^2\\) 0.001841 0.001283 AIC -752.9 -835.73 knitr::include_graphics(&quot;Plots/G_Residuals_ARMA46_D.png&quot;) Figure 3.19: Residuales del ARMA(4, 6) de una serie \\(DLPaxNal_t\\) 3.9 Pronósticos Para pronósticar el valor de la serie es necesario determinar cuál es el valor esperado de la serie en un momento \\(t + \\tau\\) condicional en que ésta se comporta como un \\(AR(p)\\), un \\(MA(q)\\) o un \\(ARMA(p, q)\\) y a que los valores antes de \\(t\\) están dados. Por lo que el pronóstico de la serie estará dado por una expresión: \\[\\begin{eqnarray} \\mathbb{E}_t[X_{t+\\tau}] = \\delta + a_1 \\mathbb{E}_t[X_{t+\\tau-1}] + a_2 \\mathbb{E}_t[X_{t+\\tau-2}] + \\ldots + + a_p \\mathbb{E}_t[X_{t+\\tau-p}] \\tag{3.56} \\end{eqnarray}\\] Lo anterior para todo \\(\\tau = 0, 1, 2, \\ldots\\) y considerando que los componentes MA(q) en la ecuación (3.56) son cero dado que para todo valor \\(t + \\tau\\) es cierto que \\(\\mathbb{E}_t[U_{t+\\tau}]\\). Continuando con el ejemplo, en la Figura 3.20 mostramos el resultado del pronóstico de la serie a partir del modelo ARMA(4, 6) que hemos discutido anteriormente. library(ggplot2) library(dplyr) library(readxl) library(stats) Datos &lt;- read_excel(&quot;BD/Base_Transporte_ARIMA.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) Pax_Nal &lt;- ts(Datos$Pax_Nal, start = c(2000, 1), freq = 12) LPax_Nal &lt;- ts(log(Datos$Pax_Nal), start = c(2000, 1), freq = 12) DLPax_Nal &lt;- ts(log(Datos$Pax_Nal) - lag(log(Datos$Pax_Nal), 1), start = c(2000, 1), freq = 12) D_Mar2020 &lt;- ts(Datos$D_Mar2020, start = c(2000, 1), freq = 12) D_Abr2020 &lt;- ts(Datos$D_Abr2020, start = c(2000, 1), freq = 12) D_Jun2020 &lt;- ts(Datos$D_Jun2020, start = c(2000, 1), freq = 12) D_Jul2020 &lt;- ts(Datos$D_Jul2020, start = c(2000, 1), freq = 12) D_Mar2021 &lt;- ts(Datos$D_Mar2021, start = c(2000, 1), freq = 12) D_Ene &lt;- ts(Datos$D_Ene, start = c(2000, 1), freq = 12) D_Feb &lt;- ts(Datos$D_Feb, start = c(2000, 1), freq = 12) D_Jul &lt;- ts(Datos$D_Jul, start = c(2000, 1), freq = 12) D_Dic &lt;- ts(Datos$D_Dic, start = c(2000, 1), freq = 12) ARMA_Ex_DLPax_Nal_2 &lt;- arima(DLPax_Nal, order = c(6, 0, 6), xreg = cbind(D_Ene, D_Feb, D_Jul, D_Dic, D_Mar2020, D_Abr2020, D_Jun2020, D_Jul2020, D_Mar2021), method = &quot;ML&quot;) Predict_Datos &lt;- read_excel(&quot;BD/Predict_Base_Transporte_ARIMA.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) D_Mar2020_f &lt;- ts(Predict_Datos$D_Mar2020, start = c(2022, 7), freq = 12) D_Abr2020_f &lt;- ts(Predict_Datos$D_Abr2020, start = c(2022, 7), freq = 12) D_Jun2020_f &lt;- ts(Predict_Datos$D_Jun2020, start = c(2022, 7), freq = 12) D_Jul2020_f &lt;- ts(Predict_Datos$D_Jul2020, start = c(2022, 7), freq = 12) D_Mar2021_f &lt;- ts(Predict_Datos$D_Mar2021, start = c(2022, 7), freq = 12) D_Ene_f &lt;- ts(Predict_Datos$D_Ene, start = c(2022, 7), freq = 12) D_Feb_f &lt;- ts(Predict_Datos$D_Feb, start = c(2022, 7), freq = 12) D_Jul_f &lt;- ts(Predict_Datos$D_Jul, start = c(2022, 7), freq = 12) D_Dic_f &lt;- ts(Predict_Datos$D_Dic, start = c(2022, 7), freq = 12) DLPax_Nal_f &lt;- predict(ARMA_Ex_DLPax_Nal_2, n.ahead = 24, newxreg = cbind(D_Ene_f, D_Feb_f, D_Jul_f, D_Dic_f, D_Mar2020_f, D_Abr2020_f, D_Jun2020_f, D_Jul2020_f, D_Mar2021_f)) Pronostico_Arima &lt;- read_excel(&quot;BD/Pronostico_Arima.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) Pronostico_Arima$Pax_Nal_f &lt;- Pronostico_Arima$Pax_Nal for(i in 1:24){ Pronostico_Arima$Pax_Nal_f[281 + i] &lt;- Pronostico_Arima$Pax_Nal_f[281 + i - 1]*(1 + DLPax_Nal_f$pred[i]) } options(scipen = 999) # NO notacion cientifica ggplot(data = Pronostico_Arima, aes(x = Periodo)) + geom_line(aes(y = Pax_Nal_f, color = &quot;Pax_Nal_f&quot;)) + geom_line(aes(y = Pax_Nal, color = &quot;Pax_Nal&quot;)) + scale_color_brewer(type = &quot;qual&quot;, palette = 2) + #theme_bw() + theme(legend.position = &quot;bottom&quot;) + theme(legend.title = element_blank()) + guides(col = guide_legend(nrow = 1, byrow = TRUE)) + xlab(&quot;Tiempo&quot;) + ylab(&quot;Pasajeros&quot;) + theme(plot.title = element_text(size = 11, face = &quot;bold&quot;, hjust = 0)) + theme(plot.subtitle = element_text(size = 10, hjust = 0)) + theme(plot.caption = element_text(size = 10, hjust = 0)) + theme(plot.margin = unit(c(1,1,1,1), &quot;cm&quot;)) + labs( title = &quot;Pasajeros en vuelos nacionales (Salidas)&quot;, subtitle = &quot;(Ene-2000 a Jun-2019)&quot;, caption = &quot;Fuente: Elaboración propia&quot; ) Figure 3.20: Pronóstico de la serie \\(PaxNAl_t\\) a partir de una ARMA(4,6) en diferencias logaritmicas ggsave(&quot;Pax_Nal_f.png&quot;, width = 20, height = 15, units = &quot;cm&quot;) 3.10 Desestacionalización y filtrado de Series 3.11 Motivación Existen múltiples enfoques para la desestacionalización de series. Algunos modelos, por ejemplo, pueden estar basados en modelos ARIMA como un conjunto de dummies. No obstante, en el caso particular que discutiremos en este curso, estará basado en un modelo ARIMA de la serie. Este enfoque está basado en el modelo X11 de la oficina del censo de Estados Unidos (Census Bureau) el cual es conocido como el modelo X13-ARIMA-SEATS. El modelo X13-ARIMA-SEATS es, como su nombre lo indica, la combinación de un modelo ARIMA con componentes estacionales (por la traducción literal de la palabra: seasonal). Un modelo ARIMA estacional emplea la serie en diferencias y como regresores los valores rezagados de las diferencias de la serie tantas veces como procesos estacionales \\(s\\) existan en ésta, con el objeto de remover los efectos aditivos de la estacionalidad. Sólo para entender qué significa este mecanismo, recordemos que cuando se utiliza la primera diferencia de la serie respecto del periodo inmediato anterior se remueve la tendencia. Por su parte, cuando se incluye la diferencia respecto del mes \\(s\\) se está en el caso en que se modela la serie como una media móvil en términos del rezago \\(s\\). El modelo ARIMA estacional incluye como componentes autorregresivos y de medias móviles a los valores rezagados de la serie en el periodo \\(s\\) en diferencias. El ARIMA(p, d, q)(P, D, Q) estacional puede ser expresado de la siguiente manera utilizando el operador rezago \\(L\\): \\[\\begin{equation} \\Theta_P(L^s) \\theta_p(L) (1 - L^s)^D (1 - L)^d X_t = \\Psi_Q(L^s) \\psi_q(L) U_t (\\#eq:ARIMA_seas) \\end{equation}\\] Donde \\(\\Theta_P(.)\\), \\(\\theta_p(.)\\), \\(\\Psi_Q(.)\\) y \\(\\psi_q(.)\\) son polinomios de \\(L\\) de orden \\(P\\), \\(p\\), \\(Q\\) y \\(q\\) respectivamente. En general, la representación es de una serie no estacionaria, aunque si \\(D = d = 0\\) y las raíces del polinomio característico (de los polinomios del lado izquierdo de la ecuación @ref(eq:ARIMA_seas) todas son más grandes que 1 en valor absoluto, el proceso modelado será estacionario. Si bien es cierto que existen otras formas de modelar la desestacionalización, como la modelación en diferencias con dummies para identificar ciertos patrones regulares, en los algoritmos disponibles como el X11 o X13-ARIMA-SEATS se emplea la formulación de la ecuación @ref(eq:ARIMA_seas). A continuación, implementaremos la desestacionalización de una serie. Como ejemplo utilizaremos la serie del Índice Nacional de Precios al Consumidor (INPC). Podemos ver que la serie original del INPC y su ajuste estacional bajo una metodología X13-ARIMA-SEATS son como se muestra en la Figura 3.21. library(ggplot2) library(dplyr) library(readxl) library(stats) library(seasonal) ## ## Attaching package: &#39;seasonal&#39; ## The following object is masked from &#39;package:tibble&#39;: ## ## view library(seasonalview) ## ## Attaching package: &#39;seasonalview&#39; ## The following object is masked from &#39;package:seasonal&#39;: ## ## view ## The following object is masked from &#39;package:tibble&#39;: ## ## view library(shiny) Datos &lt;- read_excel(&quot;BD/Base_VAR.xlsx&quot;, sheet = &quot;Datos&quot;, col_names = TRUE) INPC &lt;- ts(Datos$INPC, start = c(2000, 1), freq = 12) Seas_INPC &lt;- seas(INPC) INPC_Ad &lt;- final(Seas_INPC) #names(Seas_INPC) #summary(Seas_INPC) plot(Seas_INPC) Figure 3.21: Índice Nacional de Precios al Consumidor (\\(INPC_t\\)) y su serie desestacionalizada utilizando un proceso X13-ARIMA-SEATS # Forma Facil: #view(Seas_INPC) TC &lt;- ts(Datos$TC, start = c(2000, 1), freq = 12) Seas_TC &lt;- seas(TC) TC_Ad &lt;- final(Seas_TC) CETE28 &lt;- ts(Datos$CETE28, start = c(2000, 1), freq = 12) Seas_CETE28 &lt;- seas(CETE28) CETE28_Ad &lt;- final(Seas_CETE28) IGAE &lt;- ts(Datos$IGAE, start = c(2000, 1), freq = 12) Seas_IGAE &lt;- seas(IGAE) IGAE_Ad &lt;- final(Seas_IGAE) IPI &lt;- ts(Datos$IPI, start = c(2000, 1), freq = 12) Seas_IPI &lt;- seas(IPI) IPI_Ad &lt;- final(Seas_IPI) Datos_Ad &lt;- data.frame(cbind(INPC_Ad, TC_Ad, CETE28_Ad, IGAE_Ad, IPI_Ad)) Datos_Ad &lt;- cbind(Datos, Datos_Ad) save(Datos_Ad, file = &quot;BD/Datos_Ad0.RData&quot;) El mismo procesamiento puede ser seguido para todas las series que busquemos analizar. En particular, en adelante, además del INPC que incluimos en la lista, utilizaremos las siguientes series, así como su versión desestacionalizada: 3.12 Filtro Hodrick-Prescott Como último tema de los procesos univariados y que no necesariamente aplican a series estacionarias, a continuación desarrollaremos el procedimiento conocido como filtro de Hodrick y Prescott (1997). El trabajo de estos autores era determinar una técnica de regresión que permitiera utilizar series agregadas o macroeconómicas para separarlas en dos componentes: uno de ciclo de negocios y otro de tendencia. En su trabajo original, Hodrick y Prescott (1997) utilizaron datos trimestrales de algunas series como el Gross Domestic Product (GNP, por sus siglas en Inglés), los agregados monetarios M1, empleo, etc., de los Estados Unidos que fueron observados posteriormente a la Segunda Guerra Mundial. El marco conceptual de Hodrick y Prescott (1997) parte de suponer una serie \\(X_t\\) que se puede descomponer en la suma de componente de crecimiento tendencial, \\(g_t\\), y su componente de ciclio de negocios, \\(c_t\\), de esta forma para \\(t = 1, 2, \\ldots, T\\) tenemos que: \\[\\begin{equation} X_t = g_t + c_t (\\#eq:HP_Eq) \\end{equation}\\] En la ecuación @ref(eq:HP_Eq) se asume que el ajuste de la ruta seguida por \\(g_t\\) es resultado de la suma de los cuadrados de su segunda diferencia. En esa misma ecuación asumiremos que \\(c_t\\) son las desviaciones de \\(g_t\\), las cuales en el largo plazo tienen una media igual a cero (0). Por esta razón, se suele decir que el filtro de Hodrick y Prescott representa una descomposición de la serie en su componente de crecimiento natural y de sus desviaciones transitorias que en promedio son cero, en el largo plazo. Estas consideraciones que hemos mencionado señalan que el procesimiento de Hodrick y Prescott (1997) implican resolver el siguiente problema minimización para determinar cada uno de los componentes en que \\(X_t\\) se puede descomponer: \\[\\begin{equation} \\min_{\\{ g_t \\}^T_{t = -1} } \\left[ \\sum^T_{t = 1} c^2_t + \\lambda \\sum^T_{t = 1} [ \\Delta g_t - \\Delta g_{t-1}]^2 \\right] \\end{equation}\\] Donde \\(\\Delta g_t = g_t - g_{t-1}\\) y \\(\\Delta g_{t-1} = g_{t-1} - g_{t-2}\\); \\(c_t = X_t - g_t\\), y el parámetro \\(\\lambda\\) es un número positivo que penaliza la variabilidad en el crecimiento de las series. De acuerdo con el trabajo de Hodrick y Prescott (1997) la constante \\(\\lambda\\) debe tomar valores específicos de acuerdo con la periodicidad de las series. Así, \\(\\lambda\\) será: En resumen, podemos decir que el filtro de Hodrick y Prescott (1997) es un algoritmo que mimiza las distancias o variaciones de la trayectoria de largo plazo. De esta forma, determina una trayectoria estable de largo plazo, por lo que las desviaciones respecto de esta trayectoria serán componentes de ciclos de negocio o cambios transitorios (tanto positivos como negativos). A contiuación, ilustraremos el filtro de Hodrick y Prescott (1997) para dos series desestacionalizadas: \\(INPC_t\\) y \\(TC_t\\). Las Figura 3.22 y Figura 3.23 muestran los resultados de la implementación del filtro. library(ggplot2) library(dplyr) library(readxl) library(stats) library(mFilter) library(plm) ## ## Attaching package: &#39;plm&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, lag, lead load(&quot;BD/Datos_Ad0.RData&quot;) INPC &lt;- ts(Datos_Ad$INPC_Ad, start = c(2000, 1), freq = 12) TC &lt;- ts(Datos_Ad$TC_Ad, start = c(2000, 1), freq = 12) INPC_hpf &lt;- hpfilter(INPC, freq = 14400) #plot(INPC_hpf) knitr::include_graphics(&quot;Plots/G_INPC_HP.png&quot;) Figure 3.22: Descomposición del Índice Nacional de Precios al Consumidor (\\(INPC_t\\)) en su tendencia o trayectoria de largo plazo y su ciclo de corto plazo utilizando un filtro Hodrick y Prescott (1997) TC_hpf &lt;- hpfilter(TC, freq = 14400) #plot(TC_hpf) knitr::include_graphics(&quot;Plots/G_TC_HP.png&quot;) Figure 3.23: Descomposición del Tipo de Cambio FIX (\\(TC_t\\)) en su tendencia o trayectoria de largo plazo y su ciclo de corto plazo utilizando un filtro Hodrick y Prescott (1997) 3.12.1 Filtro Hodrick-Prescott planteado por St-Amant &amp; van Norden Existen una técnica adicional que soluciona un problema de distribuciones de colas pesadas es el planteado por St-Amant &amp; van Norden. Analicemos ese caso. Método tradicional de HP consiste en minimizar la serie \\(\\{ \\tau_t \\}_{t=-1}^T\\): \\[\\begin{equation} \\sum_{t=1}^T (y_t - \\tau_t)^2 + \\lambda \\sum_{t=1}^{T} [(\\tau_{t} - \\tau_{t-1}) - (\\tau_{t-1} - \\tau_{t-2})]^2 \\end{equation}\\] Donde \\(\\lambda\\) es un parámetro fijo (determinado ex-ante) y \\(\\tau_t\\) es un componente de tendencia de \\(y_t\\). Sin pérdida de generalidad, asumiremos que \\(\\tau_{-1}\\) y \\(\\tau_{0}\\) son cero (0). De esta manera, la forma matricial del filtro HP es: \\[\\begin{equation} (Y - G)&#39;(Y - G) + \\lambda G&#39; K&#39; K G \\end{equation}\\] La derivada de los anteriores: \\[\\begin{equation} -2 Y + 2 G + \\lambda 2 K&#39; K G = 0 \\end{equation}\\] Despejando: \\[\\begin{equation} G_{hp} = [I_T + \\lambda K&#39; K]^{-1} Y \\end{equation}\\] Donde \\(G\\) es el vector de tendencia, \\(Y\\) es el vector de la serie de datos, \\(\\lambda\\) es la constante tradicional, y \\(K\\) es de dimensión \\(T \\times T\\) y está dada por la expresión: \\[\\begin{equation} K = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ -2 &amp; 1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 1 &amp; -2 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\\\ \\end{pmatrix} \\end{equation}\\] Así: \\[\\begin{equation} K&#39; = \\begin{pmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\\\ \\end{pmatrix} \\end{equation}\\] Método modificado de HP consiste en minimizar los valores de la serie \\(\\{ \\tau_t \\}_{t=1}^T\\): \\[\\begin{equation} \\sum_{t=1}^T (y_t - \\tau_t)^2 + \\lambda \\sum_{t=2}^{T-1} [(\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1})]^2 + \\lambda_{ss} \\sum_{t=T-j}^{T} [\\Delta \\tau_t - u_{ss}] \\end{equation}\\] Donde \\(\\lambda\\) es una parámetro fijo (determinado ex-ante), \\(\\tau_t\\) es un componente de tendencia de \\(y_t\\), y los nuevos parámetros son \\(u_{ss}\\) y \\(\\lambda_{ss}\\) ajustadas por el procedimiento de Marcet y Ravn (2004). Este procedimiento asume que parte del filtro HP y que esta versión tiene el problema de pérdida de información al final y al principio de la muestra. La razón es que es un procedimeinto univariado que requiere de mucha información futura y pasada para mejorar el ajuste. El compoenente adicional al filtro HP es un componente de castigo por desviaciones de la tasa de crecimiento de largo plazo, \\(u_{ss}\\). El proceso de selección de \\(\\lambda_{ss}\\) es e propuesto por Marcet y Ravn (2004), el cual consiste en utilizar un \\(\\lambda\\) convencional y el filtro HP convencional para estimar la siguiente función: \\[\\begin{equation} F(\\lambda) = \\frac{\\sum_{t=2}^{T-1} ((\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1}))^2}{\\sum_{t=1}^T (y_t - \\tau_t)^2} \\end{equation}\\] Entonces el valor de \\(\\lambda_{ss}\\) será aquel que: \\[\\begin{equation} F(\\lambda_{ss}) = \\frac{\\sum_{t=2}^{T-1} ((\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1}))^2}{\\sum_{t=1}^T (y_t - \\tau_t)^2} = F(\\lambda) \\end{equation}\\] Nota: Antón (2009) estimó \\(\\lambda_{ss} = 1096\\) para datos trimestrales del PIB. La forma matricial del filtro HP-SAVN es: \\[\\begin{equation} (Y - G)&#39;(Y - G) + \\lambda G&#39; K&#39; K G + \\lambda_{ss} (L^j G + \\overline{u}_{ss} M^j) \\end{equation}\\] Donde \\(L^j = (0, 0, \\ldots, 0, -1, 0, \\ldots, 0, 1)\\), en el cual el valor \\(-1\\) es en la posición \\(T-j-1\\)-ésima, y \\(M^j\\) es un vector que toma valores de cero hasta antes de \\(T-j\\) y de 1 después. La derivada de los anteriores: \\[\\begin{equation} -2 Y + 2 G + \\lambda 2 K&#39; K G + \\lambda_{ss} L&#39;^j = 0 \\end{equation}\\] Despejando: \\[\\begin{equation} G_{SAVN} = \\frac{1}{2} [I_T + \\lambda K&#39; K]^{-1} (2 Y - \\lambda_{ss} L&#39;^j) \\end{equation}\\] \\[\\begin{equation} G_{SAVN} = [I_T + \\lambda K&#39; K]^{-1} Y - \\frac{1}{2} [I_T + \\lambda K&#39; K]^{-1} \\lambda_{ss} L&#39;^j \\end{equation}\\] Donde \\(G\\) es el vector de tendencia, \\(Y\\) es el vector de la serie de datos, \\(\\lambda\\) es la constante tradicional, y \\(K\\) es de dimensión \\(T \\times T\\) y está dada por la expresión: \\[\\begin{equation} K&#39; = \\begin{pmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 \\\\ \\end{pmatrix} \\end{equation}\\] Dicho lo anterior, podemos modificar \\(F(\\lambda)\\) para el filtro HP convencional como en forma matricial: \\[\\begin{equation} F(\\lambda) = \\frac{G&#39; K&#39; K G}{(Y - G)&#39;(Y - G)} \\end{equation}\\] Los datos y algoritmo está disponible en el repositorio de GitHub y corresponde a la Clase 3.↩︎ "],["procesos-no-estacionarios-univariados.html", "Chapter 4 Procesos No Estacionarios Univariados 4.1 Definición y formas de No Estacionariedad 4.2 Pruebas de Raíces Unitarias", " Chapter 4 Procesos No Estacionarios Univariados 4.1 Definición y formas de No Estacionariedad Hasta ahora hemos planteado una serie de técnicas de regresión (AR, MA, ARMA, ARIMA, ARIMAX, etc.) que aplican sólo a procesos o series estacionarias. Sin embargo, no hemos proporcionado una prueba estadística para determinar si una serie es estacionaria. Ese es uno de los objetivos de esta sección. Adicionalmente, en esta sección relajaremos la definición de estacionariedad y plantearemos pruebas para determinar cuándo una serie es estacionaria bajo tres diferentes especificaciones: Estacionariedad alrededor de una tendencia determinística; Estacionariedad alrededor de una media, y Estacionariedad alrededor del cero. A continuación, discutiremos cómo es posible que una serie sea estacionaria al rededor de una tendencia determinística. Diremos que una tendencia es determinística si ésta puede ser aproximada o modelada por un polinomio en función de \\(t\\), la cual incluye posibles transformaciones logarítmicas. Bajo este enfoque, el proceso está lejos de cumplir con la definición de estacionariedad que hemos establecido en capítulos previos, pero relajaremos el supuesto y reconoceremos que una serie puede ser estacionaria en varianza bajo una tendencia determinística. Así, diremos que la serie será descrita por una ecuación dada por: \\[\\begin{equation} Y_t = \\sum^m_{j = 0} \\delta_j t^j + X_t \\tag{4.1} \\end{equation}\\] Donde \\(X_t\\) es un proceso \\(ARMA(p, q)\\) con media cero, que se puede ver como: \\[\\begin{equation} \\alpha(L) X_t = \\beta(L) U_t \\end{equation}\\] Entonces, los momentos y variaza de la ecuación (4.1) estarán dados por: \\[\\begin{equation} \\mathbb{E}[Y_t] = \\sum^m_{j = 0} \\delta_j t^j = \\mu_t \\tag{4.2} \\end{equation}\\] Dada la ecuación (4.2) podemos plantear la siguiente ecuación de covarianzas: \\[\\begin{eqnarray} \\mathbb{E}[(Y_t - \\mu_t) \\cdot (Y_{t+\\tau} - \\mu_{t+\\tau})] &amp; = &amp; \\mathbb{E}[X_t \\cdot X_{t+\\tau}] \\nonumber \\\\ &amp; = &amp; \\gamma_X(\\tau) \\tag{4.3} \\end{eqnarray}\\] Utilizando el resultado de la ecuación (4.3) podemos establecer que: \\[\\begin{equation} \\mathbb{E}[(Y_t - \\mu_t)^2] = \\mathbb{E}[X_t^2] = \\sigma_X^2 \\tag{4.4} \\end{equation}\\] Así, las ecuaciones (4.2) y (4.4) significan que el proceso descrito por la ecuación (4.1) es estacionario pero en varianza. De esta forma, a partir de este momento diremos que una serie será estacionaria al rededor de una tendencia determinística si cumple con las condiciones establecidas en las ecuaciones (4.1), (4.2) y (4.4). Dicho lo anterior, estudiaremos el concepto de raíz unitaria de un proceso estocástico o de una serie de tiempo. Partamos de plantear que un proceso AR(1) tiene raíz unitaria cuando el coeficiente \\(a_1 = 1\\), es decir: \\[\\begin{equation} Y_t = Y_{t-1} + U_t \\tag{4.5} \\end{equation}\\] Donde \\(U_t\\) es un proceso puramente aleatorio con media cero, varianza constante y autocovarianza cero (0), al cual nos referiremos simplemente como ruido blanco. Supongamos ahora que incluimos un término constante en la ecuación (4.5), de forma que tenemos: \\[\\begin{equation} Y_t = \\delta + Y_{t-1} + U_t \\tag{4.6} \\end{equation}\\] Tomando a la ecuación (4.6) y suponiendo que existe un valor inicial \\(Y_0\\) de la serie, podemos plantear la siguiente secuencia de expresiones: \\[\\begin{eqnarray*} Y_1 &amp; = &amp; \\delta + Y_0 + U_1 \\\\ Y_2 &amp; = &amp; \\delta + Y_1 + U_2 \\\\ &amp; = &amp; \\delta + (\\delta + Y_0 + U_1) + U_2 \\\\ &amp; = &amp; 2 \\times \\delta + Y_0 + U_1 + U_2 \\end{eqnarray*}\\] Si repitieramos la sustitución sucesiva anterior hasta el momento \\(t\\) encontrariamos que la ecuación de la solución general que describe a un \\(AR(1)\\) con término constante que tiene raíz unitaria es de la forma: \\[\\begin{equation} Y_t = t \\delta + Y_0 + \\sum_{i=1}^t U_i \\tag{4.7} \\end{equation}\\] La ecuación (4.7) es equivalente a la ecuación (4.1). A la ecuación (4.7) también se le conoce como proceso con Drift o con término constante, indistintamente, ya que el componente de Drift suele asociarse a la posibilidad de incorporar el efecto de los residuales pasados, lo cual es posible simplemente agregando una constante.s Si revisamos el comportamiento de sus momentos y varianza de la ecuación (4.7) encontramos que: \\[\\begin{eqnarray*} \\mathbb{E}[Y_t] &amp; = &amp; Y_0 + \\delta t = \\mu_t \\\\ Var[Y_t] &amp; = &amp; t \\sigma^2 = \\gamma(0, t) \\\\ Cov(Y_t, Y_{t+\\tau}) &amp; = &amp; (t - \\tau) \\sigma^2 = \\gamma(t, \\tau) \\end{eqnarray*}\\] De esta forma, la ecuación (4.7) no describe un proceso estacionario, sólo podría ser estacionario si \\(t = 1\\), en cualquier otro caso sería no estacionario en varianza. Ahora hagamos un resumen y acordemos notación que se utilizará en esta sección. Supongamos un proceso o serie de tiempo que es descrito por la siguiente ecuación: \\[\\begin{equation} Y_t = \\delta + Y_{t-1} + X_t \\tag{4.8} \\end{equation}\\] Donde \\(X_t\\) es un \\(ARMA(p, q)\\) con media cero. Si definimos a \\(\\Delta Y_t = Y_t - Y_{t-1}\\), entonces la ecuación (4.8) la podemos escribir como: \\[\\begin{equation} \\Delta Y_t = \\delta + X_t \\tag{4.9} \\end{equation}\\] A la ecuación (4.9) la denominaremos como un proceso estacionario en diferencias o simplemente como un proceso integrado. Así, utilizaremos la siguiente definición. Sea un proceso estocástico \\(Y\\), decimos que es un \\(d\\), \\(I(d)\\), si este puede transformarse a uno estacionario, que sea invertible, mediante la diferenciación del mismo \\(d\\)-veces, es decir: \\[\\begin{equation} (1 - L)^d Y_t = \\delta + X_t \\tag{4.10} \\end{equation}\\] Donde \\(X_t\\) es un proceso \\(ARMA(p, q)\\). De lo cual se infiere que en la ecuación (4.10) \\(Y_t\\) será una \\(ARIMA(p, d, q)\\), el cual contiene \\(d\\) raíces unitarias. A estos procesos también se les conoce como procesos con tendencia estocástica. Dada la discusión anterior, a continuación, plantearemos un resumen de cuáles son los dos casos a los cuales nos referiremos como procesos que no son estacionarios en media, pero que sí lo son en varianza. Estos casos son: \\[\\begin{eqnarray} Y_t &amp; = &amp; Y_0 + \\delta t + U_t \\\\ Y_t &amp; = &amp; Y_0 + \\delta + \\sum_{i = 1}^t U_t \\end{eqnarray}\\] Ambos casos no son estacionarios en media, pero sí lo son en varianza. De ambos podemos decir que los choques o innovaciones del término de error tienen un efecto transitorio en el primero, pero permanentes en el segundo. 4.2 Pruebas de Raíces Unitarias En esta sección plantearemos una serie de pruebas estadísticas para determinar cuándo una serie puede ser estacionaria bajo tres posibles casos: Estacionariedad alrededor de una tendencia determinística; Estacionariedad alrededor de una media, y Estacionariedad alrededor del cero. A continuación, mostraremos como ejemplo la aplicación de las pruebas de raíces unitarias a la serie de Tipo de Cambio en forma logarítmica y de diferencias logarítmicas. Asumamos que determinamos el valor de los rezagos de la prueba con el criterio de \\(p = int\\{ 4 (T/100)^{1/4} \\}\\). En la práctica, existen otras formas de determinar el valor de \\(p\\), como el criterio de AIC, pero es decisión del investigador cuál usar. library(ggplot2) library(dplyr) library(stats) library(MASS) library(strucchange) library(zoo) library(sandwich) library(urca) library(lmtest) library(vars) # load(&quot;BD/Datos_Ad.RData&quot;) # ## Conversion a series de tiempo: Datos &lt;- ts(Datos_Ad[7: 11], start = c(2000, 1), freq = 12) LDatos &lt;- log(Datos) DLDatos &lt;- diff(log(Datos, base = exp(1)), lag = 1, differences = 1) DaLDatos &lt;- diff(log(Datos, base = exp(1)), lag = 12, differences = 1) Como primer paso, exploraremos las series en niveles (bajo una transformación logarítmica), como en la Figura 4.1, y en diferencias logarítmicas, como en la Figura 4.2. De dicha exploración, podemos concluir que las series parecen estacionarias en diferencias, pero no en niveles. plot(LDatos, plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), main = &quot;Series en Logaritmos&quot;, xlab = &quot;Tiempo&quot;) Figure 4.1: Series en Logaritmos plot(DLDatos, plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), main = &quot;Series en Diferencias Logaritmicas&quot;, xlab = &quot;Tiempo&quot;) Figure 4.2: Series en Diferencias Logaritmicas plot(DaLDatos, plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), main = &quot;Series en Diferencias Anuales Logaritmicas&quot;, xlab = &quot;Tiempo&quot;) Figure 4.3: Series en Diferencias Anuales Logaritmicas plot(cbind(DLDatos, DaLDatos), plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), main = &quot;Comparacion de Series en Diferencias&quot;, xlab = &quot;Tiempo&quot;) Figure 4.4: Comparacion de Series en Diferencias Anuales plot(cbind(LDatos, DLDatos), plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), main = &quot;Comparacion de Series en Diferencias&quot;, xlab = &quot;Tiempo&quot;) Figure 4.5: Comparacion de Series en Diferencias 4.2.1 Dickey - Fuller (DF) Partamos de una forma del proceso \\(Y_t\\) dada por: \\[\\begin{equation} Y_t = \\sum_{j = 0}^m \\delta_j t^j + X_t \\tag{4.11} \\end{equation}\\] Donde \\(X_t\\) es un \\(ARMA(p, q)\\) con media cero. Esta prueba asume que \\(m = 1\\), por lo que utilizaremos un modelo del tipo: \\[\\begin{equation} Y_t = \\alpha + \\delta t + \\rho Y_{t-1} + U_t \\tag{4.12} \\end{equation}\\] Si, el \\(AR(1)\\) planteado tiene raíz unitaria, es decir, \\(\\rho = 1\\), entonces tendríamos: \\[\\begin{eqnarray*} Y_t &amp; = &amp; \\alpha + \\delta t + Y_{t-1} + U_t \\\\ \\Delta Y_t &amp; = &amp; \\alpha + \\delta t + U_t \\end{eqnarray*}\\] De esta forma, para determinar si una serie tiene raíz unitaria basta con probar la hipótesis nula de que \\(H_0 : \\rho = 1\\), junto con las diferentes combinaciones que impliquen restricciones respecto a \\(\\delta\\) y \\(\\alpha\\). En resumen, la prueba DF consiste en asumir un modelo general dado por la ecuación (4.12) y probar tres especificaciones distintas que serían válidas bajo \\(H_0 : \\rho = 1\\): Modelo A: con intercepto y tendencia: \\[\\begin{equation*} \\Delta Y_t = \\alpha + \\delta t + \\beta Y_{t-1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta &lt; 0\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario alrededor de una tendencia determinística. Modelo B: con intercepto: \\[\\begin{equation*} \\Delta Y_t = \\alpha + \\beta Y_{t-1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta &lt; 0\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario alrededor de una constante. Modelo C: sin intercepto y tendencia: \\[\\begin{equation*} \\Delta Y_t = \\beta Y_{t-1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta &lt; 0\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario sin considerar una constante o una tendencia determinística, es decir, es un proceso puramente aleatorio. . En los siguientes mostramos los resultados de aplicar las pruebas de raíces unitarias a la serie \\(LTC_t\\), considerando \\(p = int \\{ 4 (234/100)^{(1/4)} \\} = 4\\) para aquellas pruebas que requieren de rezagos: ADF, PP y KPSS. La convención es implementar las pruebas de raíces unitarias iniciando con el modelo más general hasta llegar al más restringido. Es decir, iniciar con el Modelo A, pasar al Modelo B y cerrar con el Modelo C. Iniciaremos implementando una prueba de DF simple (o una ADF sin rezagos). En este caso obtenemos los resultados mostrados en la siguiente Tabla. En esta se reportan los resultados de la prueba para cada uno de los 3 modelos (A, B y C) y los valores críticos (CV, por sus siglas en inglés) de la prueba para los niveles 1%, 5% y 10%–estos valores son los reportados por MacKinnon (1996) para una prueba con distribución de 1 cola (izquierda). La convención es utilizar el nivel del 5% de significancia. Los resultados deben leerse de la siguiente forma. La siguiente Tabla muestra los resultados de la prueba para la serie en niveles, \\(LTC_t\\), y en diferencias \\(\\Delta LTC_t\\). De esta forma, podremos comparar si la serie es estacionaria en niveles o en diferencias. La manera de determinarlo es comparar el estadístico con el valor crítico. Así, rechazaremos la hipótesis nula en los casos en que el estadístico \\(t\\) se ubique por encima del valor crítico. En este caso, podemos observar que la serie se vuelve estacionaria hasta su primera diferencia–caso en el que rechazamos la hipótesis nula en todos los modelos. Table 4.1: Resumen de resultados de la prueba Dickey-Fuller (DF). Model Estadístico 1% CV 5% CV 10% CV LTC_t Modelo A \\(t=-2.0752\\) \\(-3.98\\) \\(-3.42\\) \\(-3.13\\) Modelo B \\(t=-1.1724\\) \\(-3.44\\) \\(-2.87\\) \\(-2.57\\) Modelo C \\(t=1.1815\\) \\(-2.58\\) \\(-1.95\\) \\(-1.62\\) \\(\\Delta LTC_t\\) Modelo A \\(t=-12.6805\\) \\(-3.98\\) \\(-3.42\\) \\(-3.13\\) Modelo B \\(t=-12.6899\\) \\(-3.44\\) \\(-2.87\\) \\(-2.57\\) Modelo C \\(t=-12.6515\\) \\(-2.58\\) \\(-1.95\\) \\(-1.62\\) ## Dickey-Fuller: ### NIVELES: Tipo de cambio summary(ur.df(LDatos[, 2], type = &quot;trend&quot;, lags = 0)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression trend ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + tt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.083973 -0.015415 -0.002526 0.009332 0.170645 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08700923 0.04029878 2.159 0.0317 * ## z.lag.1 -0.03844435 0.01852595 -2.075 0.0389 * ## tt 0.00011203 0.00006246 1.794 0.0740 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02705 on 278 degrees of freedom ## Multiple R-squared: 0.01629, Adjusted R-squared: 0.00921 ## F-statistic: 2.301 on 2 and 278 DF, p-value: 0.102 ## ## ## Value of test-statistic is: -2.0752 2.1124 2.3013 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau3 -3.98 -3.42 -3.13 ## phi2 6.15 4.71 4.05 ## phi3 8.34 6.30 5.36 summary(ur.df(LDatos[, 2], type = &quot;drift&quot;, lags = 0)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression drift ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.089442 -0.014571 -0.002497 0.009620 0.171964 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.020316 0.015600 1.302 0.194 ## z.lag.1 -0.006946 0.005925 -1.172 0.242 ## ## Residual standard error: 0.02716 on 279 degrees of freedom ## Multiple R-squared: 0.004903, Adjusted R-squared: 0.001336 ## F-statistic: 1.375 on 1 and 279 DF, p-value: 0.242 ## ## ## Value of test-statistic is: -1.1724 1.5477 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau2 -3.44 -2.87 -2.57 ## phi1 6.47 4.61 3.79 summary(ur.df(LDatos[, 2], type = &quot;none&quot;, lags = 0)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression none ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.089736 -0.014701 -0.003293 0.010359 0.169748 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## z.lag.1 0.0007280 0.0006161 1.181 0.238 ## ## Residual standard error: 0.0272 on 280 degrees of freedom ## Multiple R-squared: 0.004961, Adjusted R-squared: 0.001407 ## F-statistic: 1.396 on 1 and 280 DF, p-value: 0.2384 ## ## ## Value of test-statistic is: 1.1815 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau1 -2.58 -1.95 -1.62 ### DIFERENCIAS: Tipo de cambio summary(ur.df(DLDatos[, 2], type = &quot;trend&quot;, lags = 0)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression trend ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + tt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09095 -0.01306 -0.00158 0.01055 0.17069 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.002881589 0.003163345 0.911 0.363 ## z.lag.1 -0.736373275 0.058071315 -12.680 &lt;0.0000000000000002 *** ## tt -0.000009351 0.000019483 -0.480 0.632 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02634 on 277 degrees of freedom ## Multiple R-squared: 0.3673, Adjusted R-squared: 0.3628 ## F-statistic: 80.41 on 2 and 277 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic is: -12.6805 53.6071 80.4095 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau3 -3.98 -3.42 -3.13 ## phi2 6.15 4.71 4.05 ## phi3 8.34 6.30 5.36 summary(ur.df(DLDatos[, 2], type = &quot;drift&quot;, lags = 0)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression drift ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.090664 -0.013492 -0.001331 0.010702 0.169754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.001566 0.001577 0.993 0.322 ## z.lag.1 -0.735658 0.057972 -12.690 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02631 on 278 degrees of freedom ## Multiple R-squared: 0.3668, Adjusted R-squared: 0.3645 ## F-statistic: 161 on 1 and 278 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic is: -12.6899 80.5185 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau2 -3.44 -2.87 -2.57 ## phi1 6.47 4.61 3.79 summary(ur.df(DLDatos[, 2], type = &quot;none&quot;, lags = 0)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression none ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.089121 -0.011916 0.000311 0.012192 0.171310 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## z.lag.1 -0.73093 0.05777 -12.65 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02631 on 279 degrees of freedom ## Multiple R-squared: 0.3646, Adjusted R-squared: 0.3623 ## F-statistic: 160.1 on 1 and 279 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic is: -12.6515 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau1 -2.58 -1.95 -1.62 4.2.2 Dickey - Fuller Aumentada (ADF) A diferencia de un modelo AR(1) para el caso de una prueba DF como en la ecuación (4.12), en una prueba ADF se asume que el proceso es un AR(p) de la forma (por simplicidad, hemos omitido el término constante y el término de tendencia determinística): \\[\\begin{equation} Y_t = a_1 Y_{t-1} + a_2 Y_{t-2} + \\ldots + a_p Y_{t-p} + U_t \\tag{4.13} \\end{equation}\\] Haciendo una sustitución de términos similar a las que hemos planteado en otras secciones, podemos reexpresar la ecuación (4.13) en su versión en diferencias siguiendo el proceso: \\[\\begin{equation} Y_t = \\rho Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t \\end{equation}\\] Donde \\(\\rho = \\theta_0 = \\sum_{j = 1}^p a_j\\), \\(\\theta_i = - \\sum_{j = i + 1}^p a_j\\), \\(i = 1, 2, \\ldots, p-1\\). Así, si el proceso AR(p) tiene raíz unitaria, entonces, veremos que: \\[\\begin{eqnarray*} 1 - a_1 - a_2 - \\ldots - a_p &amp; = &amp; 0 \\\\ \\rho &amp; = &amp; 1 \\end{eqnarray*}\\] De donde podemos establecer que el modelo general de una prueba ADF será: \\[\\begin{equation} \\Delta Y_{t-1} = \\alpha + \\beta t + (\\rho - 1) Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_k \\Delta Y_{t-k} + U_t \\end{equation}\\] Donde \\(U_t\\) es un proceso puramente aleatorio y \\(k\\) es elegido de tal manera que los residuales sean un proceso puramente aleatorio. En resumen, la prueba ADF consiste en asumir un modelo general dado por la ecuación (4.13), que incluya constante y tendencia, y probar tres especificaciones distintas que serían válidas bajo \\(H_0 : \\rho = 1\\): Modelo A: con intercepto y tendencia: \\[\\begin{equation*} \\Delta Y_t = \\alpha + \\delta t + \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta &lt; 0\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario alrededor de una tendencia determinística. Modelo B: con intercepto: \\[\\begin{equation*} \\Delta Y_t = \\alpha + \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta &lt; 0\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario alrededor de una constante. Modelo C: sin intercepto y tendencia: \\[\\begin{equation*} \\Delta Y_t = \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta &lt; 0\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario sin considerar una constante o una tendencia determinística, es decir, es un proceso puramente aleatorio. . La siguiente Tabla muestra los resultados de la prueba para la serie en niveles, \\(LTC_t\\), y en diferencias \\(\\Delta LTC_t\\) de una prueba ADF. De esta forma, podremos comparar si la serie es estacionaria en niveles o en diferencias. La manera de determinarlo es, al igual que en el caso anterior, comparar el estadístico con el valor crítico. Así, rechazaremos la hipótesis nula en los casos en que el estadístico \\(t\\) se ubique por encima del valor crítico. En este caso, podemos observar que la serie se vuelve estacionaria hasta su primera diferencia–caso en el que rechazamos la hipótesis nula en todos los modelos. Table 4.1: Resumen de resultados de la prueba Augmented Dickey-Fuller (ADF) con 5 rezagos (resultado de \\(p = int \\{ 4 (282/100)^{(1/4)} \\} = 5\\)). Model Estadístico 1% CV 5% CV 10% CV LTC_t Modelo A \\(t=-2.4972\\) \\(-3.98\\) \\(-3.42\\) \\(-3.13\\) Modelo B \\(t=-1.1265\\) \\(-3.44\\) \\(-2.87\\) \\(-2.57\\) Modelo C \\(t=1.0273\\) \\(-2.58\\) \\(-1.95\\) \\(-1.62\\) \\(\\Delta LTC_t\\) Modelo A \\(t=-7.4125\\) \\(-3.98\\) \\(-3.42\\) \\(-3.13\\) Modelo B \\(t=-7.4034\\) \\(-3.44\\) \\(-2.87\\) \\(-2.57\\) Modelo C \\(t=-7.2639\\) \\(-2.58\\) \\(-1.95\\) \\(-1.62\\) ## Augmented Dickey - Fuller ### NIVELES: Tipo de cambio summary(ur.df(LDatos[, 2], type = &quot;trend&quot;, lags = 5)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression trend ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.077054 -0.014133 -0.002281 0.011551 0.167611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.10899655 0.04255838 2.561 0.0110 * ## z.lag.1 -0.04919274 0.01969939 -2.497 0.0131 * ## tt 0.00015118 0.00006679 2.263 0.0244 * ## z.diff.lag1 0.32887766 0.06112000 5.381 0.000000162 *** ## z.diff.lag2 -0.12845492 0.06432008 -1.997 0.0468 * ## z.diff.lag3 -0.01064885 0.06462312 -0.165 0.8692 ## z.diff.lag4 0.05690285 0.06350714 0.896 0.3711 ## z.diff.lag5 -0.01668349 0.06151762 -0.271 0.7864 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02597 on 268 degrees of freedom ## Multiple R-squared: 0.1196, Adjusted R-squared: 0.09662 ## F-statistic: 5.202 on 7 and 268 DF, p-value: 0.00001414 ## ## ## Value of test-statistic is: -2.4972 2.5862 3.2058 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau3 -3.98 -3.42 -3.13 ## phi2 6.15 4.71 4.05 ## phi3 8.34 6.30 5.36 summary(ur.df(LDatos[, 2], type = &quot;drift&quot;, lags = 5)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression drift ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.081102 -0.013688 -0.002598 0.011536 0.167963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.019078 0.015382 1.240 0.2160 ## z.lag.1 -0.006575 0.005837 -1.126 0.2610 ## z.diff.lag1 0.306455 0.060772 5.043 0.000000845 *** ## z.diff.lag2 -0.156749 0.063575 -2.466 0.0143 * ## z.diff.lag3 -0.034594 0.064238 -0.539 0.5907 ## z.diff.lag4 0.038455 0.063463 0.606 0.5451 ## z.diff.lag5 -0.041998 0.060954 -0.689 0.4914 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02617 on 269 degrees of freedom ## Multiple R-squared: 0.1028, Adjusted R-squared: 0.08277 ## F-statistic: 5.136 on 6 and 269 DF, p-value: 0.00005131 ## ## ## Value of test-statistic is: -1.1265 1.2978 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau2 -3.44 -2.87 -2.57 ## phi1 6.47 4.61 3.79 summary(ur.df(LDatos[, 2], type = &quot;none&quot;, lags = 5)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression none ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.080975 -0.014773 -0.002006 0.011681 0.165789 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## z.lag.1 0.0006247 0.0006081 1.027 0.3052 ## z.diff.lag1 0.3052416 0.0608250 5.018 0.000000947 *** ## z.diff.lag2 -0.1599301 0.0635869 -2.515 0.0125 * ## z.diff.lag3 -0.0366726 0.0642802 -0.571 0.5688 ## z.diff.lag4 0.0367316 0.0635108 0.578 0.5635 ## z.diff.lag5 -0.0449750 0.0609677 -0.738 0.4613 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0262 on 270 degrees of freedom ## Multiple R-squared: 0.1027, Adjusted R-squared: 0.08271 ## F-statistic: 5.148 on 6 and 270 DF, p-value: 0.00004979 ## ## ## Value of test-statistic is: 1.0273 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau1 -2.58 -1.95 -1.62 ### DIFERENCIAS: Tipo de cambio summary(ur.df(DLDatos[, 2], type = &quot;trend&quot;, lags = 5)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression trend ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.069846 -0.013941 -0.001969 0.011796 0.166553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.00398042 0.00326372 1.220 0.2237 ## z.lag.1 -0.96600985 0.13032157 -7.413 0.00000000000164 *** ## tt -0.00001242 0.00001981 -0.627 0.5310 ## z.diff.lag1 0.27428705 0.11800103 2.324 0.0209 * ## z.diff.lag2 0.11502927 0.10628825 1.082 0.2801 ## z.diff.lag3 0.07837533 0.09095430 0.862 0.3896 ## z.diff.lag4 0.09775877 0.07423509 1.317 0.1890 ## z.diff.lag5 0.07280186 0.06082666 1.197 0.2324 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02605 on 267 degrees of freedom ## Multiple R-squared: 0.3901, Adjusted R-squared: 0.3741 ## F-statistic: 24.39 on 7 and 267 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic is: -7.4125 18.3632 27.5402 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau3 -3.98 -3.42 -3.13 ## phi2 6.15 4.71 4.05 ## phi3 8.34 6.30 5.36 summary(ur.df(DLDatos[, 2], type = &quot;drift&quot;, lags = 5)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression drift ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.070858 -0.014250 -0.001793 0.011998 0.165361 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.002197 0.001601 1.372 0.1711 ## z.lag.1 -0.963143 0.130094 -7.403 0.00000000000172 *** ## z.diff.lag1 0.272137 0.117818 2.310 0.0217 * ## z.diff.lag2 0.113463 0.106139 1.069 0.2860 ## z.diff.lag3 0.077366 0.090837 0.852 0.3951 ## z.diff.lag4 0.097369 0.074148 1.313 0.1903 ## z.diff.lag5 0.072725 0.060758 1.197 0.2324 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02602 on 268 degrees of freedom ## Multiple R-squared: 0.3892, Adjusted R-squared: 0.3755 ## F-statistic: 28.46 on 6 and 268 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic is: -7.4034 27.41 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau2 -3.44 -2.87 -2.57 ## phi1 6.47 4.61 3.79 summary(ur.df(DLDatos[, 2], type = &quot;none&quot;, lags = 5)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression none ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.069424 -0.012002 0.000317 0.013820 0.167787 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## z.lag.1 -0.92780 0.12773 -7.264 0.00000000000407 *** ## z.diff.lag1 0.24302 0.11608 2.094 0.0372 * ## z.diff.lag2 0.08889 0.10479 0.848 0.3970 ## z.diff.lag3 0.05875 0.08997 0.653 0.5143 ## z.diff.lag4 0.08492 0.07371 1.152 0.2503 ## z.diff.lag5 0.06500 0.06060 1.073 0.2844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02607 on 269 degrees of freedom ## Multiple R-squared: 0.3849, Adjusted R-squared: 0.3712 ## F-statistic: 28.05 on 6 and 269 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic is: -7.2639 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau1 -2.58 -1.95 -1.62 4.2.3 Phillips - Perron (PP) Una tercera prueba es la de PP, la cual también está basada en un AR(1) descrito por la ecuación: \\[\\begin{equation} Y_t = d \\eta + \\rho Y_{t-1} + U_t \\tag{4.14} \\end{equation}\\] Donde \\(d\\) incluye a cualquiera de los componentes determinísticos como constante y tendencia. Al igual que los casos pasados, la hipótesis a probar es \\(H_0 : \\rho = 1\\) contra la alternativa \\(H_a : | \\rho | &lt; 1\\), y asumimos una estructura de MA(q) que es un término de error de la forma: \\[\\begin{equation} U_t = \\psi(L) \\varepsilon_t = \\psi_0 \\varepsilon_t + \\psi_1 \\varepsilon_{t-1} + \\ldots + \\psi_p \\varepsilon_{t-p} \\end{equation}\\] Con \\(\\varepsilon_t\\) que es un ruido blanco con media cero y varianza \\(\\sigma^2\\). En este modelo se elige el valor \\(p\\) que hace que el componente sea un MA(p). Las tablas estadísticas de PP para esta prueba pueden utilizar una estadística \\(Z_\\tau\\) o \\(Z_\\rho\\), las cuales se pueden emplear indistintamente. En resumen, la prueba PP consiste en asumir un modelo general dado por la ecuación (4.14) y probar dos especificaciones distintas que serían válidas bajo \\(H_0 : \\rho = 1\\), ambas considerando un componente Drift: Modelo A: con intercepto y tendencia: \\[\\begin{equation*} Y_t = \\alpha + \\delta t + \\rho Y_{t-1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\rho = 1\\) contra \\(H_a : | \\rho | &lt; 1\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario alrededor de una tendencia determinística. Modelo B: con intercepto: \\[\\begin{equation*} Y_t = \\alpha + \\rho Y_{t-1} + U_t \\end{equation*}\\] Buscamos probar si \\(H_0 : \\rho = 1\\) contra \\(H_a : | \\rho | &lt; 1\\), por lo que es una prueba de una cola. Otra forma de decirlo es: probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario alrededor de una constante. . La siguiente Tabla muestra los resultados de la prueba para la serie en niveles, \\(LTC_t\\), y en diferencias \\(\\Delta LTC_t\\) de una prueba PP. De esta forma, podremos comparar si la serie es estacionaria en niveles o en diferencias. La manera de determinarlo es, al igual que en los casos anteriores, comparar el estadístico con el valor crítico (\\(Z_\\alpha\\) o \\(Z_\\tau\\)). Así, rechazaremos la hipótesis nula en los casos en que el estadístico \\(Z_\\alpha\\) o \\(Z_\\tau\\) se ubique por encima del valor crítico. En este caso, podemos observar que la serie se vuelve estacionaria hasta su primera diferencia–caso en el que rechazamos la hipótesis nula en todos los modelos. Table 4.2: Resumen de resultados de la prueba Phillips-Perron (PP) con 5 rezagos (resultado de \\(p = int \\{ 4 (282/100)^{(1/4)} \\} = 5\\)). Model Estadístico 1% CV 5% CV 10% CV LTC_t Modelo A \\(Z_\\tau = -2.5039\\) \\(-3.994127\\) \\(-3.427199\\) \\(-3.136601\\) Modelo B \\(Z_\\tau = -1.2297\\) \\(-3.455219\\) \\(-2.87195\\) \\(-2.572274\\) \\(\\Delta LTC_t\\) Modelo A \\(Z_\\tau = -12.3906\\) \\(-3.994127\\) \\(-3.427199\\) \\(-3.136601\\) Modelo B \\(Z_\\tau = -12.4037\\) \\(-3.455219\\) \\(-2.87195\\) \\(-2.572274\\) ## PP: Phillips - Perron Test ### NIVELES: Tipo de cambio summary(ur.pp(LDatos[, 2], type = &quot;Z-tau&quot;, model = &quot;trend&quot;, use.lag = 5)) ## ## ################################## ## # Phillips-Perron Unit Root Test # ## ################################## ## ## Test regression with intercept and trend ## ## ## Call: ## lm(formula = y ~ y.l1 + trend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.083973 -0.015415 -0.002526 0.009332 0.170645 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.10275012 0.04851421 2.118 0.0351 * ## y.l1 0.96155565 0.01852595 51.903 &lt;0.0000000000000002 *** ## trend 0.00011203 0.00006246 1.794 0.0740 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02705 on 278 degrees of freedom ## Multiple R-squared: 0.9903, Adjusted R-squared: 0.9902 ## F-statistic: 1.416e+04 on 2 and 278 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic, type: Z-tau is: -2.5039 ## ## aux. Z statistics ## Z-tau-mu 1.1638 ## Z-tau-beta 2.2617 ## ## Critical values for Z statistics: ## 1pct 5pct 10pct ## critical values -3.994127 -3.427199 -3.136601 summary(ur.pp(LDatos[, 2], type = &quot;Z-tau&quot;, model = &quot;constant&quot;, use.lag = 5)) ## ## ################################## ## # Phillips-Perron Unit Root Test # ## ################################## ## ## Test regression with intercept ## ## ## Call: ## lm(formula = y ~ y.l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.089442 -0.014571 -0.002497 0.009620 0.171964 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.020316 0.015600 1.302 0.194 ## y.l1 0.993054 0.005925 167.617 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02716 on 279 degrees of freedom ## Multiple R-squared: 0.9902, Adjusted R-squared: 0.9901 ## F-statistic: 2.81e+04 on 1 and 279 DF, p-value: &lt; 0.00000000000000022 ## ## ## Value of test-statistic, type: Z-tau is: -1.2297 ## ## aux. Z statistics ## Z-tau-mu 1.3459 ## ## Critical values for Z statistics: ## 1pct 5pct 10pct ## critical values -3.455219 -2.87195 -2.572274 ### DIFERENCIAS: Tipo de cambio summary(ur.pp(DLDatos[, 2], type = &quot;Z-tau&quot;, model = &quot;trend&quot;, use.lag = 5)) ## ## ################################## ## # Phillips-Perron Unit Root Test # ## ################################## ## ## Test regression with intercept and trend ## ## ## Call: ## lm(formula = y ~ y.l1 + trend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09095 -0.01306 -0.00158 0.01055 0.17069 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.001572438 0.001579672 0.995 0.320 ## y.l1 0.263626725 0.058071315 4.540 0.00000841 *** ## trend -0.000009351 0.000019483 -0.480 0.632 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02634 on 277 degrees of freedom ## Multiple R-squared: 0.07036, Adjusted R-squared: 0.06365 ## F-statistic: 10.48 on 2 and 277 DF, p-value: 0.00004089 ## ## ## Value of test-statistic, type: Z-tau is: -12.3906 ## ## aux. Z statistics ## Z-tau-mu 1.2352 ## Z-tau-beta -0.4753 ## ## Critical values for Z statistics: ## 1pct 5pct 10pct ## critical values -3.994237 -3.427252 -3.136632 summary(ur.pp(DLDatos[, 2], type = &quot;Z-tau&quot;, model = &quot;constant&quot;, use.lag = 5)) ## ## ################################## ## # Phillips-Perron Unit Root Test # ## ################################## ## ## Test regression with intercept ## ## ## Call: ## lm(formula = y ~ y.l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.090664 -0.013492 -0.001331 0.010702 0.169754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.001566 0.001577 0.993 0.322 ## y.l1 0.264342 0.057972 4.560 0.00000768 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02631 on 278 degrees of freedom ## Multiple R-squared: 0.06959, Adjusted R-squared: 0.06624 ## F-statistic: 20.79 on 1 and 278 DF, p-value: 0.000007682 ## ## ## Value of test-statistic, type: Z-tau is: -12.4037 ## ## aux. Z statistics ## Z-tau-mu 0.969 ## ## Critical values for Z statistics: ## 1pct 5pct 10pct ## critical values -3.455298 -2.871985 -2.572293 4.2.4 Kwiatkowsky - Phillips - Schmidt - Shin (KPSS) La prueba KPSS considera que el proceso es estacionario bajo la hipótesis nula, lo cual hace una diferencia respecto de las anteriores pruebas. El modelo considerado es: \\[\\begin{equation} Y_t = \\delta t + \\xi_t + U_t \\end{equation}\\] Donde \\(U_t\\) es un proceso estacionario y \\(\\xi_t\\) es un ruido blanco descrito por la forma: \\(\\xi_t = \\xi_{t-1} + \\varepsilon_t\\), donde \\(\\varepsilon_t\\) es un proceso normalmente distribuido con media cero y varianza \\(\\sigma^2_\\varepsilon\\). Así, bajo la hipótesis nula \\(H_0 : \\sigma^2_\\varepsilon = 0\\), \\(\\xi\\) se vuelve una constante y el proceso puede tener una tendencia estacionaria. Dado el planteamiento de la prueba, los valores críticos al 95% son: : 0.146, para un modelo con tendencia (estadístico \\(\\tau\\)). : 0.463, para un modelo con constante (estadístico \\(\\mu\\)). . Finalmente, la siguiente Tabla muestra los resultados de la prueba para la serie en niveles, \\(LTC_t\\), y en diferencias \\(\\Delta LTC_t\\) de una prueba KPSS. De esta forma, podremos comparar si la serie es estacionaria en niveles o en diferencias. La manera de determinarlo es, al igual que en los casos anteriores, comparar el estadístico con el valor crítico (\\(\\mu\\) y \\(\\tau\\)). No obstante, en este caso buscaremos aceptar la hipótesis nula en los casos en que los estadísticos \\(\\mu\\) y \\(\\tau\\) se ubique por debajo del valor crítico. En este caso, podemos observar que la serie se vuelve estacionaria hasta su primera diferencia–caso en el que rechazamos la hipótesis nula en todos los modelos. Table 4.3: Resumen de resultados de la prueba Kwiatkowsky - Phillips - Schmidt - Shin (KPSS)). Model Estadístico 1% CV 5% CV 10% CV LTC_t Modelo A \\(\\tau = 0.3433\\) 0.216 0.146 0.119 Modelo B \\(\\mu = 4.429\\) 0.739 0.463 0.347 \\(\\Delta LTC_t\\) Modelo A \\(\\tau = 0.0701\\) 0.216 0.146 0.119 Modelo B \\(\\mu = 0.0801\\) 0.739 0.463 0.347 ## KPSS: ### NIVELES: Tipo de cambio summary(ur.kpss(LDatos[, 2], type = &quot;tau&quot;)) ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: tau with 5 lags. ## ## Value of test-statistic is: 0.3433 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.119 0.146 0.176 0.216 summary(ur.kpss(LDatos[, 2], type = &quot;mu&quot;)) ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 5 lags. ## ## Value of test-statistic is: 4.429 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 ### DIFERENCIAS: Tipo de cambio summary(ur.kpss(DLDatos[, 2], type = &quot;tau&quot;)) ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: tau with 5 lags. ## ## Value of test-statistic is: 0.0701 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.119 0.146 0.176 0.216 summary(ur.kpss(DLDatos[, 2], type = &quot;mu&quot;)) ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 5 lags. ## ## Value of test-statistic is: 0.0801 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 "],["causalidad-de-granger-modelos-multivariados-de-vectores-autoregresivos-y-de-cointegración-y-modelos-ardl.html", "Chapter 5 Causalidad de Granger, Modelos Multivariados de Vectores Autoregresivos y de Cointegración, y Modelos ARDL 5.1 Causalidad de Granger 5.2 Definición y representación del Sistema o Modelo de Vectores Autorregresicos (VAR(p)) 5.3 Análisis de Impulso-Respuesta 5.4 VAR Estructural 5.5 Otras opciones del análisis impulso-respuesta 5.6 Cointegración 5.7 Modelos ADRL", " Chapter 5 Causalidad de Granger, Modelos Multivariados de Vectores Autoregresivos y de Cointegración, y Modelos ARDL En este capítulo removeremos el supuesto de que el análisis es univariado, ya que introduciremos la posibilidad de que los procesos generadores de datos compartan información entre dos o más series. Como primera aproximación desarrollaremos el concepto de Causalidad de Granger. Mediante esta metodología discutiremos cuándo dos series se causan estadísticamente. Posteriormente, introduciremos una técnica más sofisticada conocida como la metodología de Vectores Autoregresivos (VAR), la cual es una generalización de los procesos Autoregresivos (AR) que analizamos al principio del curso. Finalmente, introduciremos la técnica de cointegración y de rezagos distribuidos (ARDL) para los casos en que las series que analicemos sean procesos no estacionarios. A partir de este punto, asumiremos que las series empleadas son estacionarias en sus primeras diferencias y solo nos preocuparemos por su estacionariedad en los casos particulares de Cointegración y los modelos ARDL. 5.1 Causalidad de Granger Hasta ahora hemos supuesto que una serie puede ser explicada únicamente con la información contenida en ella misma. No obstante, en adelante trataremos de analizar el caso en el que buscamos determinar relaciones entre variables y cómo el comportamiento de una serie influye en las demás. Algunas relaciones más importantes son las llamadas: causalidad. En este caso analizaremos el procedimiento de Granger (1969), conocido como causalidad de Granger. En adelante asumiremos que las series involucradas son débilmente estacionarias. Sean \\(X\\) y \\(Y\\) dos series débilmente estacionarias. Definamos a \\(I_t\\) un conjunto de toda la información disponible hasta el momento \\(t\\). Asimismo, digamos que \\(\\overline{X}_t\\) y \\(\\overline{Y}_t\\) son los conjuntos de toda la información disponible (actual y pasada) de \\(X\\) y \\(Y\\), respectivamente. Es decir: \\[\\begin{eqnarray*} \\overline{X}_t &amp; := &amp; \\{ X_t, X_{t-1}, X_{t-2}, \\ldots \\} \\\\ \\overline{Y}_t &amp; := &amp; \\{ Y_t, Y_{t-1}, Y_{t-2}, \\ldots \\} \\\\ I_t &amp; := &amp; \\overline{X}_t + \\overline{Y}_t \\end{eqnarray*}\\] Adicionalmente, definamos \\(\\sigma^2(*)\\) como la varianza del término de error estimado de una regresión dada. Dicho lo anterior, digamos que: La definición anterior aplica de igual forma si se reemplaza a \\(X\\) por \\(Y\\) y a \\(Y\\) por \\(X\\), respectivamente. De acuerdo a la definición anterior, existen 5 diferentes posibilidades de relaciones causales entre las dos series: Por lo anterior, representaremos mediante un \\(AR(p)\\) con variables exógenas lo siguiente: \\[\\begin{equation} A(L) \\begin{bmatrix} Y_t \\\\ X_t \\end{bmatrix} = \\begin{bmatrix} a_{11}(L) &amp; a_{12}(L) \\\\ a_{21}(L) &amp; a_{22}(L) \\end{bmatrix} \\begin{bmatrix} Y_t \\\\ X_t \\end{bmatrix} = \\begin{bmatrix} V_t \\\\ U_t \\end{bmatrix} \\tag{5.1} \\end{equation}\\] O en su versión \\(MA(q)\\) con variables exógenas: \\[\\begin{equation} \\begin{bmatrix} Y_t \\\\ X_t \\end{bmatrix} = B(L) \\begin{bmatrix} V_t \\\\ U_t \\end{bmatrix} = \\begin{bmatrix} b_{11}(L) &amp; b_{12}(L) \\\\ b_{21}(L) &amp; b_{22}(L) \\end{bmatrix} \\begin{bmatrix} V_t \\\\ U_t \\end{bmatrix} \\end{equation}\\] Para determinar el test de causalidad utilizaremos una especificación similar a la de la ecuación (5.1). Para probar si \\(X\\) causa a \\(Y\\), consideraremos la siguiente regresión: \\[\\begin{equation} Y_t = \\alpha_0 + \\sum^{k_1}_{k = 1} a^k_{11} Y_{t-k} + \\sum^{k_2}_{k = k_0} a^k_{12} X_{t-k} + U_{1,t} \\end{equation}\\] Donde \\(k_0 = 1\\) y, en general, se asume que \\(k_1 = k_2\\). Asimismo, el valor de estas constantes se puede determinar con el criterio de Akaike (o cualquier otro criterio de información). No obstante, algunos autores sugieren que una buena práctica es considerar valores de \\(k_1\\) y \\(k_2\\) que recorran al 4, 8, 12 y 16. Dicho lo anterior, el test de causalidad de Granger se establece con una prueba F, en la cual se prueba la siguiente hipótesis nula: \\[\\begin{equation} H_0: a^1_{12} = a^2_{12} = \\ldots = a^{k2}_{12} = 0 \\end{equation}\\] . Consideremos como variables analizadas al Índice Nacional de Precios al Consumidor (\\(INPC_t\\)), al Tipo de Cambio (\\(TDC_t\\)) y al rendimiento anual de los Cetes a 28 días (\\(CETE28_t\\)), todas desestacionalizadas para el periodo de enero de 2000 a julio de 2019. Dado que la metodología de Granger supone que las series son estacionarias, utilizaremos las diferencias logarítmicas de cada una de las tres series (es decir, utilizaremos una transformación del tipo \\(ln(X_t) - ln(X_{t-1})\\)). La Figura 5.1 muestra las series en su transformación de diferencias logarítmicas. library(ggplot2) library(dplyr) library(stats) library(MASS) library(strucchange) library(zoo) library(sandwich) library(urca) library(lmtest) library(vars) # load(&quot;BD/Datos_Ad.RData&quot;) # INPC &lt;- ts(Datos_Ad$INPC_Ad, start = c(2000, 1), freq = 12) DLINPC &lt;- diff(log( ts(Datos_Ad$INPC_Ad, start = c(2000, 1), freq = 12) )) TC &lt;- ts(Datos_Ad$TC_Ad, start = c(2000, 1), freq = 12) DLTC &lt;- diff(log( ts(Datos_Ad$TC_Ad, start = c(2000, 1), freq = 12) )) CETE28 &lt;- ts(Datos_Ad$CETE28_Ad, start = c(2000, 1), freq = 12) DLCETE28 &lt;- diff(log( ts(Datos_Ad$CETE28_Ad, start = c(2000, 1), freq = 12) )) # #png(&quot;Plots/DLGranger.png&quot;, width = 800, height = 1200) par(mfrow=c(3, 1)) plot(DLINPC, xlab = &quot;Tiempo&quot;, main = &quot;Diferencias Logarítmicas del INPC&quot;, col = &quot;darkgreen&quot;) plot(DLTC, xlab = &quot;Tiempo&quot;, main = &quot;Diferencias Logarítmicas del Tipo de Cambio&quot;, col = &quot;darkblue&quot;) plot(DLCETE28, xlab = &quot;Tiempo&quot;, main = &quot;Diferencias Logarítmicas de los Cetes a 28 dias&quot;, col = &quot;darkred&quot;) Figure 5.1: Series en diferencias logarítmicas dadas por las siguientes expresiones: \\(DLINPC_t = ln(DLINPC_t) - ln(DLINPC_{t-1})\\), \\(DLTC_t = ln(TC_t) - ln(TC_{t-1})\\) y \\(DLCETE28_t = ln(CETE28_t) - ln(CETE28_{t-1})\\). par(mfrow=c(1, 1)) #dev.off() Por simplicidad, en el Cuadro 5.1 se muestra el resultado de aplicar el test de Granger a diferentes especificaciones, con rezagos 4, 8, 12 y 16, sólo para la serie de Tipo de Cambio en diferencias logarítmicas. En cada una de las pruebas se compara el modelo considerado como regresor a la variable que es candidata de causar, respecto del modelo sin considerar a dicha variable. Table 5.1: Prueba de si \\(DLINPC_t\\) Granger causa a \\(DLTC_t\\). Rezagos Estadiística F Probabilidad (\\(&gt;\\)F) Significancia 4 3.2621 0.01265 * 8 1.9079 0.06030 12 2.2577 0.01067 * 16 1.6735 0.05495 * Notas: *** signif. al 0.1% ** signif. al 1% * signif. al 5% De acuerdo con el Cuadro 5.1, podemos concluir que existe información estadísticamente significativa para concluir que la inflación causa a la tasa de depreciación cambiaria, ambas medidas como las diferencias logaritmicas. El resto de los resultados para las otras combinaciones de causalidad se encuentran en el R Markdown llamado Clase 13 ubicado en el repositorio de GitHub. 5.2 Definición y representación del Sistema o Modelo de Vectores Autorregresicos (VAR(p)) En esta sección ampliaremos la discusión planteada en el apartado anterior. En el sentido de que en la sección pasada nuestra discusión se limitó al análisis de causalidad entre dos variables a la vez, que si bien es posible extenderlo a más variables, es un procedimiento limitado a casos particulares por las siguientes razones. El procedimiento de causalidad de Granger supone que es posible identificar un sistema de ecuaciones que debe conformarse una vez que se ha identificado el sentido de la causalidad. Así, el proceso anterior necesita del conocimiento previo de las relaciones que existen entre las variables. Adicionalmente, no resuelve el problema más general que está relacionado con cómo identificar la causalidad cuando se tienen múltiples variables con múltiples sentidos de causalidad. En esta sección analizaremos una mejor aproximación al problema de cómo identificar la causalidad múltiple. Por lo tanto, como mecanismo para solucionar el problema planteado, analizaremos el caso de un Sistema o Modelo de Vectores Autoregresivos conocido como VAR. El primer supuesto del que partiremos es que existe algún grado de endogeneidad entre las variables consideradas en el análisis. Adicionalmente, el segundo supuesto que estableceremos es que requerimos que las variables que tengamos consideradas sean estacionarias. Por lo anterior, diremos que un modelo de Vectores Autoregresivos (VAR) es un procedimiento que sigue fundado en el supuesto de que las variables consideradas son estacionarias. Así, hasta este momento del curso hemos pasado de modelos univariados a modelos multivariados, pero no hemos podido dejar de asumir que las series son estacionarias. En lo subsecuente asumiremos que las series empleadas son estacionarias y sólo lo demostraremos cuando, en su caso, sea necesario. Esto no significa que el lector deba asumir estacionariedad. Por el contrario, siempre debe probar que las series son estacionarias antes de iniciar la implementación de cualquier técnica de series de tiempo. Ahora bien, iniciaremos con el establecimiento de la representación del proceso. Digamos que tenemos un proceso estocástico \\(\\mathbf{X}_t\\) estacionario vectorial de dimensión \\(k\\): \\[\\begin{equation*} \\mathbf{X}_t = \\begin{bmatrix} X_{1t} \\\\ X_{2t} \\\\ \\vdots \\\\ X_{kt} \\end{bmatrix} \\end{equation*}\\] Para cualquier \\(i = 1, 2, \\ldots, p\\): \\[\\begin{equation*} \\mathbf{X}_{t-i} = \\begin{bmatrix} X_{1t-i} \\\\ X_{2t-i} \\\\ \\vdots \\\\ X_{kt-i} \\end{bmatrix} \\end{equation*}\\] Donde cada \\(X_{kt}\\) en \\(\\mathbf{X}_t\\) es una serie de tiempo por sí misma. De esta forma, la expresión reducida del modelo o el proceso \\(VAR(p)\\) estará dado por: \\[\\begin{equation} \\mathbf{X}_t = \\boldsymbol{\\delta} + A_1 \\mathbf{X}_{t-1} + A_2 \\mathbf{X}_{t-2} + \\ldots + A_p \\mathbf{X}_{t-p} + \\mathbf{U}_{t} \\tag{5.2} \\end{equation}\\] Donde cada uno de las \\(A_i\\), \\(i = 1, 2, \\ldots, p\\), son matrices cuadradas de dimensión \\(k\\) y \\(\\mathbf{U}_t\\) representa un vector de dimensión \\(k \\times 1\\) con los residuales en el momento del tiempo \\(t\\) que son, por individual, un proceso puramente aleatorio. También se incorpora un vector de términos constantes denominado como \\(\\mathbf{\\delta}\\), el cual es de dimensión \\(k \\times 1\\). Así, la ecuación (5.2) supone la siguiente estructura del vector \\(\\boldsymbol{\\delta}\\): \\[\\begin{equation*} \\boldsymbol{\\delta} = \\begin{bmatrix} \\delta_{1} \\\\ \\delta_{2} \\\\ \\vdots \\\\ \\delta_{k} \\end{bmatrix} \\end{equation*}\\] También, la ecuación (5.2) supone que cada matriz \\(A_i\\), \\(i = 1, 2, \\ldots, p\\) está definida de la siguiente forma: \\[\\begin{equation*} \\mathbf{A}_i = \\begin{bmatrix} a^{(i)}_{11} &amp; a^{(i)}_{12} &amp; \\ldots &amp; a^{(i)}_{1k} \\\\ a^{(i)}_{21} &amp; a^{(i)}_{22} &amp; \\ldots &amp; a^{(i)}_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a^{(i)}_{k1} &amp; a^{(i)}_{k2} &amp; \\ldots &amp; a^{(i)}_{kk} \\end{bmatrix} \\end{equation*}\\] Donde \\(i = 1, 2, \\ldots, p\\). Retomando la ecuación (5.2) y considerando que podemos ocupar el operador rezago \\(L^j\\) de forma análoga al caso del modelo \\(AR(p)\\), pero aplicado a un vector, tenemos las siguientes ecuaciones: \\[\\begin{eqnarray} \\mathbf{X}_t - A_1 \\mathbf{X}_{t-1} - A_2 \\mathbf{X}_{t-2} - \\ldots - A_p \\mathbf{X}_{t-p} &amp; = &amp; \\boldsymbol{\\delta} + \\mathbf{U}_{t} \\nonumber \\\\ \\mathbf{X}_t - A_1 L \\mathbf{X}_{t} - A_2 L^2 \\mathbf{X}_{t} - \\ldots - A_p L^p \\mathbf{X}_{t-p} &amp; = &amp; \\boldsymbol{\\delta} + \\mathbf{U}_{t} \\nonumber \\\\ (I_k - \\mathbf{A_1} L - \\mathbf{A_2} L^2 - \\ldots - \\mathbf{A_p} L^p) \\mathbf{X}_t &amp; = &amp; \\boldsymbol{\\delta} + \\mathbf{U}_{t} \\nonumber \\\\ \\mathbf{A}(L) \\mathbf{X}_t &amp; = &amp; \\boldsymbol{\\delta} + \\mathbf{U}_{t} \\tag{5.3} \\end{eqnarray}\\] Adicionalmente, requeriremos que dado que \\(\\mathbf{U}_t\\) es un proceso puramente aleatorio, este debe cumplir con las siguientes condiciones: El valor esperado del término de error es cero: \\[\\begin{equation} \\mathbb{E}[\\mathbf{U}_t] = 0 \\end{equation}\\] Existe una matriz de varianzas y covarianzas entre los términos de error contemporáneos dada por: \\[\\begin{eqnarray} \\mathbb{E}[\\mathbf{U}_t \\mathbf{U}_t&#39;] &amp; = &amp; \\mathbb{E} \\left[ \\begin{bmatrix} U^{(t)}_{1} \\\\ U^{(t)}_{2} \\\\ \\vdots \\\\ U^{(t)}_{k} \\end{bmatrix} \\begin{bmatrix} U^{(t)}_{1} &amp; U^{(t)}_{2} &amp; \\ldots &amp; U^{(t)}_{k} \\end{bmatrix} \\right] \\nonumber \\\\ &amp; = &amp; \\mathbb{E} \\begin{bmatrix} U^{(t)}_{1} U^{(t)}_{1} &amp; U^{(t)}_{1} U^{(t)}_{2} &amp; \\ldots &amp; U^{(t)}_{1} U^{(t)}_{k} \\\\ U^{(t)}_{2} U^{(t)}_{1} &amp; U^{(t)}_{2} U^{(t)}_{2} &amp; \\ldots &amp; U^{(t)}_{2} U^{(t)}_{k} \\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ U^{(t)}_{k} U^{(t)}_{1} &amp; U^{(t)}_{k} U^{(t)}_{2} &amp; \\ldots &amp; U^{(t)}_{k} U^{(t)}_{k} \\end{bmatrix} \\nonumber \\\\ &amp; = &amp; \\begin{bmatrix} \\sigma^2_1 &amp; \\rho_{12} &amp; \\ldots &amp; \\rho_{1k} \\\\ \\rho_{21} &amp; \\sigma^2_2 &amp; \\ldots &amp; \\rho_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ \\rho_{k1} &amp; \\rho_{k2} &amp; \\ldots &amp; \\sigma^2_k \\end{bmatrix} \\nonumber \\\\ &amp; = &amp; \\mathbf{\\Sigma}_{UU} \\tag{5.4} \\end{eqnarray}\\] La matriz de varianzas y covarianzas no contemporáneas es nula. Es decir, que para todo \\(t \\neq s\\): \\[\\begin{eqnarray} \\mathbb{E} [\\mathbf{U}_t \\mathbf{U}_s&#39;] &amp; = &amp; \\mathbb{E} \\left[ \\begin{bmatrix} U^{(t)}_{1} \\\\ U^{(t)}_{2} \\\\ \\vdots \\\\ U^{(t)}_{k} \\end{bmatrix} \\begin{bmatrix} U^{(s)}_{1} &amp; U^{(s)}_{2} &amp; \\ldots &amp; U^{(s)}_{k} \\end{bmatrix} \\right] \\nonumber \\\\ &amp; = &amp; \\mathbb{E} \\begin{bmatrix} U^{(t)}_{1} U^{(s)}_{1} &amp; U^{(t)}_{1} U^{(s)}_{2} &amp; \\ldots &amp; U^{(t)}_{1} U^{(s)}_{k} \\\\ U^{(t)}_{2} U^{(s)}_{1} &amp; U^{(t)}_{2} U^{(s)}_{2} &amp; \\ldots &amp; U^{(t)}_{2} U^{(s)}_{k} \\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ U^{(t)}_{k} U^{(s)}_{1} &amp; U^{(t)}_{k} U^{(s)}_{2} &amp; \\ldots &amp; U^{(t)}_{k} U^{(s)}_{k} \\end{bmatrix} \\nonumber \\\\ &amp; = &amp; \\mathbf{0} \\tag{5.5} \\end{eqnarray}\\] Las ecuaciones (5.4) y (5.5) significan que los residuales \\(\\mathbf{U}_t\\) pueden estar correlacionados entre ellos solo en el caso de que la información sea contemporánea, pero no tienen información en común entre residuales de otros periodos. Al igual que en el caso del modelo o especificación \\(AR(p)\\) en la especificación del modelo \\(VAR(p)\\) existen condiciones de estabilidad. Dichas condiciones están dadas por lo siguiente, definamos el siguiente polinomio que resulta de tomar la matriz \\(\\mathbf{A}(L)\\) en la ecuación (5.3): \\[\\begin{equation} Det[I_t - A_1 z - A_2 z^2 - \\ldots - A_p z^p] \\neq 0 \\end{equation}\\] Donde las raíces del polinomio cumplen que \\(|z| \\leq 1\\), es decir, se ubican dentro del circulo unitario. La ecuación (5.3) puede ser rexpresada en una forma similar al un proceso de MA. Al respecto, de forma similar a la siguiente ecuación podemos construir un modelo \\(VARMA(p,q)\\), el cual no estudiamos es este curso. Reromando el primer planteamiento, podemos escribir: \\[\\begin{eqnarray} \\mathbf{X}_t &amp; = &amp; \\mathbf{A}^{-1}(L) \\boldsymbol{\\delta} + \\mathbf{A}^{-1}(L) \\mathbf{U}_t \\nonumber \\\\ &amp; = &amp; \\boldsymbol{\\mu} + \\boldsymbol{\\beta}(L) \\mathbf{U}_t \\tag{5.6} \\end{eqnarray}\\] Donde \\(\\boldsymbol{\\mu}\\) es un vector de \\(k \\times 1\\) constantes y \\(\\boldsymbol{\\beta}(L)\\) es una matriz que depende de \\(L\\). Por el lado de las matrices que representan la autocovarianza, estás resultan de resolver lo siguiente: \\[\\begin{equation} \\Gamma_X(\\tau) = E[(\\mathbf{X}_t - \\mu)(\\mathbf{X}_{t-\\tau} - \\mu)&#39;] \\end{equation}\\] Ahora, sin pérdida de generalidad digamos que la especificación VAR(p) en la ecuación (5.2) no tiene constante, por lo que \\(\\boldsymbol{\\delta} = \\mathbf{0}\\), lo que implica que \\(\\boldsymbol{\\mu} = \\mathbf{0}\\). De esta forma las matrices de autocovarianza resultan de: \\[\\begin{eqnarray*} \\Gamma_{\\mathbf{X}}(\\tau) &amp; = &amp; E[(\\mathbf{X}_t)(\\mathbf{X}_{t-\\tau})&#39;] \\\\ &amp; = &amp; \\mathbf{A_1} E[(\\mathbf{X}_{t-1})(\\mathbf{X}_{t-\\tau})&#39;] + \\mathbf{A_2} E[(\\mathbf{X}_{t-2})(\\mathbf{X}_{t-\\tau})&#39;] \\\\ &amp; &amp; + \\ldots + \\mathbf{A_p} E[(\\mathbf{X}_{t-p})(\\mathbf{X}_{t-\\tau})&#39;] + E[(\\mathbf{U}_t(\\mathbf{X}_{t-\\tau})&#39;] \\end{eqnarray*}\\] Finalmente, al igual que en el caso \\(AR(p)\\), requerimos de una métrica que nos permita determinar el número de rezagos óptimo \\(p\\) en el \\(VAR(p)\\). Así, establecemos criterios de información similares a los del \\(AR(p)\\) dados por: 1.Final Prediction Error (FPE): \\[\\begin{equation} FPE(p) = \\left[ \\frac{T + kp + 1}{T - kp - 1} \\right]^k |\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| \\end{equation}\\] Akaike Criterion (AIC): \\[\\begin{equation} AIC(p) = ln|\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| + (k + p k^2) \\frac{2}{T} \\end{equation}\\] Hannan - Quinn Criterion (HQ): \\[\\begin{equation} HQ(p) = ln|\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| + (k + p k^2) \\frac{2ln(ln(2))}{T} \\end{equation}\\] Schwartz Criterion (SC): \\[\\begin{equation} SC(p) = ln|\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p)| + (k + p k^2) \\frac{ln(T)}{T} \\end{equation}\\] Donde la matriz de varianzas y covarianzas contemporáneas estará dada por: \\[\\begin{equation*} \\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}(p) = \\mathbb{E} \\left[ \\begin{bmatrix} U^{(t)}_{1} \\\\ U^{(t)}_{2} \\\\ \\vdots \\\\ U^{(t)}_{k} \\end{bmatrix} \\begin{bmatrix} U^{(t)}_{1} &amp; U^{(t)}_{2} &amp; \\ldots &amp; U^{(t)}_{k} \\end{bmatrix} \\right] \\end{equation*}\\] . Ahora veámos un ejemplo de estimación de \\(VAR(p)\\). Para el ejemplo utilizaremos las series de INPC, Tipo de Cambio, rendimiento de los Cetes a 28 días, el IGAE y el Índice de Producción Industrial de los Estados Unidos, todas desestacionalizadas y para el período de enero de 2000 a julio de 2019. Dado que el supuesto estacionariedad sigue presente en nuestro análisis, emplearemos cada una de las series en su versión de diferencias logaritmicas. Las Figuras 5.2 y 5.3 muestra las series referidas. library(ggplot2) library(dplyr) library(stats) library(MASS) library(strucchange) library(zoo) library(sandwich) library(urca) library(lmtest) library(vars) # load(&quot;BD/Datos_Ad.RData&quot;) # DLINPC &lt;- diff(log( ts(Datos_Ad$INPC_Ad, start = c(2000, 1), freq = 12) )) DLTC &lt;- diff(log( ts(Datos_Ad$TC_Ad, start = c(2000, 1), freq = 12) )) DLCETE28 &lt;- diff(log( ts(Datos_Ad$CETE28_Ad, start = c(2000, 1), freq = 12) )) DLIGAE &lt;- diff(log( ts(Datos_Ad$IGAE_Ad, start = c(2000, 1), freq = 12) )) DLIPI &lt;- diff(log( ts(Datos_Ad$IPI_Ad, start = c(2000, 1), freq = 12) )) Datos &lt;- data.frame(cbind(DLINPC, DLTC, DLCETE28, DLIGAE, DLIPI)) Datos &lt;- ts(Datos, start = c(2000, 2), freq = 12) plot(Datos, plot.type = &quot;s&quot;, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;black&quot;, &quot;purple&quot;), main = &quot;Series en Diferencias logaritmicas&quot;, xlab = &quot;Tiempo&quot;, ylab = &quot;Variacion&quot;) legend(&quot;bottomright&quot;, c(&quot;INPC&quot;, &quot;TC&quot;, &quot;CETES28&quot;, &quot;IGAE&quot;, &quot;IPI&quot;), cex = 0.6, lty = 1:1, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;black&quot;, &quot;purple&quot;)) Figure 5.2: Series en diferencias logarítmicas (Forma 1) plot(Datos, plot.type = &quot;m&quot;, col = &quot;darkgreen&quot;, main = &quot;Series en Diferencias logaritmicas&quot;, xlab = &quot;Tiempo&quot;) Figure 5.3: Series en diferencias logarítmicas (Forma 2) Dicho lo anterior, a continuación mostraremos la tabla que resume el valor de los distintos criterios de información para una especificación de un \\(VAR(p)\\) con constante. Notése que es posible especificar un \\(VAR(p)\\) con tendencia, siempre que exista evidencia de que algunas de las series sean estacionarias alrededor de una tendencia. Caso que no aplica hasta este momento, ya que nuestro análisis de estacionariedad es claro respecto a la media constante (más adelante aportaremos la evidencia de esto), lo cual elimina la posibilidad de incluir una tendencia. En el Cuadro 5.2 reportamos el número de rezagos propuesto a partir de cada criterio de información y en el Cuadro 5.3 reportamos los resultados de aplicar una prueba de criterios de información para diferentes valores de rezagos. Del cual se concluye que el número óptimo de rezagos es 2 (según el criterio AIC y el FPE) y 1 (según el criterio HQ y el SC). Recordemos que es común que el criterio AIC siempre reporte el mayor valor de rezagos, por lo que es una buena práctica utilizarlo como referente principal. Table 5.2: Número de rezagos determinados por cada uno de los criterios de información para diferentes especificaciones de modelos VAR(p) con término constante de la series \\(DLINPC_t\\), \\(DLTC_t\\), \\(DLCETE28_t\\), \\(DLIGAE_t\\) y \\(DLIPI_t\\). AIC HQ SC FPE 2 1 1 2 Table 5.3: Criterios de información para diferentes especificaciones de modelos VAR(p) con término constante de la series \\(DLINPC_t\\), \\(DLTC_t\\), \\(DLCETE28_t\\), \\(DLIGAE_t\\) y \\(DLIPI_t\\). Rezagos AIC HQ SC FPE 1 -4.636412e+01 -4.617847e+01 -4.590430e+01 7.317262e-21 2 -4.639541e+01 -4.605506e+01 -4.555241e+01 7.094216e-21 3 -4.635305e+01 -4.585799e+01 -4.512686e+01 7.407479e-21 \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) VARselect(Datos, lag.max = 12, type = &quot;const&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 2 1 1 2 ## ## $criteria ## 1 2 ## AIC(n) -43.4369385930444451560106245 -43.4878280980991220872056147 ## HQ(n) -43.2759374580961733158801508 -43.1926593506939511257769482 ## SC(n) -43.0360414131631969780755753 -42.7528499349835016118959174 ## FPE(n) 0.0000000000000000001366449 0.0000000000000000001298899 ## 3 4 ## AIC(n) -43.4268489491758202802884625 -43.3868843889051234441467386 ## HQ(n) -42.9975125893137573029889609 -42.8233804165861613455490442 ## SC(n) -42.3577898028258275076041173 -41.9837442593207583740877453 ## FPE(n) 0.0000000000000000001381225 0.0000000000000000001438821 ## 5 6 ## AIC(n) -43.293163285544032703455741 -43.2431704133633161291072611 ## HQ(n) -42.595491700768178588987212 -42.4113312161305699987678963 ## SC(n) -41.555942172725302441449458 -41.1718683173102135697263293 ## FPE(n) 0.000000000000000000158246 0.0000000000000000001667106 ## 7 8 ## AIC(n) -43.2902076769675190348607430 -43.2419352439015938216471113 ## HQ(n) -42.3242008672778737832231855 -42.1417608217550565541387186 ## SC(n) -40.8848245976800441781051632 -40.5024711813797466675168835 ## FPE(n) 0.0000000000000000001595179 0.0000000000000000001680601 ## 9 10 ## AIC(n) -43.2201406224138153788771888 -43.1788072276712782127106038 ## HQ(n) -41.9857985878103789900706033 -41.8102975806109569134605408 ## SC(n) -40.1465955766575959273723129 -39.7711811986806935692584375 ## FPE(n) 0.0000000000000000001726236 0.0000000000000000001810365 ## 11 12 ## AIC(n) -43.1191364389135785017970193 -43.0502244962885995960277796 ## HQ(n) -41.6164591793963580812487635 -41.4133796243144800541813311 ## SC(n) -39.3774294266886144555428473 -38.9744365008292632523989596 ## FPE(n) 0.0000000000000000001936459 0.0000000000000000002093856 De esta forma, justificamos la estimación de un \\(VAR(2)\\). Los resultados del mismo se reportan en los siguientes cuadros, en los que se muestra el resultado de una de las ecuaciones. Los resultados restantes se encuentran en el código de R mostrado más abajo. Primero mostraremos los resultados de las raíces del polinomio característico en el Cuadro 5.4, seguido de un cuadro para la ecuación del IGAE en el Cuadro 5.5 (por simplicidad se omiten las otras cuatro ecuaciones del VAR(2)), y del Cuadro 5.6 con la matriz \\(\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}\\) estimada del VAR. Table 5.4: Raíces del polinomio característico de un VAR(2). 0.7452 0.4403 0.4403 0.3503 0.3503 0.3342 0.3342 0.3339 0.3339 0.06951 Table 5.5: Criterios de información para diferentes especificaciones de modelos VAR(p) con término constante de la series \\(DLINPC_t\\), \\(DLTC_t\\), \\(DLCETE28_t\\), \\(DLIGAE_t\\) y \\(DLIPI_t\\). Variable Coeficiente Error Est. Estad. t Prob.(\\(&gt;\\) t) \\(DLINPC_{t-1}\\) -0.2584978 0.1658396 -1.559 0.120493 \\(DLTC_{t-1}\\) 0.0022016 0.0152876 0.144 0.885620 \\(DLCETE28_{t-1}\\) 0.0009547 0.0049115 0.194 0.846054 \\(DLIGAE_{t-1}\\) -0.2351453 0.0699797 -3.360 0.000917 *** \\(DLIPI_{t-1}\\) 0.2442406 0.0600502 4.067 6.62e-05 *** \\(DLINPC_{t-2}\\) -0.0775039 0.1694809 -0.457 0.647904 \\(DLTC_{t-2}\\) -0.0413316 0.0144650 -2.857 0.004680 ** \\(DLCETE28_{t-2}\\) 0.0005341 0.0048058 0.111 0.911612 \\(DLIGAE_{t-2}\\) -0.0646890 0.0693711 -0.933 0.352092 \\(DLIPI_{t-2}\\) 0.1796286 0.0620861 2.893 0.004195 ** \\(\\delta_4\\) 0.0030377 0.0008077 3.761 0.000217 *** Notas: *** signif. al 0.1% ** signif. al 1% * signif. al 5% Table 5.6: Matriz \\(\\mathbf{\\Sigma}_{\\hat{U}\\hat{U}}\\) estimada del VAR(2). \\(DLINPC_t\\) \\(DLTC_t\\) \\(DLCE28_t\\) \\(DLIGAE_t\\) \\(DLIGAE_t\\) \\(DLINPC_t\\) 3.95e-06 3.19e-06 -1.83e-06 -5.29-07 1.34e-06 \\(DLTC_t\\) 3.19e-06 5.04e-04 4.27e-04 9.81e-06 1.61e-05 \\(DLCE28_t\\) -1.83e-06 4.27e-04 4.63e-03 1.26e-05 2.76e-05 \\(DLIGAE_t\\) -5.29e-07 9.81e-06 1.26e-05 2.43e-05 8.75e-06 \\(DLIGAE_t\\) 1.34e-06 1.61e-05 2.76e-05 8.75e-06 3.13e-05 VAR_p &lt;- VAR(Datos, p = 2, type = &quot;const&quot;) summary(VAR_p) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: DLINPC, DLTC, DLCETE28, DLIGAE, DLIPI ## Deterministic variables: const ## Sample size: 279 ## Log Likelihood: 4142.895 ## Roots of the characteristic polynomial: ## 0.53 0.53 0.4501 0.4501 0.4425 0.4425 0.3251 0.3251 0.1677 0.1677 ## Call: ## VAR(y = Datos, p = 2, type = &quot;const&quot;) ## ## ## Estimation results for equation DLINPC: ## ======================================= ## DLINPC = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 0.387061 0.063100 6.134 0.0000000030610259 *** ## DLTC.l1 -0.004611 0.005286 -0.872 0.3839 ## DLCETE28.l1 0.001229 0.002078 0.592 0.5546 ## DLIGAE.l1 -0.022088 0.015077 -1.465 0.1441 ## DLIPI.l1 0.009878 0.019363 0.510 0.6104 ## DLINPC.l2 -0.006707 0.063621 -0.105 0.9161 ## DLTC.l2 0.009608 0.005496 1.748 0.0816 . ## DLCETE28.l2 0.001827 0.002051 0.891 0.3737 ## DLIGAE.l2 0.006544 0.014289 0.458 0.6473 ## DLIPI.l2 -0.015245 0.019632 -0.777 0.4381 ## const 0.002325 0.000294 7.908 0.0000000000000688 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.002176 on 268 degrees of freedom ## Multiple R-Squared: 0.1783, Adjusted R-squared: 0.1476 ## F-statistic: 5.815 on 10 and 268 DF, p-value: 0.00000006289 ## ## ## Estimation results for equation DLTC: ## ===================================== ## DLTC = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 -1.344803 0.745382 -1.804 0.07233 . ## DLTC.l1 0.315786 0.062442 5.057 0.00000079 *** ## DLCETE28.l1 -0.039912 0.024542 -1.626 0.10506 ## DLIGAE.l1 0.379306 0.178102 2.130 0.03411 * ## DLIPI.l1 -0.514827 0.228727 -2.251 0.02521 * ## DLINPC.l2 0.071779 0.751538 0.096 0.92398 ## DLTC.l2 -0.183813 0.064926 -2.831 0.00499 ** ## DLCETE28.l2 0.030125 0.024228 1.243 0.21481 ## DLIGAE.l2 0.070713 0.168797 0.419 0.67561 ## DLIPI.l2 -0.166610 0.231909 -0.718 0.47312 ## const 0.006463 0.003473 1.861 0.06388 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.0257 on 268 degrees of freedom ## Multiple R-Squared: 0.1425, Adjusted R-squared: 0.1105 ## F-statistic: 4.454 on 10 and 268 DF, p-value: 0.000008102 ## ## ## Estimation results for equation DLCETE28: ## ========================================= ## DLCETE28 = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 3.933798 1.861561 2.113 0.0355 * ## DLTC.l1 0.112997 0.155945 0.725 0.4693 ## DLCETE28.l1 0.113300 0.061292 1.849 0.0656 . ## DLIGAE.l1 -0.588758 0.444803 -1.324 0.1868 ## DLIPI.l1 1.174496 0.571237 2.056 0.0407 * ## DLINPC.l2 -2.213087 1.876936 -1.179 0.2394 ## DLTC.l2 0.117661 0.162151 0.726 0.4687 ## DLCETE28.l2 0.063398 0.060509 1.048 0.2957 ## DLIGAE.l2 -0.237974 0.421565 -0.565 0.5729 ## DLIPI.l2 1.110203 0.579183 1.917 0.0563 . ## const -0.007514 0.008674 -0.866 0.3872 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.06419 on 268 degrees of freedom ## Multiple R-Squared: 0.08823, Adjusted R-squared: 0.05421 ## F-statistic: 2.593 on 10 and 268 DF, p-value: 0.005122 ## ## ## Estimation results for equation DLIGAE: ## ======================================= ## DLIGAE = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 0.719855 0.393617 1.829 0.0685 . ## DLTC.l1 -0.218508 0.032974 -6.627 0.000000000188 *** ## DLCETE28.l1 0.007718 0.012960 0.596 0.5520 ## DLIGAE.l1 0.037559 0.094051 0.399 0.6900 ## DLIPI.l1 0.389894 0.120785 3.228 0.0014 ** ## DLINPC.l2 -0.737653 0.396868 -1.859 0.0642 . ## DLTC.l2 0.073830 0.034286 2.153 0.0322 * ## DLCETE28.l2 -0.013542 0.012794 -1.058 0.2908 ## DLIGAE.l2 -0.142845 0.089138 -1.603 0.1102 ## DLIPI.l2 -0.154835 0.122465 -1.264 0.2072 ## const 0.001637 0.001834 0.893 0.3729 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.01357 on 268 degrees of freedom ## Multiple R-Squared: 0.3272, Adjusted R-squared: 0.3021 ## F-statistic: 13.03 on 10 and 268 DF, p-value: &lt; 0.00000000000000022 ## ## ## Estimation results for equation DLIPI: ## ====================================== ## DLIPI = DLINPC.l1 + DLTC.l1 + DLCETE28.l1 + DLIGAE.l1 + DLIPI.l1 + DLINPC.l2 + DLTC.l2 + DLCETE28.l2 + DLIGAE.l2 + DLIPI.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 0.042383 0.326611 0.130 0.8968 ## DLTC.l1 -0.163742 0.027361 -5.985 0.00000000693 *** ## DLCETE28.l1 0.012143 0.010754 1.129 0.2598 ## DLIGAE.l1 0.098821 0.078041 1.266 0.2065 ## DLIPI.l1 0.067742 0.100224 0.676 0.4997 ## DLINPC.l2 -0.524835 0.329309 -1.594 0.1122 ## DLTC.l2 0.053353 0.028449 1.875 0.0618 . ## DLCETE28.l2 -0.016237 0.010616 -1.529 0.1273 ## DLIGAE.l2 -0.088483 0.073964 -1.196 0.2326 ## DLIPI.l2 -0.082982 0.101618 -0.817 0.4149 ## const 0.002361 0.001522 1.551 0.1220 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.01126 on 268 degrees of freedom ## Multiple R-Squared: 0.1978, Adjusted R-squared: 0.1679 ## F-statistic: 6.61 on 10 and 268 DF, p-value: 0.000000003715 ## ## ## ## Covariance matrix of residuals: ## DLINPC DLTC DLCETE28 DLIGAE DLIPI ## DLINPC 0.0000047345 -0.0000001939 0.000004559 0.000006652 0.00000700 ## DLTC -0.0000001939 0.0006606619 0.000369751 -0.000035441 -0.00003174 ## DLCETE28 0.0000045591 0.0003697510 0.004120747 0.000063618 0.00008525 ## DLIGAE 0.0000066522 -0.0000354410 0.000063618 0.000184234 0.00011641 ## DLIPI 0.0000069998 -0.0000317443 0.000085254 0.000116407 0.00012685 ## ## Correlation matrix of residuals: ## DLINPC DLTC DLCETE28 DLIGAE DLIPI ## DLINPC 1.000000 -0.003466 0.03264 0.22524 0.2856 ## DLTC -0.003466 1.000000 0.22409 -0.10159 -0.1097 ## DLCETE28 0.032640 0.224095 1.00000 0.07301 0.1179 ## DLIGAE 0.225240 -0.101585 0.07301 1.00000 0.7615 ## DLIPI 0.285629 -0.109656 0.11792 0.76147 1.0000 Finalmente, en el Cuadro 5.7 reportamos las pruebas de diagnóstico del VAR(2). Incluimos las pruebas de correlación serial (o autocorrelación), normalidad y de heterocedasticidad. De acuerdo con esa información, la correlación serial muestra que existe relación de la matriz de covarianzas no contemporánea considerando pocos rezagos, pero se elimina conforme los rezagos se incrementan. En cuanto a normalidad, se observa que los residuales no lo son, por lo que se requeriría mejorar la especificación del VAR. Finalmente, se observa que los residuales no son homocedásticos. Table 5.7: Pruebas de diagnóstico sobre los residuales del VAR(2). Estadística (rezagos) Coeficiente p-value Conclusión Correlación Serial (\\(\\chi^2 (2)\\)) 59.436 0.1696 Existe autocorrelación serial Correlación Serial (\\(\\chi^2 (4)\\)) 127.17 0.03461 No existe autocorrelación serial Correlación Serial (\\(\\chi^2 (6)\\)) 183.14 0.03393 No existe autocorrelación serial Normalidad - JB (\\(\\chi^2\\)) 2335 0.0000 Los residuales no son normales ARCH (\\(\\chi^2 (2)\\)) 691.58 0.0000 Los residuales no son homocedásticos ### Diagnostic tests #### Normalidad: normality.test(VAR_p) ## $JB ## ## JB-Test (multivariate) ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 36186, df = 10, p-value &lt; 0.00000000000000022 ## ## ## $Skewness ## ## Skewness only (multivariate) ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 1261, df = 5, p-value &lt; 0.00000000000000022 ## ## ## $Kurtosis ## ## Kurtosis only (multivariate) ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 34925, df = 5, p-value &lt; 0.00000000000000022 #### Autocorrelacion Serial: #### LAGS = 2: serial.test(VAR_p, lags.bg = 2, type = &quot;BG&quot;) ## ## Breusch-Godfrey LM test ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 74.839, df = 50, p-value = 0.013 #### LAGS = 4: serial.test(VAR_p, lags.bg = 4, type = &quot;BG&quot;) ## ## Breusch-Godfrey LM test ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 131.69, df = 100, p-value = 0.01849 #### LAGS = 6: serial.test(VAR_p, lags.bg = 6, type = &quot;BG&quot;) ## ## Breusch-Godfrey LM test ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 201.63, df = 150, p-value = 0.003147 #### Homocedasticidad: arch.test(VAR_p, lags.multi = 6) ## ## ARCH (multivariate) ## ## data: Residuals of VAR object VAR_p ## Chi-squared = 2122.4, df = 1350, p-value &lt; 0.00000000000000022 5.3 Análisis de Impulso-Respuesta Una de las grandes ventajas que aporta el análisis de los modelos VAR es el análisis de Impulso-Respuesta. Dicho análisis busca cuantificar el efecto que tiene en \\(\\mathbf{X}_t\\) una innovación o cambio en los residuales de cualquiera de las variables en un momento definido. Partamos de la ecuación (5.6) de forma que tenemos: \\[\\begin{eqnarray} \\mathbf{X}_t &amp; = &amp; \\mathbf{A}^{-1}(L) \\delta + \\mathbf{A}^{-1}(L) \\mathbf{U}_t \\nonumber \\\\ &amp; = &amp; \\mu + \\mathbf{B}(L) \\mathbf{U}_t \\nonumber \\\\ &amp; = &amp; \\mu + \\Psi_0 \\mathbf{U}_t + \\Psi_1 \\mathbf{U}_{t-1} + \\Psi_2 \\mathbf{U}_{t-2} + \\Psi_3 \\mathbf{U}_{t-3} + \\ldots \\end{eqnarray}\\] Donde \\(\\Psi_0 = I\\) y cada una de las \\(\\Psi_i = - \\mathbf{B}_i\\), \\(i = 1, 2, \\ldots\\). De esta forma se verifica el efecto que tiene en \\(\\mathbf{X}_t\\) cada una de las innovaciones pasadas. Por lo que el análisis de Impulso-Respuesta cuantifica el efecto de cada una de esas matrices en las que hemos descompuesto a \\(\\mathbf{B}(L)\\). . Retomando el modelo \\(VAR(2)\\) anteriormente estimado, en las siguientes figuras reportamos las gráficas de Impulso-Respuesta de la serie \\(DLTC_t\\) ante cambios en los residuales del resto de las series y de la propia serie. IR_DLINPC &lt;- irf(VAR_p, n.ahead = 12, boot = TRUE, ci = 0.95, response = &quot;DLINPC&quot;) IR_DLINPC ## ## Impulse response coefficients ## $DLINPC ## DLINPC ## [1,] 0.0021759002410 ## [2,] 0.0008094393250 ## [3,] 0.0002429936391 ## [4,] 0.0001048999556 ## [5,] 0.0000430519256 ## [6,] 0.0000257195308 ## [7,] 0.0000108405478 ## [8,] 0.0000011471582 ## [9,] -0.0000014256627 ## [10,] -0.0000009999499 ## [11,] -0.0000003443770 ## [12,] -0.0000001512886 ## [13,] -0.0000001157354 ## ## $DLTC ## DLINPC ## [1,] 0.0000000000000 ## [2,] -0.0000826837093 ## [3,] 0.0003117546911 ## [4,] 0.0002648974979 ## [5,] 0.0000587162871 ## [6,] -0.0000289491380 ## [7,] -0.0000105463736 ## [8,] 0.0000105785893 ## [9,] 0.0000076216336 ## [10,] -0.0000004038885 ## [11,] -0.0000021138204 ## [12,] -0.0000004691794 ## [13,] 0.0000003771599 ## ## $DLCETE28 ## DLINPC ## [1,] 0.00000000000000 ## [2,] 0.00006486928725 ## [3,] 0.00013182636385 ## [4,] 0.00003487943931 ## [5,] 0.00004740702286 ## [6,] 0.00003336371156 ## [7,] 0.00001571609856 ## [8,] 0.00000377046957 ## [9,] 0.00000005672766 ## [10,] 0.00000025569500 ## [11,] 0.00000033794064 ## [12,] 0.00000004626880 ## [13,] -0.00000011376620 ## ## $DLIGAE ## DLINPC ## [1,] 0.00000000000000 ## [2,] -0.00021156780483 ## [3,] -0.00017900748374 ## [4,] -0.00001453803984 ## [5,] 0.00004387550079 ## [6,] 0.00001348412556 ## [7,] -0.00000660541189 ## [8,] -0.00000344927548 ## [9,] 0.00000223865243 ## [10,] 0.00000206681740 ## [11,] 0.00000008258756 ## [12,] -0.00000046595182 ## [13,] -0.00000009698431 ## ## $DLIPI ## DLINPC ## [1,] 0.00000000000000 ## [2,] 0.00007038187903 ## [3,] -0.00011077803202 ## [4,] -0.00003218007799 ## [5,] -0.00001697303009 ## [6,] 0.00000048565947 ## [7,] 0.00001045292187 ## [8,] 0.00000469541763 ## [9,] 0.00000035996398 ## [10,] -0.00000051048646 ## [11,] 0.00000005294854 ## [12,] 0.00000026647657 ## [13,] 0.00000009015073 ## ## ## Lower Band, CI= 0.95 ## $DLINPC ## DLINPC ## [1,] 0.001864170847 ## [2,] 0.000499776775 ## [3,] 0.000002450675 ## [4,] -0.000078104730 ## [5,] -0.000075950781 ## [6,] -0.000037958817 ## [7,] -0.000025483439 ## [8,] -0.000012332298 ## [9,] -0.000011329253 ## [10,] -0.000007641493 ## [11,] -0.000004043746 ## [12,] -0.000001841703 ## [13,] -0.000001172170 ## ## $DLTC ## DLINPC ## [1,] 0.000000000000 ## [2,] -0.000286990353 ## [3,] 0.000061870696 ## [4,] 0.000078542144 ## [5,] -0.000039820370 ## [6,] -0.000095870183 ## [7,] -0.000050900347 ## [8,] -0.000019789658 ## [9,] -0.000005574237 ## [10,] -0.000008547257 ## [11,] -0.000009189057 ## [12,] -0.000004530429 ## [13,] -0.000001945539 ## ## $DLCETE28 ## DLINPC ## [1,] 0.000000000000 ## [2,] -0.000230649362 ## [3,] -0.000196536071 ## [4,] -0.000126449052 ## [5,] -0.000058700460 ## [6,] -0.000023095154 ## [7,] -0.000007659372 ## [8,] -0.000008557329 ## [9,] -0.000008242220 ## [10,] -0.000005060508 ## [11,] -0.000002432306 ## [12,] -0.000001136291 ## [13,] -0.000001066601 ## ## $DLIGAE ## DLINPC ## [1,] 0.000000000000 ## [2,] -0.000410323921 ## [3,] -0.000418493922 ## [4,] -0.000146714896 ## [5,] -0.000044408954 ## [6,] -0.000035327961 ## [7,] -0.000035119635 ## [8,] -0.000022423812 ## [9,] -0.000012260613 ## [10,] -0.000003350492 ## [11,] -0.000003212534 ## [12,] -0.000004039474 ## [13,] -0.000002483970 ## ## $DLIPI ## DLINPC ## [1,] 0.0000000000000 ## [2,] -0.0001982996896 ## [3,] -0.0003103493634 ## [4,] -0.0001445269721 ## [5,] -0.0001147657072 ## [6,] -0.0000494580078 ## [7,] -0.0000148574620 ## [8,] -0.0000103989132 ## [9,] -0.0000059299583 ## [10,] -0.0000046364719 ## [11,] -0.0000029746270 ## [12,] -0.0000012355125 ## [13,] -0.0000008043852 ## ## ## Upper Band, CI= 0.95 ## $DLINPC ## DLINPC ## [1,] 0.002423055334 ## [2,] 0.001069138548 ## [3,] 0.000514403106 ## [4,] 0.000299906999 ## [5,] 0.000185297033 ## [6,] 0.000101768840 ## [7,] 0.000056864142 ## [8,] 0.000028728717 ## [9,] 0.000018624195 ## [10,] 0.000012817976 ## [11,] 0.000006987585 ## [12,] 0.000004523406 ## [13,] 0.000002520736 ## ## $DLTC ## DLINPC ## [1,] 0.000000000000 ## [2,] 0.000137344529 ## [3,] 0.000583804216 ## [4,] 0.000433186162 ## [5,] 0.000171212748 ## [6,] 0.000036307348 ## [7,] 0.000034282255 ## [8,] 0.000038723787 ## [9,] 0.000024964010 ## [10,] 0.000009783037 ## [11,] 0.000003517692 ## [12,] 0.000003748501 ## [13,] 0.000003845745 ## ## $DLCETE28 ## DLINPC ## [1,] 0.000000000000 ## [2,] 0.000304305857 ## [3,] 0.000373465696 ## [4,] 0.000167853443 ## [5,] 0.000127305791 ## [6,] 0.000089751434 ## [7,] 0.000049983292 ## [8,] 0.000025579016 ## [9,] 0.000012448007 ## [10,] 0.000006485072 ## [11,] 0.000004515005 ## [12,] 0.000002515525 ## [13,] 0.000001179168 ## ## $DLIGAE ## DLINPC ## [1,] 0.000000000000 ## [2,] -0.000006014034 ## [3,] 0.000035752798 ## [4,] 0.000101880979 ## [5,] 0.000107697317 ## [6,] 0.000067291001 ## [7,] 0.000019302644 ## [8,] 0.000009260325 ## [9,] 0.000010330953 ## [10,] 0.000010351320 ## [11,] 0.000004299667 ## [12,] 0.000002280664 ## [13,] 0.000001077557 ## ## $DLIPI ## DLINPC ## [1,] 0.0000000000000 ## [2,] 0.0002938283073 ## [3,] 0.0001164391542 ## [4,] 0.0000642643310 ## [5,] 0.0000621769713 ## [6,] 0.0000441663489 ## [7,] 0.0000332198998 ## [8,] 0.0000181489496 ## [9,] 0.0000082995640 ## [10,] 0.0000034179891 ## [11,] 0.0000034672138 ## [12,] 0.0000024110405 ## [13,] 0.0000009791902 #plot(IR_DLINPC) IR_DLTC &lt;- irf(VAR_p, n.ahead = 12, boot = TRUE, ci = 0.95, response = &quot;DLTC&quot;) plot(IR_DLTC) Figure 5.4: Impulso - Respuesta en \\(DLTC_t\\) Figure 5.5: Impulso - Respuesta en \\(DLTC_t\\) Figure 5.6: Impulso - Respuesta en \\(DLTC_t\\) Figure 5.7: Impulso - Respuesta en \\(DLTC_t\\) Figure 5.8: Impulso - Respuesta en \\(DLTC_t\\) Los resultados muestran que la respuesta de \\(DLTC_t\\) ante impulsos en los términos de error fue estadísticamente significativo sólo para alguunos de los casos y en periodos cortos de tiempo. El resto de los resultados de Impulso-Respuesta se encuentra en el Scrip llamado Clase 15 que se ubica en el repositorio de GitHub. 5.4 VAR Estructural El enfoque VAR de Sims (1980) tiene la propiedad deseable de que todas las variables se tratan simétricamente, de modo que todas las variables son endógenas en conjunto y el econometrista no depende de ninguna restricción de identificación. Consideremos un sistema VAR de primer orden del tipo representado en la ecuación (5.2): \\[\\begin{equation} \\mathbf{X}_t = \\mathbf{\\delta} + A_1 \\mathbf{X}_{t-1} + \\mathbf{U}_{t} (\\#eq:VAR_p2) \\end{equation}\\] Sin embargo, dada la naturaleza un tanto ad hoc de la descomposición de Choleski, la belleza del enfoque parece disminuida cuando se construyen funciones de respuesta al impulso y descomposiciones de varianza del error de pronóstico. Además, el enfoque VAR ha sido criticado por estar desprovisto de cualquier contenido económico. El único papel del economista es sugerir las variables apropiadas para incluir en el VAR. A partir de ese punto, el procedimiento es casi mecánico. Sin embargo, es posible utilizar una teoría económica para imponer restricciones a las variables de modo que los resultados no sean ad hoc. A menos que el modelo estructural subyacente pueda identificarse a partir del modelo VAR de forma reducida, las innovaciones en una descomposición de Choleski no tienen una interpretación económica directa. Sin embargo, en lugar de utilizar una descomposición de Choleski, es posible imponer restricciones a los errores para identificar completamente los shocks estructurales de una manera que sea consistente con un modelo económico subyacente. Consideremos un VAR(2) en el que \\(\\mathbf{X}_t\\) contiene a dos series \\(Y_t\\) y \\(Z_t\\): \\[\\begin{eqnarray} Y_t &amp; = &amp; a_{10} + a_{11} Y_{t - 1} + a_{12} Z_{t-1} + U_{1t} \\nonumber \\\\ Z_t &amp; = &amp; a_{20} + a_{21} Y_{t - 1} + a_{22} Z_{t-1} + U_{2t} \\tag{5.7} \\end{eqnarray}\\] Para nuestros propósitos, el punto importante a destacar es que los dos términos de error \\(U_{1t}\\) y \\(U_{2t}\\) son en realidad compuestos de los shocks subyacentes \\(\\varepsilon_{Yt}\\) y \\(\\varepsilon_{Zt}\\). Así, \\[\\begin{equation*} \\mathbf{U}_t = \\begin{bmatrix} U_{1t} \\\\ U_{2t} \\end{bmatrix} = \\begin{bmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\end{bmatrix} \\begin{bmatrix} \\varepsilon_{Yt} \\\\ \\varepsilon_{Zt} \\end{bmatrix} \\end{equation*}\\] Aunque estos shocks compuestos son los errores de pronóstico de un valor adelante en \\(Y_t\\) y \\(Z_t\\), no tienen una interpretación estructural. Por lo tanto, existe una diferencia importante entre usar VAR para pronósticos y análisis económico. En nuestra representación (5.7) \\(U_{1t}\\) y \\(U_{2t}\\) son errores de pronóstico. Si solo nos interesa el pronóstico, los componentes de los errores de pronóstico no son importantes. Dado el modelo económico, \\(\\varepsilon_{Yt}\\) y \\(\\varepsilon_{Zt}\\) son los cambios autónomos en \\(Y_t\\) y \\(Z_t\\) en el período \\(t\\), respectivamente. Si queremos obtener las funciones de impulso - respuesta o las descomposiciones de varianza, es necesario usar los shocks estructurales (es decir, \\(\\varepsilon_{Yt}\\) y \\(\\varepsilon_{Zt}\\)), no los errores de pronóstico. El objetivo de un VAR estructural es usar la teoría económica (en lugar de la descomposición de Choleski) para recuperar las innovaciones estructurales a partir de los residuos \\(U_{1t}\\) y \\(U_{2t}\\). Una forma de sustituir el VAR estructural es ordenar las series en el VAR de las más correlacionadas a las menos correlacionadas. Si el coeficiente de correlación entre \\(U_{1t}\\) y \\(U_{2t}\\) es bajo, es poco probable que el ordenamiento sea importante. Sin embargo, en un VAR con varias variables, es improbable que todas las correlaciones sean pequeñas. Después de todo, al seleccionar las variables que se incluirán en un modelo, es probable que se elijan variables que muestren fuertes comovimientos. Cuando los residuos de un VAR están correlacionados, no es práctico probar todos los ordenamientos alternativos. Con un modelo de cuatro variables, hay 24 (es decir, \\(4!\\)) ordenamientos posibles. Sims (1986) y Bernanke (1986) propusieron modelar las innovaciones utilizando el análisis económico. La idea básica es estimar las relaciones entre los shocks estructurales utilizando un modelo económico. Para entender el procedimiento, es útil examinar la relación entre los errores de pronóstico y las innovaciones estructurales en un VAR de \\(n\\) variables. Dado que la relación es invariante a la longitud de los rezagos, podemos escribir el VAR(1) como: \\[\\begin{eqnarray*} \\mathbf{X}_t &amp; = &amp; \\boldsymbol{\\delta} + \\mathbf{A_1} \\mathbf{X}_{t-1} + \\mathbf{U}_{t} \\\\ &amp; = &amp; \\mathbf{B}^{-1} \\mathbf{\\Gamma}_{0} + \\mathbf{B}^{-1} \\mathbf{\\Gamma}_{1} \\mathbf{X}_{t-1} + \\mathbf{B}^{-1} \\mathbf{\\varepsilon}_{t} \\\\ \\mathbf{B} \\mathbf{X}_t &amp; = &amp; \\mathbf{\\Gamma}_{0} + \\mathbf{\\Gamma}_{1} \\mathbf{X}_{t-1} + \\mathbf{\\varepsilon}_{t} \\end{eqnarray*}\\] Donde \\(\\mathbf{B}\\) es una matriz que establece la estructura entre las variables. Sin embargo, la selección de los distintos \\(bij \\in \\mathbf{B}\\) no puede ser completamente arbitraria. La cuestión es restringir el sistema de modo que: Para resolver este problema de identificación, simplemente cuente las ecuaciones y las incógnitas. Usando MCO, podemos obtener la matriz de varianza/covarianza \\(\\mathbf{\\Sigma}\\): \\[\\begin{equation*} \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1n} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ \\sigma_{n1} &amp; \\sigma_{n2} &amp; \\cdots &amp; \\sigma_n^2 \\\\ \\end{bmatrix} \\end{equation*}\\] Donde cada elemento de \\(\\mathbf{\\Sigma}\\) se estima como la suma: \\[\\begin{equation*} \\sigma_{ij} = \\frac{\\sum_{i = 1}^{T} U_{it} U_{jt} }{T} \\end{equation*}\\] Como \\(\\mathbf{\\Sigma}\\) es simétrica, contiene sólo \\((n + 1)n/2\\) elementos distintos. Hay \\(n\\) elementos a lo largo de la diagonal principal, \\((n - 1)\\) a lo largo del primer elemento fuera de la diagonal, \\((n - 2)\\) a lo largo del siguiente elemento fuera de la diagonal, así sucesivamente, y un elemento de esquina para un total de \\((n2 + n)/2\\) elementos libres. Dado que los elementos diagonales de \\(\\mathbf{B}\\) son todos la unidad, \\(\\mathbf{B}\\) contiene \\(n^2 - n\\) valores desconocidos. Además, existen los \\(n\\) valores desconocidos \\(Var(\\varepsilon_{it})\\) para un total de \\(n^2\\) valores desconocidos en el modelo estructural [es decir, los \\(n^2 - n\\) valores de \\(\\mathbf{B}\\) más los \\(n\\) valores \\(Var(\\varepsilon_{it})\\)]. Ahora la respuesta al problema de identificación es simple; para identificar las \\(n^2\\) incógnitas a partir de los elementos independientes conocidos \\((n^2 + n)/2\\) de \\(\\mathbf{\\Sigma}\\), es necesario imponer restricciones adicionales \\(n^2 - [(n^2 + n)/2] = (n^2 + n)/2\\) al sistema. Este resultado se generaliza a un modelo con \\(p\\) rezagos: Para identificar el modelo estructural a partir de un VAR estimado, es necesario imponer restricciones \\((n^2 - n)/2\\) al modelo estructural. Para aquellos que desean un poco más de formalidad, escriba la matriz de varianzas / covarianzas de los residuos de regresión como: \\[\\begin{equation*} \\mathbb{E}[\\hat{\\mathbf{U}}_{t} \\hat{\\mathbf{U}}_{t}&#39;] = \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1n} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ \\sigma_{n1} &amp; \\sigma_{n2} &amp; \\cdots &amp; \\sigma_n^2 \\\\ \\end{bmatrix} \\end{equation*}\\] Dado que: \\(\\mathbf{U}_t = \\mathbf{B}^{-1} \\mathbf{\\varepsilon}_t\\), de esta forma: \\[\\begin{equation*} \\mathbb{E}[\\hat{\\mathbf{U}}_{t} \\hat{\\mathbf{U}}_{t}&#39;] = \\mathbb{E}[(\\mathbf{B}^{-1} \\mathbf{\\varepsilon}_t)(\\mathbf{B}^{-1} \\mathbf{\\varepsilon}_t)&#39;] = \\mathbb{E}[\\mathbf{B}^{-1} \\mathbf{\\varepsilon}_t \\mathbf{\\varepsilon}_t&#39; (\\mathbf{B}^{-1})&#39;] = \\mathbf{B}^{-1} \\mathbb{E}[\\mathbf{\\varepsilon}_t \\mathbf{\\varepsilon}_t&#39;] (\\mathbf{B}^{-1})&#39; \\end{equation*}\\] Donde \\(\\mathbb{E}[\\mathbf{\\varepsilon}_t \\mathbf{\\varepsilon}_t&#39;]\\) es varianza estructural de las innovaciones y \\(\\mathbf{B}^{-1}\\) es la matriz que permite darle estructura a la varianza estructural. De esta forma: \\[\\begin{equation*} \\mathbf{\\Sigma_{\\varepsilon}} = \\mathbb{E}[\\mathbf{\\varepsilon}_t \\mathbf{\\varepsilon}_t&#39;] \\end{equation*}\\] . Retomemos el conjunto de variables del ejemplo de una VAR(p) antes discutido, pero solo consideremos las variables domésticas y en orden de las más endógenas a las menos endógenas (\\(\\Delta LINPC\\), \\(\\Delta LIGAE\\), \\(\\Delta LCETE28\\), \\(\\Delta LTC\\))–dejando fuera al IPI de Estados Unidos–. En este caso requerimos establecer las restricciones \\(\\frac{K^2 - K}{2}\\). Pensemos en una matriz: \\[\\begin{equation*} \\begin{pmatrix} \\text{NA} &amp; \\text{NA} &amp; \\text{NA} &amp; \\text{NA}\\\\ 0 &amp; \\text{NA} &amp; \\text{NA} &amp; \\text{NA}\\\\ 0 &amp; 0 &amp; \\text{NA} &amp; \\text{NA}\\\\ 0 &amp; 0 &amp; 0 &amp; \\text{NA} \\end{pmatrix} \\end{equation*}\\] Y en una matriz que capture los efectos individuales: \\[\\begin{equation*} \\begin{pmatrix} \\text{NA} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\text{NA} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\text{NA} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\text{NA} \\end{pmatrix} \\end{equation*}\\] Donde estaríamos imponiendo la restricción de que las variables solo se relacionan en los casos que son distintos de cero (0). Dado lo anterior, obtendríamos el siguiente resultado de la matriz de covarianza restringida: \\[\\begin{equation*} \\begin{pmatrix} 1 &amp; -0.03652 &amp; -0.000252 &amp; -0.001698 \\\\ 0 &amp; 1.00000 &amp; -0.024109 &amp; 0.077889 \\\\ 0 &amp; 0 &amp; 1.000000 &amp; -0.503508 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\end{equation*}\\] Y de la matiz de efectos individuales: \\[\\begin{equation*} \\begin{pmatrix} 0.002113 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.01369 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.0634 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.02585 \\end{pmatrix} \\end{equation*}\\] # Datos &lt;- data.frame(cbind(DLINPC, DLIGAE, DLCETE28, DLTC)) Datos &lt;- ts(Datos, start = c(2000, 2), freq = 12) VARselect(Datos, lag.max = 12, type = &quot;const&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 2 2 1 2 ## ## $criteria ## 1 2 ## AIC(n) -33.521409602805981364781 -33.616338087137677348437 ## HQ(n) -33.414075512840469173170 -33.423136725199746877024 ## SC(n) -33.254144816218484947967 -33.135261471280180956001 ## FPE(n) 0.000000000000002765949 0.000000000000002515668 ## 3 4 ## AIC(n) -33.565395809217491773779 -33.541826085160096226900 ## HQ(n) -33.286327175307150127992 -33.176890179277343406739 ## SC(n) -32.870507364089995405720 -32.633125810762606988646 ## FPE(n) 0.000000000000002647675 0.000000000000002711834 ## 5 6 ## AIC(n) -33.477447219141787115859 -33.417817003960756494507 ## HQ(n) -33.026644041286616015896 -32.881146554133181325597 ## SC(n) -32.354935115474297901983 -32.081493071023274410436 ## FPE(n) 0.000000000000002893893 0.000000000000003074404 ## 7 8 ## AIC(n) -33.44082242666117110730 -33.416926070357718003834 ## HQ(n) -32.81828470486117765859 -32.708521076585313380747 ## SC(n) -31.89068666445368549489 -31.652978478880235968518 ## FPE(n) 0.00000000000000300813 0.000000000000003085823 ## 9 10 ## AIC(n) -33.373717109782838008414 -33.317048231898660048955 ## HQ(n) -32.579444844038022210952 -32.436908694181433077119 ## SC(n) -31.395957689035355997476 -31.125476981881185167822 ## FPE(n) 0.000000000000003228696 0.000000000000003425694 ## 11 12 ## AIC(n) -33.252581037301176536403 -33.212431345326635323545 ## HQ(n) -32.286574227611531284765 -32.160557263664578897533 ## SC(n) -30.847197958013701679647 -30.593236436769156938453 ## FPE(n) 0.000000000000003665237 0.000000000000003829719 VAR_p &lt;- VAR(Datos, p = 2, type = &quot;const&quot;) summary(VAR_p) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: DLINPC, DLIGAE, DLCETE28, DLTC ## Deterministic variables: const ## Sample size: 279 ## Log Likelihood: 3139.909 ## Roots of the characteristic polynomial: ## 0.5346 0.5346 0.4467 0.4467 0.4372 0.4372 0.2395 0.2395 ## Call: ## VAR(y = Datos, p = 2, type = &quot;const&quot;) ## ## ## Estimation results for equation DLINPC: ## ======================================= ## DLINPC = DLINPC.l1 + DLIGAE.l1 + DLCETE28.l1 + DLTC.l1 + DLINPC.l2 + DLIGAE.l2 + DLCETE28.l2 + DLTC.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 0.3936901 0.0621471 6.335 0.0000000009896467 *** ## DLIGAE.l1 -0.0178878 0.0092660 -1.930 0.0546 . ## DLCETE28.l1 0.0011197 0.0020507 0.546 0.5855 ## DLTC.l1 -0.0043366 0.0052130 -0.832 0.4062 ## DLINPC.l2 -0.0162582 0.0621158 -0.262 0.7937 ## DLIGAE.l2 -0.0022728 0.0088328 -0.257 0.7971 ## DLCETE28.l2 0.0018184 0.0020464 0.889 0.3750 ## DLTC.l2 0.0092343 0.0054652 1.690 0.0922 . ## const 0.0023387 0.0002932 7.976 0.0000000000000432 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.002172 on 270 degrees of freedom ## Multiple R-Squared: 0.175, Adjusted R-squared: 0.1505 ## F-statistic: 7.157 on 8 and 270 DF, p-value: 0.0000000135 ## ## ## Estimation results for equation DLIGAE: ## ======================================= ## DLIGAE = DLINPC.l1 + DLIGAE.l1 + DLCETE28.l1 + DLTC.l1 + DLINPC.l2 + DLIGAE.l2 + DLCETE28.l2 + DLTC.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 0.944639 0.397173 2.378 0.01808 * ## DLIGAE.l1 0.249038 0.059217 4.205 0.000035471815 *** ## DLCETE28.l1 0.008873 0.013106 0.677 0.49900 ## DLTC.l1 -0.221922 0.033315 -6.661 0.000000000152 *** ## DLINPC.l2 -1.037719 0.396972 -2.614 0.00945 ** ## DLIGAE.l2 -0.234709 0.056449 -4.158 0.000043182412 *** ## DLCETE28.l2 -0.012941 0.013078 -0.990 0.32328 ## DLTC.l2 0.062581 0.034927 1.792 0.07429 . ## const 0.001917 0.001874 1.023 0.30737 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.01388 on 270 degrees of freedom ## Multiple R-Squared: 0.291, Adjusted R-squared: 0.27 ## F-statistic: 13.85 on 8 and 270 DF, p-value: &lt; 0.00000000000000022 ## ## ## Estimation results for equation DLCETE28: ## ========================================= ## DLCETE28 = DLINPC.l1 + DLIGAE.l1 + DLCETE28.l1 + DLTC.l1 + DLINPC.l2 + DLIGAE.l2 + DLCETE28.l2 + DLTC.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 4.480766 1.851857 2.420 0.0162 * ## DLIGAE.l1 0.209477 0.276107 0.759 0.4487 ## DLCETE28.l1 0.136087 0.061108 2.227 0.0268 * ## DLTC.l1 0.052508 0.155336 0.338 0.7356 ## DLINPC.l2 -2.845520 1.850923 -1.537 0.1254 ## DLIGAE.l2 0.388994 0.263200 1.478 0.1406 ## DLCETE28.l2 0.068584 0.060978 1.125 0.2617 ## DLTC.l2 0.096184 0.162852 0.591 0.5553 ## const -0.007588 0.008738 -0.868 0.3860 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.06473 on 270 degrees of freedom ## Multiple R-Squared: 0.06608, Adjusted R-squared: 0.03841 ## F-statistic: 2.388 on 8 and 270 DF, p-value: 0.01678 ## ## ## Estimation results for equation DLTC: ## ===================================== ## DLTC = DLINPC.l1 + DLIGAE.l1 + DLCETE28.l1 + DLTC.l1 + DLINPC.l2 + DLIGAE.l2 + DLCETE28.l2 + DLTC.l2 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## DLINPC.l1 -1.610981 0.739621 -2.178 0.03026 * ## DLIGAE.l1 0.062128 0.110275 0.563 0.57364 ## DLCETE28.l1 -0.045981 0.024406 -1.884 0.06064 . ## DLTC.l1 0.332109 0.062040 5.353 0.000000185 *** ## DLINPC.l2 0.404104 0.739248 0.547 0.58508 ## DLIGAE.l2 -0.020673 0.105121 -0.197 0.84424 ## DLCETE28.l2 0.028537 0.024354 1.172 0.24233 ## DLTC.l2 -0.171879 0.065042 -2.643 0.00871 ** ## const 0.006309 0.003490 1.808 0.07173 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.02585 on 270 degrees of freedom ## Multiple R-Squared: 0.1261, Adjusted R-squared: 0.1002 ## F-statistic: 4.871 on 8 and 270 DF, p-value: 0.00001254 ## ## ## ## Covariance matrix of residuals: ## DLINPC DLIGAE DLCETE28 DLTC ## DLINPC 0.0000047185 0.000006982 0.000004359 -0.0000003849 ## DLIGAE 0.0000069818 0.000192716 0.000074798 -0.0000439413 ## DLCETE28 0.0000043588 0.000074798 0.004189604 0.0003364986 ## DLTC -0.0000003849 -0.000043941 0.000336499 0.0006683077 ## ## Correlation matrix of residuals: ## DLINPC DLIGAE DLCETE28 DLTC ## DLINPC 1.000000 0.23153 0.03100 -0.006853 ## DLIGAE 0.231529 1.00000 0.08324 -0.122441 ## DLCETE28 0.031002 0.08324 1.00000 0.201098 ## DLTC -0.006853 -0.12244 0.20110 1.000000 ### Create Restrictions Matrix: a.mat &lt;- diag(4) diag(a.mat) &lt;- NA a.mat[1, 2] &lt;- NA a.mat[1, 3] &lt;- NA a.mat[1, 4] &lt;- NA a.mat[2, 3] &lt;- NA a.mat[2, 4] &lt;- NA a.mat[3, 4] &lt;- NA a.mat ## [,1] [,2] [,3] [,4] ## [1,] NA NA NA NA ## [2,] 0 NA NA NA ## [3,] 0 0 NA NA ## [4,] 0 0 0 NA #### individual shocks b.mat &lt;- diag(4) diag(b.mat) &lt;- NA b.mat ## [,1] [,2] [,3] [,4] ## [1,] NA 0 0 0 ## [2,] 0 NA 0 0 ## [3,] 0 0 NA 0 ## [4,] 0 0 0 NA ### SVAR Estimation #?SVAR SVAR_p &lt;- SVAR(VAR_p, Amat = a.mat, Bmat = b.mat, max.iter = 10000, hessian = TRUE) SVAR_p ## ## SVAR Estimation Results: ## ======================== ## ## ## Estimated A matrix: ## DLINPC DLIGAE DLCETE28 DLTC ## DLINPC 1 -0.03652 -0.000252 -0.001698 ## DLIGAE 0 1.00000 -0.024109 0.077889 ## DLCETE28 0 0.00000 1.000000 -0.503508 ## DLTC 0 0.00000 0.000000 1.000000 ## ## Estimated B matrix: ## DLINPC DLIGAE DLCETE28 DLTC ## DLINPC 0.002113 0.00000 0.0000 0.00000 ## DLIGAE 0.000000 0.01369 0.0000 0.00000 ## DLCETE28 0.000000 0.00000 0.0634 0.00000 ## DLTC 0.000000 0.00000 0.0000 0.02585 Como resultado, obtenemos las siguientes Impulse-Response, que tienen la característica de que representan un VAR con la estructura definida en las matrices anteriores. # IR_DLINPC &lt;- irf(SVAR_p, n.ahead = 12, boot = TRUE, ci = 0.95, response = &quot;DLINPC&quot;) plot(IR_DLINPC) Figure 5.9: Impulso - Respuesta en \\(DLINPC_t\\) Figure 5.10: Impulso - Respuesta en \\(DLINPC_t\\) Figure 5.11: Impulso - Respuesta en \\(DLINPC_t\\) Figure 5.12: Impulso - Respuesta en \\(DLINPC_t\\) 5.5 Otras opciones del análisis impulso-respuesta IR_DLINPC_2 &lt;- irf(SVAR_p, n.ahead = 12, boot = TRUE, ci = 0.95, response = &quot;DLINPC&quot;, ortho = TRUE, cumulative = FALSE) ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. ## Warning in SVAR(x = varboot, Amat = a.mat, Bmat = b.mat, max.iter = 10000, : ## The AB-model is just identified. No test possible. plot(IR_DLINPC_2) 5.6 Cointegración Hasta ahora en el curso hemos usado el supuesto de que las series son estacionarias para el conjunto de técnicas \\(ARMA(p,q)\\) y \\(VAR(p)\\). No obstante, dado que relajamos el supuesto de estacionariedad (incluyendo la estacionariedad en varianza) y que establecimos una serie de pruebas para determinar cuándo una serie es estadísticamente estacionaria, ahora podemos plantear una técnica llamada Cointegración. Para esta técnica consideraremos sólo series que son \\(I(1)\\) y reconoceremos que se originó con los trabajos de Engle y Granger (1987), Stock (1987) y Johansen (1988). 5.6.1 Definición y propiedades del proceso de cointegración Cointegración puede ser caracterizada o definida en palabras sencillas como que dos o más variables tienen una relación común estable en el largo plazo. Es decir, estas no suelen tomar caminos o trayectorias diferentes, excepto por períodos de tiempo transitorios y eventuales. A continuación, utilizaremos la definición de Engle y Granger (1984) de cointegración. Sea \\(\\mathbf{Y}\\) un vector de k-series de tiempo, decimos que los elementos en \\(\\mathbf{Y}\\) están cointegrados en un orden (d, c), es decir, \\(\\mathbf{Y} \\sim CI(d, c)\\), si todos los elementos de \\(\\mathbf{Y}\\) son series integradas de orden d, I(d), y si existe al menos una combinación lineal no trivial \\(\\mathbf{Z}\\) de esas variables que es de orden I(d - c), donde \\(d \\geq c &gt; 0\\), si y sólo si: \\[\\begin{equation} \\boldsymbol{\\beta}_i&#39; \\mathbf{Y}_t = \\mathbf{Y}_{it} \\sim I(d-c) \\end{equation}\\] Donde \\(i = 1, 2, \\ldots, r\\) y \\(r &lt; k\\). A los diferentes vectores \\(\\boldsymbol{\\beta}_i\\) se les denomina como vectores de cointegración. El rango de la matriz de vectores de cointegración \\(r\\) es el número de vectores de cointegración linealmente independientes. En general diremos que los vectores de la matriz de cointegración \\(\\boldsymbol{\\beta}\\) tendrán la forma de: \\[\\begin{equation} \\boldsymbol{\\beta}&#39; \\mathbf{Y}_t = \\mathbf{Z}_t \\end{equation}\\] Antes de continuar hagamos algunas observaciones. Si todas las variables de \\(\\mathbf{Y}\\) son I(1) y \\(0 \\leq r &lt; k\\), diremos que las series no cointegran si \\(r = 0\\). Si esto pasa, entonces, como demostraremos más adelante, la mejor opción será estimar un modelo VAR(p) en diferencias. Adicionalmente, asumiremos que \\(c = d = 1\\), por lo que la relación de cointegración, en su caso, generará combinaciones lineales \\(\\mathbf{Z}\\) estacionarias. 5.6.2 Cointegración para modelos de más de una ecuación o para modelos basados en Vectores Autoregresivos Sean \\(Y_1, Y_2, \\ldots, Y_k\\) son series que forman \\(\\mathbf{Y}\\) y que todas son I(1), entonces los siguientes casos son posibles: Si \\(r = 1\\) entonces se trata de un caso de cointegración de Granger. Si \\(r \\geq 1\\) entonces se trata de un caso de cointegración múltiple de Johansen. Por lo anterior, en este curso analizaremos el caso de Cointegración de Johansen. Ahora plantearemos la forma de estimar el proceso de cointegración. El primer paso para ello es determinar un modelo VAR(p) con las k-series no estacionarias (series en niveles)–en este punto se vuelve fundamental caracterizar las series a través de pruebas de raíces unitarias–. Elegimos el valor de \\(p\\) mediante el uso de los criterios de información. De esta forma tendremos una especificación similar a: \\[\\begin{equation} \\mathbf{Y}_t = \\sum_{j=1}^p \\mathbf{A}_j \\mathbf{Y}_{t-j} + \\mathbf{D}_t + \\mathbf{U}_t \\tag{5.8} \\end{equation}\\] Donde \\(\\mathbf{U}_t\\) es un término de error k-dimensional puramente aleatorio; \\(\\mathbf{D}_t\\) contiene los componentes determinísticos de constante y tendencia, y \\(\\mathbf{A}_i\\), \\(i = 1, 2, \\ldots, p\\), son matrices de \\(k \\times k\\) coeficientes. Notemos que el VAR(p) involucrado en este caso, a diferencia del VAR anteriormente estudiado, puede incluir un término de tendencia. Esto en razón de que hemos relajado el concepto de estacionariedad. Si reescribimos la ecuación (5.8) en su forma de Vector Corrector de Errores (VEC, por sus siglas en inglés) tenemos: \\[\\begin{eqnarray} \\mathbf{Y}_t - \\mathbf{Y}_{t-1} &amp; = &amp; \\Delta \\mathbf{Y}_t \\nonumber \\\\ &amp; = &amp; \\sum_{j=1}^p \\mathbf{A}_j \\mathbf{Y}_{t-j} + \\mathbf{D}_t - \\mathbf{Y}_{t-1} + \\mathbf{U}_t \\nonumber \\\\ &amp; = &amp; (\\mathbf{A}_1 - \\mathbf{I}) \\mathbf{Y}_{t-1} + \\mathbf{A}_2 \\mathbf{Y}_{t-2} + \\ldots + \\mathbf{A}_p \\mathbf{Y}_{t-p} + \\mathbf{D}_t + \\mathbf{U}_t \\nonumber \\\\ &amp; = &amp; \\left( \\sum_{j=1}^{p} \\mathbf{A}_j - \\mathbf{I} \\right) \\mathbf{Y}_{t-1} + \\sum_{j=1}^{p-1} \\mathbf{A}^*_j \\Delta \\mathbf{Y}_{t-j} + \\mathbf{D}_t \\mathbf{U}_t \\nonumber \\\\ &amp; = &amp; - \\left( \\mathbf{I} - \\sum_{j=1}^{p} \\mathbf{A}_j \\right) \\mathbf{Y}_{t-1} + \\sum_{j=1}^{p-1} \\mathbf{A}^*_j \\Delta \\mathbf{Y}_{t-j} + \\mathbf{D}_t \\mathbf{U}_t \\nonumber \\\\ \\Delta \\mathbf{Y}_t &amp; = &amp; - \\Pi \\mathbf{Y}_{t-1} + \\sum_{j=1}^{p-1} \\mathbf{A}^*_j \\Delta \\mathbf{Y}_{t-j} + \\mathbf{D}_t + \\mathbf{U}_t \\tag{5.9} \\end{eqnarray}\\] Donde \\(\\mathbf{A}_j^* = - \\sum_{i=j+1}^p \\mathbf{A}_i\\), \\(i = 1, 2, \\ldots, p-1\\), y la matriz \\(\\Pi\\) representa todas las relaciones de largo plazo entre las variables, por lo que la matriz es de rango completo \\(k \\times k\\). Por lo tanto, tenemos que dicha matriz en la ecuación (5.9) se puede factorizar como: \\[\\begin{equation} \\Pi_{(k \\times k)} = \\Gamma_{(k \\times r)} \\boldsymbol{\\beta}_{(r \\times k)}&#39; (\\#eq:Pi_Matrix) \\end{equation}\\] Donde \\(\\boldsymbol{\\beta}_{(r \\times k)}&#39; \\mathbf{Y}_{t-1}\\) son \\(r\\) combinaciones linealmente independientes que son estacionarias. Dada la ecuación (5.9) podemos establecer la aproximación de Johansen (1988) que se realiza mediante una estimación por Máxima Verosimilitud de la ecuación: \\[\\begin{equation} \\Delta \\mathbf{Y}_t + \\Gamma \\boldsymbol{\\beta}&#39; \\mathbf{Y}_{t-1} = \\sum_{j=1}^{p-1} \\mathbf{A}^*_j \\Delta \\mathbf{Y}_{t-j} + \\mathbf{D}_t + \\mathbf{U}_t \\end{equation}\\] Donde una vez estimado el sistema: \\[\\begin{equation} \\boldsymbol{\\beta} = [v_1, v_2, \\ldots, v_r] \\end{equation}\\] Cada \\(v_i\\), \\(i = 1, 2, \\ldots, r\\) es un vector propio que está asociado con los \\(r\\) valores propios positivos, mismos que están asociados con la prueba de hipótesis de cointegración. Dicha hipótesis está basada en dos estadísticas con las que se determina el rango \\(r\\) de \\(\\Pi\\): Prueba de Traza: \\(H_0 :\\) Existen al menos \\(r\\) valores propios positivos o Existen al menos \\(r\\) relaciones de largo plazo estacionarias. Prueba del valor propio máximo o \\(\\lambda_{max}\\): \\(H_0 :\\) Existen \\(r\\) valores propios positivos o Existen \\(r\\) relaciones de largo plazo estacionarias. . Para ejemplificar el procedimiento de cointegración utilizaremos las series de INPC, Tipo de Cambio, rendimiento de los Cetes a 28 días, IGAE e Índice de Producción Industrial de Estados Unidos. Quizá el marco teórico de la relación entre las variables no sea del todo correcto, pero dejando de lado ese problema, estimaremos si las 5 series cointegran. Por principio, probaremos que todas las series son I(1), lo cual es cierto (ver Scrip para mayores detalles). En las Figuras 5.13, 5.14 y 5.15 se muestran las series en niveles y en diferencias, con lo cual ilustramos como es viable que las series sean I(1). library(ggplot2) library(dplyr) library(stats) library(MASS) library(strucchange) library(zoo) library(sandwich) library(urca) library(lmtest) library(vars) # load(&quot;BD/Datos_Ad.RData&quot;) # ## Conversion a series de tiempo: Datos &lt;- ts(Datos_Ad[7: 11], start = c(2000, 1), freq = 12) LDatos &lt;- log(Datos) DLDatos &lt;- diff(log(Datos, base = exp(1)), lag = 1, differences = 1) plot(LDatos, plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), #main = &quot;Series en Logaritmos&quot;, xlab = &quot;Tiempo&quot;) Figure 5.13: Series en niveles (logaritmos) para la prueba de Cointegración plot(DLDatos, plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), #main = &quot;Series en Diferencias Logaritmicas&quot;, xlab = &quot;Tiempo&quot;) Figure 5.14: Series en Diferencias Logarítmicas para la prueba de Cointegración plot(cbind(LDatos, DLDatos), plot.type = &quot;m&quot;, nc = 2, col = c(&quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;orange&quot;, &quot;purple&quot;), #main = &quot;Comparacion de Series en Diferencias&quot;, xlab = &quot;Tiempo&quot;) Figure 5.15: Comparacion de Series en Diferencias para la prueba de Cointegración Posteriormente, determinamos cuál es el orden adecuado de un VAR(p) en niveles. En el Cuadro 5.8 mostramos los resultados de los criterios de información para determinar el número de rezagos óptimos, el cual resultó en \\(p = 3\\) para los criterios AIC y FPE, \\(p = 2\\) para el criterio HQ y \\(p = 1\\) para el criterio SC. Por lo tanto, decidiremos utilizar un VAR(3) con tendencia y constante. Note que es posible elegir otros modelos de VAR que incluyan: solo tendencia, solo constante o ninguno de estos elementos. Table 5.8: Criterios de información para diferentes especificaciones de modelos VAR(p) con término constante y tendencia de las series \\(LINPC_t\\), \\(LTC_t\\), \\(LCETE28_t\\), \\(LIGAE_t\\) y \\(LIPI_t\\). Rezagos AIC HQ SC FPE 1 -4.606707e+01 -4.585260e+01 -4.553568e+01 9.848467e-21 2 -4.643287e+01 -4.606521e+01 -4.552191e+01 6.834064e-21 3 -4.647783e+01 -4.595697e+01 -4.518730e+01 6.539757e-21 4 -4.645834e+01 -4.578428e+01 -4.478824e+01 6.679778e-21 ## VAR(p) Seleccion: VARselect(LDatos, lag.max = 10, type = &quot;both&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 3 2 2 3 ## ## $criteria ## 1 2 ## AIC(n) -43.1308318646810135987834656 -43.5668661854371990216350241 ## HQ(n) -42.9445592539269327403417265 -43.2475417098587726627556549 ## SC(n) -42.6668499811502783813921269 -42.7714686708130784609238617 ## FPE(n) 0.0000000000000000001855848 0.0000000000000000001200259 ## 3 4 ## AIC(n) -43.5797269785975984746073664 -43.5035550099222803055454278 ## HQ(n) -43.1273506381948337207177246 -42.9181268046951771566455136 ## SC(n) -42.4529138328800996760037378 -42.0453262331114032690493332 ## FPE(n) 0.0000000000000000001185548 0.0000000000000000001280605 ## 5 6 ## AIC(n) -43.4665644031667497415583057 -43.3662074624718840709647338 ## HQ(n) -42.7480843331153010922207613 -42.5146755275960970266169170 ## SC(n) -41.6769199952624873617423873 -41.2451474234742434532563493 ## FPE(n) 0.0000000000000000001330869 0.0000000000000000001474594 ## 7 8 ## AIC(n) -43.3576732552204475723556243 -43.4475783817780367712657608 ## HQ(n) -42.3730894555203079221428197 -42.3299427172535587260426837 ## SC(n) -40.9051975851294216113274160 -40.6636870805936325723450864 ## FPE(n) 0.0000000000000000001491718 0.0000000000000000001368875 ## 9 10 ## AIC(n) -43.3742641642367701138027769 -43.3660356138277762738653109 ## HQ(n) -42.1235766348879465681420697 -41.9822962196546143331943313 ## SC(n) -40.2589572319589734661349212 -39.9193130504566013883049891 ## FPE(n) 0.0000000000000000001480478 0.0000000000000000001502148 VARselect(LDatos, lag.max = 10, type = &quot;trend&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 3 2 2 3 ## ## $criteria ## 1 2 ## AIC(n) -43.0218334558386885646541486 -43.4733608627676417768270767 ## HQ(n) -42.8621712180494753852144640 -43.1806467601540902023771196 ## SC(n) -42.6241346985266318370122463 -42.7442464743621997058653506 ## FPE(n) 0.0000000000000000002069525 0.0000000000000000001317817 ## 3 4 ## AIC(n) -43.4926570116813877575623337 -43.4403901004477006608794909 ## HQ(n) -43.0668910442434906826747465 -42.8815722681854651909816312 ## SC(n) -42.4321269921825603432807839 -42.0484444498554950087054749 ## FPE(n) 0.0000000000000000001293226 0.0000000000000000001363788 ## 5 6 ## AIC(n) -43.4055918510038054591859691 -43.3210122680509144288407697 ## HQ(n) -42.7137221539172173834231216 -42.4960907061399879580676497 ## SC(n) -41.6822305693182144636921294 -41.2662353552719451954544638 ## FPE(n) 0.0000000000000000001414042 0.0000000000000000001541996 ## 7 8 ## AIC(n) -43.3018930975745490741246613 -43.3631837990786621617189667 ## HQ(n) -42.3439196708392771029139112 -42.2721585075190517954979441 ## SC(n) -40.9157005537021944974185317 -40.6455756241129293471203709 ## FPE(n) 0.0000000000000000001576227 0.0000000000000000001488111 ## 9 10 ## AIC(n) -43.3123078988938914335449226 -43.289311818269602838427090 ## HQ(n) -42.0882307425099355668862700 -41.932182797061308576758165 ## SC(n) -40.2632840928347732756265032 -39.908872381117106442616205 ## FPE(n) 0.0000000000000000001573361 0.000000000000000000161972 VARselect(LDatos, lag.max = 10, type = &quot;const&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 3 2 2 3 ## ## $criteria ## 1 2 ## AIC(n) -43.0529318587713945021278050 -43.495191294123372927060700 ## HQ(n) -42.8932696209821813226881204 -43.202477191509821352610743 ## SC(n) -42.6552331014593377744859026 -42.766076905717930856098974 ## FPE(n) 0.0000000000000000002006156 0.000000000000000000128936 ## 3 4 ## AIC(n) -43.5086218228659333817631705 -43.4510965812944505159975961 ## HQ(n) -43.0828558554280363068755833 -42.8922787490322150460997364 ## SC(n) -42.4480918033671059674816206 -42.0591509307022448638235801 ## FPE(n) 0.0000000000000000001272744 0.0000000000000000001349265 ## 5 6 ## AIC(n) -43.4144752420798099024068506 -43.325553474358613925687678 ## HQ(n) -42.7226055449932289320713608 -42.500631912447687454914558 ## SC(n) -41.6911139603942189069130109 -41.270776561579644692301372 ## FPE(n) 0.0000000000000000001401536 0.000000000000000000153501 ## 7 8 ## AIC(n) -43.3069657328556019137977273 -43.3724989978216228792007314 ## HQ(n) -42.3489923061203299425869773 -42.2814737062620125129797088 ## SC(n) -40.9207731889832473370915977 -40.6548908228558900646021357 ## FPE(n) 0.0000000000000000001568252 0.0000000000000000001474313 ## 9 10 ## AIC(n) -43.316467649333738165751129 -43.2911053192979622394886974 ## HQ(n) -42.092390492949782299092476 -41.9339762980896679778197722 ## SC(n) -40.267443843274620007832709 -39.9106658821454658436778118 ## FPE(n) 0.000000000000000000156683 0.0000000000000000001616817 VARselect(LDatos, lag.max = 10, type = &quot;none&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 3 2 2 3 ## ## $criteria ## 1 2 ## AIC(n) -43.009650989538556586921914 -43.4653473630038362784944184 ## HQ(n) -42.876599124714218191911641 -43.1992436333551523830465158 ## SC(n) -42.678235358445178349029447 -42.8025161008170726972821285 ## FPE(n) 0.000000000000000000209486 0.0000000000000000001328347 ## 3 4 ## AIC(n) -43.4961615576300175689539174 -43.4416013221261536614292709 ## HQ(n) -43.0970059631569952784957422 -42.9093938628287787651061080 ## SC(n) -42.5019146643498757498491614 -42.1159387977526264990046911 ## FPE(n) 0.0000000000000000001288547 0.0000000000000000001361851 ## 5 6 ## AIC(n) -43.3978234579566617412638152 -43.31937496312242075191534 ## HQ(n) -42.7325641338349484499303799 -42.52106377417636196014428 ## SC(n) -41.7407453024897563409467693 -41.33088117656213000827847 ## FPE(n) 0.0000000000000000001424605 0.00000000000000000015438 ## 7 8 ## AIC(n) -43.2856068136311264993310033 -43.342694393587237300380366 ## HQ(n) -42.3542437598607293125496653 -42.278279474992494613161398 ## SC(n) -40.9656973959774575178016676 -40.691369344840175870103849 ## FPE(n) 0.0000000000000000001601088 0.000000000000000000151765 ## 9 10 ## AIC(n) -43.2981268022445746623816376 -43.2689202279021429831118439 ## HQ(n) -42.1006600188254935801523970 -41.9384015796587164004449733 ## SC(n) -40.3153861224041349942126544 -39.9547639169683179716230370 ## FPE(n) 0.0000000000000000001594144 0.0000000000000000001650919 El mismo número de rezagos los utilizaremos para probar la Cointegración, ya sea por una estadística de la Traza o por una del máximo valor propio. Dado que los resultados se sostienen, sólo mostraremos uno de los casos en que las series cointegran y únicamente para el caso de la prueba de la traza (el otro caso está disponible en el código de R disponible abajo). En el Cuadro 5.9 reportamos los resultados del Test de Cointegración para un modelo con 3 rezagos. Cuadro: (#tab:TrazaTest) Prueba de la traza para cointegración considerando un VAR(p) con término constante y tendencia de las series \\(LINPC_t\\), \\(LTC_t\\), \\(LCETE28_t\\), \\(LIGAE_t\\) y \\(LIPI_t\\). r \\(\\leq\\) Estadística 10% 5% 1% 4 4.79 10.49 12.25 16.26 3 13.97 22.76 25.32 30.45 2 27.45 39.06 42.44 48.45 1 48.14 59.14 62.99 70.05 0 118.98 83.20 87.31 96.58 Los resultados del Cuadro 5.9 indican que aceptamos la hipótesis nula para el caso de \\(r \\leq 1\\) al \\(5\\%\\), por lo que podemos concluir que existe evidencia estadística para probar que existe al menos 1 vector de cointegración. Por lo que dicho vector normalizado a la primera entrada es: \\[\\begin{equation} \\boldsymbol{\\beta} = \\left[ \\begin{matrix} 1.00000000 \\\\ 0.2100057 \\\\ 0.4812626 \\\\ -2.8386112 \\\\ -1.2576912 \\\\ 14.2887887 \\\\ \\end{matrix} \\right] \\end{equation}\\] Donde el vector esta normalizado para la serie \\(LINPC_t\\), por lo que concluímos que la relación de largo plazo que encontramos cointegra estará dada por: \\[\\begin{eqnarray*} LINPC_t &amp; = &amp; -0.2100057 LTC_t - 0.4812626 LCETE28_t \\\\ &amp; &amp; + 2.8386112 LIGAE_t + 1.2576912 LIPI_t \\\\ &amp; &amp; - 14.2887887 \\end{eqnarray*}\\] ## VAR Estimacion: VAR_1 &lt;- VAR(LDatos, p = 3, type = &quot;both&quot;) #summary(VAR_1) #plot(VAR_1, names = &quot;INPC_Ad&quot;) #plot(VAR_1, names = &quot;TC_Ad&quot;) #plot(VAR_1, names = &quot;CETE28_Ad&quot;) #plot(VAR_1, names = &quot;IGAE_Ad&quot;) #plot(VAR_1, names = &quot;IPI_Ad&quot;) # Cointegration Test: #ca.jo = function (x, type = c(&quot;eigen&quot;, &quot;trace&quot;), ecdet = c(&quot;none&quot;, &quot;const&quot;, #&quot;trend&quot;), K = 2, spec = c(&quot;longrun&quot;, &quot;transitory&quot;), season = NULL, #dumvar = NULL) #summary(ca.jo(LDatos, type = &quot;trace&quot;, ecdet = &quot;trend&quot;, K = 3, spec = &quot;longrun&quot;)) #summary(ca.jo(LDatos, type = &quot;trace&quot;, ecdet = &quot;const&quot;, K = 3, spec = &quot;longrun&quot;)) #summary(ca.jo(LDatos, type = &quot;trace&quot;, ecdet = &quot;none&quot;, K = 3, spec = &quot;longrun&quot;)) CA_1 &lt;- ca.jo(LDatos, type = &quot;trace&quot;, ecdet = &quot;const&quot;, K = 3, spec = &quot;longrun&quot;) summary(CA_1) ## ## ###################### ## # Johansen-Procedure # ## ###################### ## ## Test type: trace statistic , without linear trend and constant in cointegration ## ## Eigenvalues (lambda): ## [1] 0.2242278157790942305638 0.0714829742929301154009 0.0471697381127354625763 ## [4] 0.0323558252755368550013 0.0170373866386099859227 0.0000000000000001925707 ## ## Values of teststatistic and critical values of test: ## ## test 10pct 5pct 1pct ## r &lt;= 4 | 4.79 7.52 9.24 12.97 ## r &lt;= 3 | 13.97 17.85 19.96 24.60 ## r &lt;= 2 | 27.45 32.00 34.91 41.07 ## r &lt;= 1 | 48.14 49.65 53.12 60.16 ## r = 0 | 118.98 71.86 76.07 84.45 ## ## Eigenvectors, normalised to first column: ## (These are the cointegration relations) ## ## INPC_Ad.l3 TC_Ad.l3 CETE28_Ad.l3 IGAE_Ad.l3 IPI_Ad.l3 ## INPC_Ad.l3 1.0000000 1.0000000 1.0000000 1.000000 1.00000000 ## TC_Ad.l3 0.2100057 0.3958412 0.6038714 -3.946863 -0.66160415 ## CETE28_Ad.l3 0.4812626 -0.3505729 0.2932620 1.025381 0.02134859 ## IGAE_Ad.l3 -2.8386112 -4.9448228 -4.1431506 10.685831 -1.62038123 ## IPI_Ad.l3 -1.2576912 0.8682351 2.2789480 -19.080015 1.60589077 ## constant 14.2887887 13.2882018 1.6072247 43.673551 -2.75645487 ## constant ## INPC_Ad.l3 1.00000000 ## TC_Ad.l3 -0.84381863 ## CETE28_Ad.l3 0.06866727 ## IGAE_Ad.l3 1.32792512 ## IPI_Ad.l3 -3.05544132 ## constant 5.81151022 ## ## Weights W: ## (This is the loading matrix) ## ## INPC_Ad.l3 TC_Ad.l3 CETE28_Ad.l3 IGAE_Ad.l3 ## INPC_Ad.d 0.001551917 0.00004896886 0.0002254138 -0.0001685412 ## TC_Ad.d 0.002200808 -0.00575637923 -0.0293306746 -0.0003866727 ## CETE28_Ad.d -0.009399815 -0.02250446234 -0.0166361225 -0.0160384399 ## IGAE_Ad.d 0.001105776 0.01441007456 0.0023574644 -0.0015712694 ## IPI_Ad.d 0.001442047 0.01286401841 -0.0014986278 -0.0011379265 ## IPI_Ad.l3 constant ## INPC_Ad.d -0.0002093347 0.000000000000001290255 ## TC_Ad.d 0.0139760105 -0.000000000000017253317 ## CETE28_Ad.d -0.0111636433 -0.000000000000295711342 ## IGAE_Ad.d 0.0077903982 0.000000000000030602278 ## IPI_Ad.d -0.0046051140 0.000000000000027991698 Considerando lo anterior, podemos determinar \\(\\hat{U}_t\\) para esta ecuación de cointegración. En la Figura 5.16 mostramos los residuales estimados. Derivado de la inspección visual, parecería que estos no son estacionarios, condición que debería ser cierta. De esta forma, una prueba deseable es aplicar todas las pruebas de raíces unitarias a esta serie para mostrar que es I(0). En el Scrip llamado Clase 18 en la carpeta de GoogleDrive se muestran algunas pruebas sobre esta serie y se encuentra que es posible que no sea estacionaria. TT &lt;- ts(c(1:282), start = c(2000, 1), freq = 12) U &lt;- LDatos[ , 1] + 0.2100057 *LDatos[ , 2] + 0.4812626*LDatos[ , 3] - 2.8386112*LDatos[ , 4] - 1.2576912*LDatos[ , 5] + 14.2887887 # plot(U, main = &quot;Residuales de la Ecuación de Cointegración&quot;, type = &quot;l&quot;, col = &quot;darkred&quot;) Figure 5.16: Residuales estimados de la ecuación de cointegración 5.7 Modelos ADRL 5.7.1 Teoría Una vez que hemos analizado diversas técnicas de series de tiempo, el problema consiste en seleccionar el modelo correcto. La Figura 5.17 muestra un esquema o diagrama de cómo podríamos proceder para seleccionar el modelo correcto. knitr::include_graphics(&quot;Plots/TimeSeries_Models.png&quot;) Figure 5.17: Method selection for time series data. OLS: Ordinary least squares; VAR: Vector autoregressive; ARDL: Autoregressive distributed lags; ECM: Error correction models, retomado de: Shrestha y Bhatta (2018) En este caso incorporaremos a los modelos autogregressive distributed lag models (ARDL, por sus siglas en inglés). En estos casos el procedimiento de Johansen no podría aplicarse directamennte cuando las variables incluidas son de un orden mixto o cuando simplemente todas no son estacionarias. Un modelo ARDL está basado en procedimientos de MCO. Este tipo de modelos toma suficientes rezagos para capturar el mecanismo generador de datos. También es posible llegar a una especificación del mecanismo corrector de errores a partir de una trasformación lineal del ARDL. Consideremos la siguiente ecuación: \\[\\begin{equation} Y_t = \\alpha + \\delta X_t + \\gamma Z_t + U_t \\tag{5.10} \\end{equation}\\] Dada la ecuación (5.10) podemos establecer su forma de mecanismo corrector de errores en forma ARDL dada por: \\[\\begin{eqnarray*} \\Delta Y_t &amp; = &amp; \\alpha + \\sum_{i = 1}^p \\beta_i \\Delta Y_{t-i} + \\sum_{i = 1}^p \\delta_i \\Delta X_{t-i} + \\sum_{i = 1}^p \\gamma_i \\Delta Z_{t-i} \\\\ &amp; &amp; + \\lambda_1 Y_{t-1} + \\lambda_2 X_{t-1} + \\lambda_3 Z_{t-1} + U_t \\end{eqnarray*}\\] Donde los coeficientes \\(\\beta_i\\), \\(\\delta_i\\), \\(\\gamma_i\\) representan la dinámica de corto plazo y las \\(\\lambda\\)’s la dinámica de largo plazo. La hipótesis nula (\\(H_0\\)) es que las \\(\\lambda_1 + \\lambda_2 + \\lambda_3 = 0\\), es decir, que no existe relación de largo plazo. En la práctica estimamos una especificación con rezafos distribuidos: \\[\\begin{equation} Y_t = \\alpha + \\sum_{i = 1}^p \\beta_i Y_{t-i} + \\sum_{i = 1}^p \\delta_i X_{t-i} + \\sum_{i = 1}^p \\gamma_i Z_{t-i} + U_t \\end{equation}\\] Además de verificar si las series involucradas son estacionarias y decidir el número de reagos \\(p\\) mediante criterios de información. 5.7.2 Ejemplo 5.7.2.1 DESCRIPCIÓN DEL PROBLEMA Supongamos que queremos modelar el logaritmo de dinero (M2) como una función de LRY (logarithm of real income), IBO (bond rate) e IDE (bank deposit rate). El problema es que la aplicación de una regresión de MCO en datos no estacionarios daría lugar a una regresión espúria. Los parámetros estimados serían consistentes solo si las series estuvieran cointegradas. 5.7.2.2 Importamos Datos desde un dataset de R: Utilizaremos un dataset integrado en la biblioteca ARDL de R. Se trata de un dataframe con 55 renglones y 5 variables en el período de 1974:Q1 a 1987:Q3 de las siguientes variables: LRM: logarithm of real money, M2 LRY: logarithm of real income LPY: logarithm of price deflator IBO: bond rate IDE: bank deposit rate library(zoo) library(xts) library(ARDL) # data(denmark) names(denmark) ## [1] &quot;LRM&quot; &quot;LRY&quot; &quot;LPY&quot; &quot;IBO&quot; &quot;IDE&quot; 5.7.2.3 Procedimiento: Calculamos un auto ADRL para determinar la combinación óptima de rezagos. models &lt;- auto_ardl(LRM ~ LRY + IBO + IDE, data = denmark, max_order = 5) names(models) ## [1] &quot;best_model&quot; &quot;best_order&quot; &quot;top_orders&quot; # models$top_orders ## LRM LRY IBO IDE AIC ## 1 3 1 3 2 -251.0259 ## 2 3 1 3 3 -250.1144 ## 3 2 2 0 0 -249.6266 ## 4 3 2 3 2 -249.1087 ## 5 3 2 3 3 -248.1858 ## 6 2 2 0 1 -247.7786 ## 7 2 1 0 0 -247.5643 ## 8 2 2 1 1 -246.6885 ## 9 3 3 3 3 -246.3061 ## 10 2 2 1 2 -246.2709 ## 11 2 1 1 1 -245.8736 ## 12 2 2 2 2 -245.7722 ## 13 1 1 0 0 -245.6620 ## 14 2 1 2 2 -245.1712 ## 15 3 1 2 2 -245.0996 ## 16 1 0 0 0 -244.4317 ## 17 1 1 0 1 -243.7702 ## 18 5 5 5 5 -243.3120 ## 19 4 1 3 2 -243.0728 ## 20 4 1 3 3 -242.4378 # models$best_order ## LRM LRY IBO IDE ## 3 1 3 2 # models$best_model ## ## Time series regression with &quot;zooreg&quot; data: ## Start = 1974 Q4, End = 1987 Q3 ## ## Call: ## dynlm::dynlm(formula = full_formula, data = data, start = start, ## end = end) ## ## Coefficients: ## (Intercept) L(LRM, 1) L(LRM, 2) L(LRM, 3) LRY L(LRY, 1) ## 2.6202 0.3192 0.5326 -0.2687 0.6728 -0.2574 ## IBO L(IBO, 1) L(IBO, 2) L(IBO, 3) IDE L(IDE, 1) ## -1.0785 -0.1062 0.2877 -0.9947 0.1255 -0.3280 ## L(IDE, 2) ## 1.4079 # BestMod &lt;- models$best_model summary(BestMod) ## ## Time series regression with &quot;zooreg&quot; data: ## Start = 1974 Q4, End = 1987 Q3 ## ## Call: ## dynlm::dynlm(formula = full_formula, data = data, start = start, ## end = end) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.029939 -0.008856 -0.002562 0.008190 0.072577 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.6202 0.5678 4.615 0.00004187 *** ## L(LRM, 1) 0.3192 0.1367 2.336 0.024735 * ## L(LRM, 2) 0.5326 0.1324 4.024 0.000255 *** ## L(LRM, 3) -0.2687 0.1021 -2.631 0.012143 * ## LRY 0.6728 0.1312 5.129 0.00000832 *** ## L(LRY, 1) -0.2574 0.1472 -1.749 0.088146 . ## IBO -1.0785 0.3217 -3.353 0.001790 ** ## L(IBO, 1) -0.1062 0.5858 -0.181 0.857081 ## L(IBO, 2) 0.2877 0.5691 0.505 0.616067 ## L(IBO, 3) -0.9947 0.3925 -2.534 0.015401 * ## IDE 0.1255 0.5545 0.226 0.822161 ## L(IDE, 1) -0.3280 0.7213 -0.455 0.651847 ## L(IDE, 2) 1.4079 0.5520 2.550 0.014803 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0191 on 39 degrees of freedom ## Multiple R-squared: 0.988, Adjusted R-squared: 0.9843 ## F-statistic: 266.8 on 12 and 39 DF, p-value: &lt; 0.00000000000000022 UECM (Unrestricted Error Correction Model) of the underlying ARDL. UECM_BestMod &lt;- uecm(BestMod) summary(UECM_BestMod) ## ## Time series regression with &quot;zooreg&quot; data: ## Start = 1974 Q4, End = 1987 Q3 ## ## Call: ## dynlm::dynlm(formula = full_formula, data = data, start = start, ## end = end) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.029939 -0.008856 -0.002562 0.008190 0.072577 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.62019 0.56777 4.615 0.00004187 *** ## L(LRM, 1) -0.41685 0.09166 -4.548 0.00005154 *** ## L(LRY, 1) 0.41538 0.11761 3.532 0.00108 ** ## L(IBO, 1) -1.89172 0.39111 -4.837 0.00002093 *** ## L(IDE, 1) 1.20534 0.44690 2.697 0.01028 * ## d(L(LRM, 1)) -0.26394 0.10192 -2.590 0.01343 * ## d(L(LRM, 2)) 0.26867 0.10213 2.631 0.01214 * ## d(LRY) 0.67280 0.13116 5.129 0.00000832 *** ## d(IBO) -1.07852 0.32170 -3.353 0.00179 ** ## d(L(IBO, 1)) 0.70701 0.46874 1.508 0.13953 ## d(L(IBO, 2)) 0.99468 0.39251 2.534 0.01540 * ## d(IDE) 0.12546 0.55445 0.226 0.82216 ## d(L(IDE, 1)) -1.40786 0.55204 -2.550 0.01480 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0191 on 39 degrees of freedom ## Multiple R-squared: 0.7458, Adjusted R-squared: 0.6676 ## F-statistic: 9.537 on 12 and 39 DF, p-value: 0.00000003001 RECM (Restricted Error Correction Model) of the underlying ARDL Obs: allowing the constant to join the short-run relationship (case 2), instead of the long-run (case 3) RECM_BestMod &lt;- recm(UECM_BestMod, case = 2) summary(RECM_BestMod) ## ## Time series regression with &quot;zooreg&quot; data: ## Start = 1974 Q4, End = 1987 Q3 ## ## Call: ## dynlm::dynlm(formula = full_formula, data = data, start = start, ## end = end) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.029939 -0.008856 -0.002562 0.008190 0.072577 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## d(L(LRM, 1)) -0.26394 0.09008 -2.930 0.005405 ** ## d(L(LRM, 2)) 0.26867 0.09127 2.944 0.005214 ** ## d(LRY) 0.67280 0.11591 5.805 0.000000703 *** ## d(IBO) -1.07852 0.30025 -3.592 0.000837 *** ## d(L(IBO, 1)) 0.70701 0.44359 1.594 0.118300 ## d(L(IBO, 2)) 0.99468 0.36491 2.726 0.009242 ** ## d(IDE) 0.12546 0.48290 0.260 0.796248 ## d(L(IDE, 1)) -1.40786 0.48867 -2.881 0.006160 ** ## ect -0.41685 0.07849 -5.311 0.000003633 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01819 on 43 degrees of freedom ## (0 observations deleted due to missingness) ## Multiple R-squared: 0.7613, Adjusted R-squared: 0.7113 ## F-statistic: 15.24 on 9 and 43 DF, p-value: 0.00000000009545 long-run levels relationship (cointegration) bounds_f_test(BestMod, case = 2) ## ## Bounds F-test (Wald) for no cointegration ## ## data: d(LRM) ~ L(LRM, 1) + L(LRY, 1) + L(IBO, 1) + L(IDE, 1) + d(L(LRM, 1)) + d(L(LRM, 2)) + d(LRY) + d(IBO) + d(L(IBO, 1)) + d(L(IBO, 2)) + d(IDE) + d(L(IDE, 1)) ## F = 5.1168, p-value = 0.004418 ## alternative hypothesis: Possible cointegration ## null values: ## k T ## 3 1000 Long-run multipliers (with standard errors, t-statistics and p-values) multipliers(BestMod) ## Term Estimate Std. Error t value Pr(&gt;|t|) ## 1 (Intercept) 6.2856579 0.7719160 8.142930 0.0000000006107445 ## 2 LRY 0.9964676 0.1239310 8.040503 0.0000000008358472 ## 3 IBO -4.5381160 0.5202961 -8.722180 0.0000000001058619 ## 4 IDE 2.8915201 0.9950853 2.905801 0.0060092393605960 # Result &lt;- coint_eq(BestMod, case = 2) 5.7.2.4 Make the plot Datos &lt;- cbind.zoo(LRM = denmark[,&quot;LRM&quot;], Result) Datos &lt;- xts(Datos) plot(Datos, legend.loc = &quot;right&quot;) Figure 5.18: Gráfica de la ecuación de cointegración "],["modelos-univariados-y-multivariados-de-volatilidad.html", "Chapter 6 Modelos Univariados y Multivariados de Volatilidad 6.1 Motivación 6.2 Prueba de normalidad 6.3 Modelos ARCH y GARCH Univariados 6.4 Ejemplo GARCH(0,1) 6.5 Modelos ARCH y GARCH Multivariados 6.6 Pruebas para detectar efectos ARCH", " Chapter 6 Modelos Univariados y Multivariados de Volatilidad 6.1 Motivación En este capítulo discutiremos los modelos de Heterocedasticidad Condicional Autoregresiva (ARCH, por sus siglas en inglés) y modelos de Heterocedasticidad Condicional Autoregresiva Generalizados (GARCH, por sus siglas en inglés) tienen la característica de modelar situaciones como las que ilustra la Figura 6.2. Es decir: Existen zonas donde la variación de los datos es mayor y zonas donde la variación es más estable–a estas situaciones se les conoce como de variabilidad por clúster–, y los datos corresponden a innformación de alta frecuencia. Iniciemos por las bibliotecas necesarias: library(expm) library(Matrix) library(ggplot2) library(quantmod) library(moments) library(dynlm) library(broom) library(FinTS) library(lubridate) library(forecast) library(readxl) library(MASS) library(rugarch) library(tsbox) library(MTS) library(rmgarch) library(Rcpp) Para el análisis de temas financieros existe una librería de mucha utilidad llamada quantmod. En primer lugar esta librería permite acceder a datos financieros de un modo muy simple, es posible decargar series financieras desde yahoo, la FRED (Federal Reserve Economic Data), google, etc. Por otro lado, también es una librería que permite realizar gráficos altamente estéticos con unas cuantas líneas de código. Ahora, usaremos datos de Yahoo Finance respecto de la cotización del bitcoin (ticker: “BTC-USD”). options(&quot;getSymbols.warning4.0&quot;=FALSE) BTC &lt;-getSymbols(&quot;BTC-USD&quot;, src = &quot;yahoo&quot;, auto.assign = FALSE) BTC &lt;- na.omit(BTC) chartSeries(BTC,TA=&#39;addBBands(); addBBands(draw=&quot;p&quot;); addVo(); addMACD()&#39;,# subset=&#39;2021&#39;, theme=&quot;white&quot;) head(BTC) ## BTC-USD.Open BTC-USD.High BTC-USD.Low BTC-USD.Close BTC-USD.Volume ## 2014-09-17 465.864 468.174 452.422 457.334 21056800 ## 2014-09-18 456.860 456.860 413.104 424.440 34483200 ## 2014-09-19 424.103 427.835 384.532 394.796 37919700 ## 2014-09-20 394.673 423.296 389.883 408.904 36863600 ## 2014-09-21 408.085 412.426 393.181 398.821 26580100 ## 2014-09-22 399.100 406.916 397.130 402.152 24127600 ## BTC-USD.Adjusted ## 2014-09-17 457.334 ## 2014-09-18 424.440 ## 2014-09-19 394.796 ## 2014-09-20 408.904 ## 2014-09-21 398.821 ## 2014-09-22 402.152 tail(BTC) ## BTC-USD.Open BTC-USD.High BTC-USD.Low BTC-USD.Close BTC-USD.Volume ## 2025-01-22 106136.4 106294.3 103360.3 103653.1 53878181052 ## 2025-01-23 103657.7 106820.3 101257.8 103960.2 104104515428 ## 2025-01-24 103965.7 107098.5 102772.1 104819.5 52388229265 ## 2025-01-25 104824.0 105243.8 104120.4 104714.6 23888996502 ## 2025-01-26 104713.2 105438.6 102507.7 102682.5 22543395879 ## 2025-01-28 102097.5 103466.5 100265.7 101335.8 47335792640 ## BTC-USD.Adjusted ## 2025-01-22 103653.1 ## 2025-01-23 103960.2 ## 2025-01-24 104819.5 ## 2025-01-25 104714.6 ## 2025-01-26 102682.5 ## 2025-01-28 101335.8 Para fines del ejercicio de esta clase, usaremos el valor de la acción ajustado. Esto nos servirá para calcular el rendimiento diario, o puesto en lenguaje de series temporales podemos decir que usaremos la serie en diferencias logarítmicas. plot(BTC$`BTC-USD.Adjusted`) Figure 6.1: Evolución del precio del Bitcoin Una de las preguntas relevantes al observar la serie en diferencias, es si podríamos afirmar que esta serie cumple con el supuesto de homoscedasticidad. Para ello, la Figura 6.2 muestra que las variaciones en el precio del Bitcoin muestran un escenario en el que no se cumple el supuesto de la homocedasticidad. logret &lt;- ts(diff(log(BTC$`BTC-USD.Adjusted`))[-1]) plot(logret) Figure 6.2: Evolución del rendimiento (diferenccias logarítmicas) del Bitcoin 6.1.1 Value at Risk (VaR) Utilicemos como ejemplo el Valor en Riesgo. Este es básicamente es un cálculo que nos permite estimar el monto que una acción o portafolio podría perder dada una probabilidad \\((1-\\alpha)\\). Supongamos un \\(\\alpha = 0.05\\), de esta forma, la Figura 6.3 ilustra la región de la distribución que consideraríamos como el VaR. alpha &lt;- 0.05 VaR &lt;- quantile( logret, alpha ) round( VaR, 4 )*100 ## 5% ## -5.75 qplot(logret , geom = &#39;histogram&#39;) + geom_histogram(fill = &#39;lightblue&#39; , bins = 30) + geom_histogram( aes(logret[logret &lt; quantile(logret , 0.05)]) , fill = &#39;red&#39; , bins = 30) + labs(x = &#39;Daily Returns&#39;) Figure 6.3: Histograma de rendimientos del Bitcoin Ahora bien, una de las preguntas que nos podemos hacer es si los rendimientos del Bitcoin se aproximan a una distribución normal. Para ello, la Figura 6.4 ilustra esta comparación, de la cual podemos observar que una prueba de normalidad rechaza esa hipótesis–el estadístico Jarque-Bera indica que la serie de rendimientos no tiene una distribución normal–. normal_dist &lt;- rnorm(100000, mean(logret), sd(logret)) VaR_n &lt;- quantile(normal_dist, 0.05) ES_n &lt;- mean(normal_dist[normal_dist&lt;VaR]) ggplot()+ geom_density(aes(logret, geom =&#39;density&#39;, col = &#39;returns&#39;))+ geom_density(aes(normal_dist, col = &#39;normal&#39;)) Figure 6.4: Densidad de rendimientos del Bitcoin Vs. una distribución normal vector_ret &lt;- as.vector(logret) ##Kurtosis round(kurtosis(vector_ret),2) ## [1] 14.4 ##Sesgo round(skewness(vector_ret),2) ## [1] -0.72 6.2 Prueba de normalidad \\(H_o: K=S=0\\) jarque.test(vector_ret) ## ## Jarque-Bera Normality Test ## ## data: vector_ret ## JB = 20836, p-value &lt; 0.00000000000000022 ## alternative hypothesis: greater 6.3 Modelos ARCH y GARCH Univariados Para plantear el modelo, supongamos–por simplicidad–que hemos construido y estimado un modelo AR(1). Es decir, asumamos que el proceso subyacente para la media condicional está dada por: \\[\\begin{equation} X_t = a_0 + a_1 X_{t-1} + U_t \\end{equation}\\] Donde \\(| a_1 |&lt; 1\\) para garantizar la convergencia del proceso en el largo plazo, en el cual: \\[\\begin{eqnarray*} \\mathbb{E}[X_t] &amp; = &amp; \\frac{a_0 }{1 - a_1} = \\mu \\\\ Var[X_t] &amp; = &amp; \\frac{\\sigma^2}{1 - a_1^2} \\end{eqnarray*}\\] Ahora, supongamos que este tipo de modelos pueden ser extendidos y generalizados a un modelo ARMA(p, q), que incluya otras variables exógenas. Denotemos a \\(\\mathbf{Z}_t\\) como el conjunto que incluye los componentes AR, MA y variables exógenas que pueden explicar a \\(X_t\\) de forma que el proceso estará dado por: \\[\\begin{equation} X_t = \\mathbf{Z}_t \\boldsymbol{\\beta} + U_t \\end{equation}\\] Donde \\(U_t\\) es un proceso estacionario que representa el error asociado a un proceso ARMA(p, q) y donde siguen diendo válidos los supuestos: \\[\\begin{eqnarray*} \\mathbb{E}[U_t] &amp; = &amp; 0 \\\\ Var[U_t^2] &amp; = &amp; \\sigma^2 \\end{eqnarray*}\\] No obstante, en este caso podemos suponer que existe autocorrelación en el término de error al cuadrado que puede ser capturada por un proceso similar a uno de medias móviles (MA) dado por: \\[\\begin{equation} U_t^2 = \\gamma_0 + \\gamma_1 U_{t-1}^2 + \\gamma_2 U_{t-2}^2 + \\ldots + \\gamma_q U_{t-q}^2 + \\nu_t \\end{equation}\\] Donde \\(\\nu_t\\) es un ruido blanco y \\(U_{t-i} = X_{t-i} - \\mathbf{Z}_{t-i} \\boldsymbol{\\beta}\\), $i = 1, 2 ,$. Si bien los procesos son estacionarios por los supuestos antes enunciados, la varianza condicional estará dada por: \\[\\begin{eqnarray*} \\sigma^2_{t | t-1} &amp; = &amp; Var[ U_t | \\Omega_{t-1} ] \\\\ &amp; = &amp; \\mathbb{E}[ U^2_t | \\Omega_{t-1} ] \\end{eqnarray*}\\] Donde \\(\\Omega_{t-1} = \\{U_{t-1}, U_{t-2}, \\ldots \\}\\) es el conjunto de toda la información pasada de \\(U_t\\) y observada hasta el momento \\(t-1\\), por lo que: \\[\\begin{equation*} U_t | \\Omega_{t-1} \\sim \\mathbb{D}(0, \\sigma^2_{t | t-1}) \\end{equation*}\\] Así, de forma similar a un proceso MA(q) podemos decir que la varianza condicional tendrá efectos ARCH de orden \\(q\\) (ARCH(q)) cuando: \\[\\begin{equation} \\sigma^2_{t | t-1} = \\gamma_0 + \\gamma_1 U_{t-1}^2 + \\gamma_2 U_{t-2}^2 + \\ldots + \\gamma_q U_{t-q}^2 \\tag{6.1} \\end{equation}\\] Donde \\(\\mathbb{E}[\\nu_t] = 0\\) y \\(\\gamma_0\\) y \\(\\gamma_i \\geq 0\\), para \\(i = 1, 2, \\ldots, q-1\\) y \\(\\gamma_q &gt; 0\\). Estas condiciones son necesarias para garantizar que la varianza sea positiva. En general, la varianza condicional se expresa de la forma \\(\\sigma^2_{t | t-1}\\), no obstante, para facilitar la notación, nos referiremos en cada caso a esta simplemente como \\(\\sigma^2_{t}\\). Podemos generalizar esta situación si asumimos a la varianza condicional como dependiente de los valores de la varianza rezagados, es decir, como si fuera un proceso AR de orden \\(p\\) para la varianza y juntándolo con la ecuación (6.1). Bollerslev (1986) y Taylor (1986) generalizaron el problema de heterocedasticidad condicional. El modelo se conoce como GARCH(p, q), el cual se especifica como: \\[\\begin{eqnarray} \\sigma^2_t &amp; = &amp; \\gamma_0 + \\gamma_1 U_{t-1}^2 + \\gamma_2 U_{t-2}^2 + \\ldots + \\gamma_q U_{t-q}^2 \\\\ \\nonumber &amp; &amp; + \\beta_1 \\sigma^2_{t-1} + \\beta_2 \\sigma^2_{t-2} + \\ldots + \\beta_p \\sigma^2_{t-p} \\tag{6.2} \\end{eqnarray}\\] Donde las condiciones de no negatividad son que \\(\\gamma_0 &gt; 0\\), \\(\\gamma_i \\geq 0\\), \\(i = 1, 2, \\ldots, q-1\\), \\(\\beta_j \\geq 0\\), \\(j = 1, 2, \\ldots, p-1\\), \\(\\gamma_q &gt; 0\\) y \\(\\beta_p &gt; 0\\). Además, otra condición de convergencia es que: \\[\\begin{equation*} \\gamma_1 + \\ldots + \\gamma_q + \\beta_1 + \\ldots + \\beta_p &lt; 1 \\end{equation*}\\] Usando el operador rezago \\(L\\) en la ecuación (6.2) podemos obtener: \\[\\begin{equation} \\sigma^2_t = \\gamma_0 + \\alpha(L) U_t^2 + \\beta(L) \\sigma^2_t \\tag{6.3} \\end{equation}\\] De donde podemos establecer: \\[\\begin{equation} \\sigma^2_t = \\frac{\\gamma_0}{1 - \\beta(L)} + \\frac{\\alpha(L)}{1 - \\beta(L)} U_t^2 \\end{equation}\\] Por lo que la ecuación (6.2) del GARCH(p, q) representa un ARCH(\\(\\infty\\)): \\[\\begin{equation} \\sigma^2_t = \\frac{a_0}{1 - b_1 - b_2 - \\ldots - b_p} + \\sum_{i = 1}^\\infty U_{t-i}^2 \\end{equation}\\] 6.3.1 Ejemplo ARCH(1) Hasta ahora, las distribuciones utilizadas para medir el Valor en Riesgo de un activo (Bitcoin, en este caso) asumen que no existe correlación serial en los retornos diarios del activo. Observemos un par de gráficas de la función de autocorrelación para corroborar este hecho–ver la Figura 6.5–. acf(logret) Figure 6.5: Función de autocorrelación parcial de lo rendimientos del Bitcoin La idea de clusterización de volatilidad asume que períodos de alta volatilidad serán seguidos por alta volatilidad y viceversa. Por esta razón, la función de autocorrelación útil para saber si existen clústers de volatilidad es utilizando el valor absoluto, ya que lo que importa es saber si la serie está autocorrelacionada en la magnitud de los movimientos. La Figura 6.6 muestra el comportamiento de los rendimientos en valor absoluto y la Figura 6.7 la función de autocorrelación parcial. plot(abs(logret)) Figure 6.6: Evolución de los rendimientos del Bitcoin en valor absoluto acf(abs(logret)) Figure 6.7: Función de autocorrelación parcial de los rendimientos del Bitcoin en valor absoluto Otra manera de corroborar esta idea es volviendo IID nuestra serie de datos y observar que de este modo se pierde la autocorrelación serial, lo que refuerza la idea de que en esta serie existen clusters de volatilidad. logret_random &lt;- sample(as.vector(logret), size = length(logret), replace = FALSE) acf(abs(logret_random)) Figure 6.8: Función de autocorrelación parcial de los rendimientos del Bitcoin en valor absoluto par(mfrow = c(1,2)) plot(logret) plot(logret_random, type = &#39;l&#39;) Figure 6.9: Función de autocorrelación parcial de los rendimientos del Bitcoin en valor absoluto Ahora busquemos una prueba formal. El primer enfoque para comprobar aceptar o rechazar la hipótesis de que necesitamos estimar un ARCH(q) es realizar una prueba de efectos ARCH. De acuerdo con esta, nuestros datos de Bitcoin muestran que se rechaza la hipótesis de no efectos ARCH. Esta prueba resulta del siguiente código de R: logret_mean = dynlm(logret~1) summary(logret_mean) ## ## Time series regression with &quot;ts&quot; data: ## Start = 1, End = 3785 ## ## Call: ## dynlm(formula = logret ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46616 -0.01409 -0.00011 0.01542 0.22369 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0014269 0.0005913 2.413 0.0159 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03638 on 3784 degrees of freedom ehatsq = ts(resid(logret_mean)^2) ARCH_m = dynlm(ehatsq~L(ehatsq)) summary(ARCH_m) ## ## Time series regression with &quot;ts&quot; data: ## Start = 2, End = 3785 ## ## Call: ## dynlm(formula = ehatsq ~ L(ehatsq)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.016924 -0.001151 -0.001019 -0.000314 0.216152 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.00115053 0.00008097 14.210 &lt; 0.0000000000000002 *** ## L(ehatsq) 0.12955385 0.01612193 8.036 0.00000000000000123 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.004805 on 3782 degrees of freedom ## Multiple R-squared: 0.01679, Adjusted R-squared: 0.01653 ## F-statistic: 64.58 on 1 and 3782 DF, p-value: 0.000000000000001231 acf(ARCH_m$residuals) acf(abs(ARCH_m$residuals)) ArchTest(logret, lags = 1, demean = TRUE) ## ## ARCH LM-test; Null hypothesis: no ARCH effects ## ## data: logret ## Chi-squared = 63.525, df = 1, p-value = 0.000000000000001584 La prueba puede repetirse para diferentes especificaciones de ARCH, sin embargo, para efectos ilustrativos usaremos un ARCH(1). Estimemos un ARCH(1), considerando la siguente especificación: \\[\\begin{eqnarray*} Y_t &amp; = &amp; \\mu+\\sqrt{h_t}\\varepsilon_t \\\\ h_t &amp; = &amp; \\omega+\\alpha_ih_{t-i}+u_t \\\\ \\varepsilon &amp; \\sim &amp; N(0,1) \\end{eqnarray*}\\] Considerando que la media es una AR(2), los resultados de esta estimación son los siguientes: Table 6.1: Resumen de la estimación ARIMA(2,0,0) con media distinta de cero (y posible componente GARCH) \\(\\sigma^2 = 0.001324, \\ \\text{log-likelihood} = 7170.99,\\ \\text{AIC}=-14333.99,\\ \\text{AICc}=-14333.97,\\ \\text{BIC}=-14309.03\\) Parameter Estimate Std. Error t value Pr(\\(&gt;\\mid t\\mid\\)) mu 0.001642394 0.0003553101 4.622424 3.792818e-06 ar1 -0.051859993 0.0160262284 -3.235945 1.212408e-03 ar2 -0.001225748 0.0132050756 -0.092824 9.260434e-01 omega 0.001956396 0.0004839458 4.042593 5.286335e-05 alpha1 0.730450078 0.2140120509 3.413126 6.422226e-04 shape 2.347586356 0.1135148037 20.680883 0.000000e+00 Notas: Primera línea en negrita como título de la tabla (Markdown no tiene un \\caption{} como LaTeX). Bajo el título, se añade un párrafo en itálicas con los valores adicionales (\\(\\sigma^2\\), log-likelihood, AIC, etc.). La fila de encabezados va seguida por una fila de separadores (|:---|---:| etc.) para indicar la alineación a la izquierda, derecha o centro. Las celdas con datos numéricos se alinean a la derecha (:---: o ---:) para mantener la estética de los números. Si prefieres, puedes omitir la columna de “Notas” y añadir la información directamente en el título de la tabla o en un párrafo posterior. library(rugarch) auto.arima(logret) ## Series: logret ## ARIMA(2,0,0) with non-zero mean ## ## Coefficients: ## ar1 ar2 mean ## -0.0200 0.0110 0.0014 ## s.e. 0.0163 0.0163 0.0006 ## ## sigma^2 = 0.001324: log likelihood = 7173.3 ## AIC=-14338.6 AICc=-14338.59 BIC=-14313.65 #?ugarchspec model.spec = ugarchspec( variance.model = list(model = &#39;sGARCH&#39; , garchOrder = c(1, 0)), mean.model = list(armaOrder = c(2,0)), distribution.model = &quot;std&quot;) #?ugarchfit arch.fit = ugarchfit(spec = model.spec , data = logret, solver = &#39;solnp&#39;) arch.fit@fit$matcoef ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 0.001635494 0.0003554112 4.60169405 0.000004190685 ## ar1 -0.051576946 0.0160262100 -3.21828716 0.001289586382 ## ar2 -0.001200551 0.0132061754 -0.09090829 0.927565460663 ## omega 0.001954042 0.0004855857 4.02409266 0.000057195396 ## alpha1 0.729178819 0.2142614480 3.40321988 0.000665966632 ## shape 2.348190977 0.1142719913 20.54913851 0.000000000000 boot.garch &lt;- ugarchboot(arch.fit, method = &quot;Partial&quot;, sampling = &quot;raw&quot;, #bootstrap from fitted varepsilon n.ahead = 1, #simulation horizon n.bootpred = 100000, #number of simulations solver = &quot;solnp&quot;) boot.garch ## ## *-----------------------------------* ## * GARCH Bootstrap Forecast * ## *-----------------------------------* ## Model : sGARCH ## n.ahead : 1 ## Bootstrap method: partial ## Date (T[0]): 3785-01-01 ## ## Series (summary): ## min q.25 mean q.75 max forecast[analytic] ## t+1 -0.48538 -0.010958 0.002004 0.015936 0.21018 0.002426 ## ..................... ## ## Sigma (summary): ## min q0.25 mean q0.75 max forecast[analytic] ## t+1 0.046252 0.046252 0.046252 0.046252 0.046252 0.046252 ## ..................... 6.4 Ejemplo GARCH(0,1) Estimemos un GARCH(0,1) usando la siguiente especificación: \\[\\begin{eqnarray*} Y_t &amp; = &amp; \\mu+\\sqrt{h_t}\\varepsilon_t \\\\ h_t &amp; = &amp; \\omega+\\beta_i \\sigma^2_{t-i}+u_t \\\\ \\varepsilon &amp; \\sim &amp; N(0,1) \\end{eqnarray*}\\] Considerando que la media es una AR(2), los resultados de esta estimación son los siguientes: Table 6.2: Resumen de estimaciones del modelo GARCH(0,1) Parameter Estimate Std. Error t value Pr(&gt;|t|) mu 1.743510e-03 3.728562e-04 4.6760931 2.923919e-06 ar1 -6.104183e-02 1.366530e-02 -4.4669219 7.935308e-06 ar2 -4.231427e-03 1.311300e-02 -0.3226895 7.469304e-01 omega 3.843436e-06 6.414905e-08 59.9141495 0.000000e+00 beta1 9.981016e-01 3.215075e-05 31044.4303 0.000000e+00 shape 2.515980e+00 2.796609e-02 89.9653552 0.000000e+00 model.spec = ugarchspec(variance.model = list(model = &#39;sGARCH&#39; , garchOrder = c(0,1)), mean.model = list(armaOrder = c(2,0)), distribution.model = &quot;std&quot;) fit.garch.n = ugarchfit(spec = model.spec, data = logret, solver = &quot;solnp&quot;) fit.garch.n@fit$matcoef ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 0.001740704580 0.00037337366261 4.6620979 0.000003130022 ## ar1 -0.060902729267 0.01367359636521 -4.4540388 0.000008426985 ## ar2 -0.003943353751 0.01312346900047 -0.3004811 0.763810248528 ## omega 0.000003777145 0.00000006384041 59.1654188 0.000000000000 ## beta1 0.998120749231 0.00003167756525 31508.7583743 0.000000000000 ## shape 2.522713645953 0.02849012793026 88.5469399 0.000000000000 boot.garch &lt;- ugarchboot(fit.garch.n, method = &quot;Partial&quot;, sampling = &quot;raw&quot;, #bootstrap from fitted varepsilon n.ahead = 1, #simulation horizon n.bootpred = 100000, #number of simulations solver = &quot;solnp&quot;) boot.garch ## ## *-----------------------------------* ## * GARCH Bootstrap Forecast * ## *-----------------------------------* ## Model : sGARCH ## n.ahead : 1 ## Bootstrap method: partial ## Date (T[0]): 3785-01-01 ## ## Series (summary): ## min q.25 mean q.75 max forecast[analytic] ## t+1 -0.46563 -0.012482 0.00199 0.01781 0.24163 0.002735 ## ..................... ## ## Sigma (summary): ## min q0.25 mean q0.75 max forecast[analytic] ## t+1 0.044826 0.044826 0.044826 0.044826 0.044826 0.044826 ## ..................... 6.4.1 Selección GARCH(p,q) óptimo ¿Cómo seleccionamos el órden adecuado para un GARCH(p,q)? Acá una respuesta. \\(Y_t = \\mu+\\sqrt{h_t}\\varepsilon_t\\) \\(h_t = \\omega+\\beta_ih_{t-i}+\\alpha_i\\varepsilon^2_{t-i}+u_t\\) \\(\\varepsilon \\sim N(0,1)\\) 6.4.1.1 Criterios de información Table 6.3: Comparación de criterios de información. Criterio Valor Akaike -4.083927 Bayes -4.074035 Shibata -4.083932 Hannan-Quinn -4.080410 infocriteria(fit.garch.n) ## ## Akaike -4.084151 ## Bayes -4.074261 ## Shibata -4.084156 ## Hannan-Quinn -4.080635 6.4.1.2 Selección del modelo óptimo Podemos hacer una búsqueda del mejor modelo de entre varios que probemos en un espectro de hasta un GARCH(4,4). Los resultados son los reportados en el siguiente Cuadro. Table 6.4: Resumen de criterios de información en función de y de un GARCH(p,q). q p AIC Óptimo 1 1 -10.93647 0 1 2 -10.93657 0 1 3 -10.93799 0 1 4 -10.93727 0 2 1 -10.93579 0 2 2 -10.93607 0 2 3 -10.93743 0 2 4 -10.93510 0 3 1 -10.93524 0 3 2 -10.93510 0 3 3 -10.93765 0 3 4 -10.93645 0 4 1 -10.93840 0 4 2 -10.93935 1 4 3 -10.93924 0 4 4 -10.93816 0 source(&quot;Lag_Opt_GARCH.R&quot;) Lag_Opt_GARCH(ehatsq,4,4) ## q p AIC Optimo ## [1,] 1 1 -10.93733 0 ## [2,] 1 2 -10.93742 0 ## [3,] 1 3 -10.93898 0 ## [4,] 1 4 -10.93828 0 ## [5,] 2 1 -10.93677 0 ## [6,] 2 2 -10.93702 0 ## [7,] 2 3 -10.93659 0 ## [8,] 2 4 -10.93600 0 ## [9,] 3 1 -10.93633 0 ## [10,] 3 2 -10.93608 0 ## [11,] 3 3 -10.93863 0 ## [12,] 3 4 -10.93627 0 ## [13,] 4 1 -10.93936 0 ## [14,] 4 2 -10.94030 1 ## [15,] 4 3 -10.94004 0 ## [16,] 4 4 -10.93977 0 6.4.1.3 Estimación de modelo óptimo De esta forma, el modelo óptimo es un GARCH(2,4). Los resultados de la estimación son los del Cuadro Table 6.5: Resumen de estimaciones del modelo (parámetros, errores estándar, estadísticos t y p-values). Parameter Estimate Std. Error t value Pr(&gt;|t|) mu 1.353612e-03 3.233488e-04 4.186229e+00 2.836277e-05 ar1 -4.916331e-02 1.495855e-02 -3.286636e+00 1.013919e-03 ar2 7.273432e-04 1.385161e-02 5.250964e-02 9.581226e-01 omega 2.759072e-05 5.151381e-06 5.355984e+00 8.509188e-08 alpha1 1.290996e-01 2.165606e-02 5.961363e+00 2.501419e-09 alpha2 1.768468e-07 3.676320e-02 4.810430e-06 9.999962e-01 alpha3 6.424436e-08 2.663630e-02 2.411910e-06 9.999981e-01 alpha4 3.333343e-02 2.238762e-02 1.488923e+00 1.365077e-01 beta1 4.260058e-01 2.263614e-01 1.881972e+00 5.983983e-02 beta2 4.105609e-01 1.819274e-01 2.256730e+00 2.402498e-02 shape 3.183045e+00 1.305307e-01 2.438541e+01 0.000000e+00 model.spec = ugarchspec(variance.model = list(model = &#39;sGARCH&#39;, garchOrder = c(4,2)), mean.model = list(armaOrder = c(2,0)), distribution.model = &quot;std&quot;) model.fit = ugarchfit(spec = model.spec , data = logret, solver = &#39;solnp&#39;) model.fit@fit$matcoef ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 0.00134678095776 0.000323398695 4.164460090105 0.000031209021018 ## ar1 -0.04886769933767 0.014957172053 -3.267175049117 0.001086264909934 ## ar2 0.00076318895730 0.013851955445 0.055096116958 0.956061889666686 ## omega 0.00002756319877 0.000005173398 5.327871678924 0.000000099370286 ## alpha1 0.12910928652310 0.021656642753 5.961648257038 0.000000002497062 ## alpha2 0.00000008097395 0.036808854555 0.000002199850 0.999998244773885 ## alpha3 0.00000003321500 0.026606198439 0.000001248393 0.999999003926203 ## alpha4 0.03332472816769 0.022402523187 1.487543518626 0.136871318270816 ## beta1 0.42610028946799 0.226401555100 1.882055488881 0.059828482006427 ## beta2 0.41046557973950 0.182031179239 2.254919082843 0.024138408284407 ## shape 3.18383228409795 0.130668633253 24.365696685023 0.000000000000000 boot.garch &lt;- ugarchboot(model.fit, method = &quot;Partial&quot;, sampling = &quot;raw&quot;, #bootstrap from fitted varepsilon n.ahead = 1, #simulation horizon n.bootpred = 100000, #number of simulations solver = &quot;solnp&quot;) boot.garch ## ## *-----------------------------------* ## * GARCH Bootstrap Forecast * ## *-----------------------------------* ## Model : sGARCH ## n.ahead : 1 ## Bootstrap method: partial ## Date (T[0]): 3785-01-01 ## ## Series (summary): ## min q.25 mean q.75 max forecast[analytic] ## t+1 -0.30321 -0.008765 0.00199 0.013453 0.23364 0.002042 ## ..................... ## ## Sigma (summary): ## min q0.25 mean q0.75 max forecast[analytic] ## t+1 0.0249 0.0249 0.0249 0.0249 0.0249 0.0249 ## ..................... 6.4.1.4 Forecasting with GARCH(1,1) Para realizar pronósticos con la estimación de un GARCH, utilizando la librería rugarch, es necesario utilizar la función ugarchforecast(). model.spec = ugarchspec(variance.model = list(model = &#39;sGARCH&#39;, garchOrder = c(1,1)), mean.model = list(armaOrder = c(4,2)), distribution.model = &quot;std&quot;) model.fit = ugarchfit(spec = model.spec , data = logret, solver = &#39;solnp&#39;) spec = getspec(model.fit) setfixed(spec) &lt;- as.list(coef(model.fit)) Esta función precisa como argumentos nuestra estimación del modelo GARCH, con una modificación en la manera en que se presentan los coeficientes, realizada en la última línea del código anterior y que llamamos spec. n.ahead es el número de periodos que vamos a pronosticar, n.roll señala el número de pronósticos móviles que utilizaremos, en caso de que haya más información para realizar el pronóstico. Finalmente damos como input nuestro set de datos y como producto obtendremos el pronostico de Sigma tanto como de la serie. forecast = ugarchforecast(spec, n.ahead = 12, n.roll = 0, logret) sigma(forecast) ## 3785-01-01 ## T+1 0.02500908 ## T+2 0.02534711 ## T+3 0.02568036 ## T+4 0.02600902 ## T+5 0.02633325 ## T+6 0.02665321 ## T+7 0.02696907 ## T+8 0.02728096 ## T+9 0.02758902 ## T+10 0.02789337 ## T+11 0.02819414 ## T+12 0.02849144 fitted(forecast) ## 3785-01-01 ## T+1 0.0022710340 ## T+2 0.0006542325 ## T+3 0.0013658170 ## T+4 0.0020368380 ## T+5 0.0011434041 ## T+6 0.0007250875 ## T+7 0.0016861390 ## T+8 0.0018505731 ## T+9 0.0008872402 ## T+10 0.0009671078 ## T+11 0.0018723392 ## T+12 0.0015722452 forecast ## ## *------------------------------------* ## * GARCH Model Forecast * ## *------------------------------------* ## Model: sGARCH ## Horizon: 12 ## Roll Steps: 0 ## Out of Sample: 0 ## ## 0-roll forecast [T0=]: ## Series Sigma ## T+1 0.0022710 0.02501 ## T+2 0.0006542 0.02535 ## T+3 0.0013658 0.02568 ## T+4 0.0020368 0.02601 ## T+5 0.0011434 0.02633 ## T+6 0.0007251 0.02665 ## T+7 0.0016861 0.02697 ## T+8 0.0018506 0.02728 ## T+9 0.0008872 0.02759 ## T+10 0.0009671 0.02789 ## T+11 0.0018723 0.02819 ## T+12 0.0015722 0.02849 6.5 Modelos ARCH y GARCH Multivariados De forma similar a los modelos univariados, los modelos multivariados de heterocedasticidad condicional asumen una estructura de la media condicional. En este caso, descrita por un VAR(p) cuyo proceso estocástico \\(\\mathbf{X}\\) es estacionario de dimensión \\(k\\). De esta forma, la expresión reducida del modelo o el proceso VAR(p) estará dado por: \\[\\begin{equation} \\mathbf{X}_t = \\boldsymbol{\\delta} + \\mathbf{A_1} \\mathbf{X}_{t-1} + \\mathbf{A_2} \\mathbf{X}_{t-2} + \\ldots + \\mathbf{A_p} \\mathbf{X}_{t-p} + \\mathbf{U}_{t} \\end{equation}\\] Donde cada uno de las \\(\\mathbf{A_i}\\), \\(i = 1, 2, \\ldots, p\\), son matrices cuadradas de dimensión \\(k\\) y \\(\\mathbf{U}_t\\) representa un vector de dimensión \\(k \\times 1\\) con los residuales en el momento del tiempo \\(t\\) que son un proceso puramente aleatorio. También se incorpora un vector de términos constantes denominado como \\(\\boldsymbol{\\delta}\\), el cual es de dimensión \\(k \\times 1\\) –en este caso también es posible incorporar procesos determinísticos adicionales–. Así, suponemos que el término de error tendrá estructura de vector: \\[\\begin{equation*} \\mathbf{U}_t = \\begin{bmatrix} U_{1t} \\\\ U_{2t} \\\\ \\vdots \\\\ U_{Kt} \\end{bmatrix} \\end{equation*}\\] De forma que diremos que: \\[\\begin{equation*} \\mathbf{U}_t | \\Omega_{t-1} \\sim (0, \\Sigma_{t | t-1}) \\end{equation*}\\] Dicho lo anterior, entonces, el modelo ARCH(q) multivariado será descrito por: \\[\\begin{equation} Vech(\\Sigma_{t | t-1}) = \\boldsymbol{\\gamma}_0 + \\Gamma_1 Vech(\\mathbf{U}_{t-1} \\mathbf{U}_{t-1}&#39;) + \\ldots + \\Gamma_q Vech(\\mathbf{U}_{t-q} \\mathbf{U}_{t-q}&#39;) (\\#eq:M_ARCH) \\end{equation}\\] Donde \\(Vech\\) es un operador que apila en un vector la parte superior de la matriz a la cual se le aplique, \\(\\boldsymbol{\\gamma}_0\\) es un vector de constantes, \\(\\Gamma_i\\), \\(i = 1, 2, \\ldots\\) son matrices de coeficientes asociados a la estimación. Para ilustrar la ecuación @ref(eq:M_ARCH), tomemos un ejemplo de \\(K = 2\\), de esta forma tenemos que un M-ARCH(1) será: \\[\\begin{equation*} \\Sigma_{t | t-1} = \\begin{bmatrix} \\sigma^2_{1, t | t-1} &amp; \\sigma_{12, t | t-1} \\\\ \\sigma_{21, t | t-1} &amp; \\sigma^2_{2, t | t-1} \\end{bmatrix} = \\begin{bmatrix} \\sigma_{11, t} &amp; \\sigma_{12, t} \\\\ \\sigma_{21, t} &amp; \\sigma_{22, t} \\end{bmatrix} = \\Sigma_{t} \\end{equation*}\\] Donde hemos simplificado la notación de las varianzas y la condición de que están en función de \\(t-1\\). Así, \\[\\begin{equation*} Vech(\\Sigma_{t}) = Vech \\begin{bmatrix} \\sigma_{11, t} &amp; \\sigma_{12, t} \\\\ \\sigma_{21, t} &amp; \\sigma_{22, t} \\end{bmatrix} = \\begin{bmatrix} \\sigma_{11, t} \\\\ \\sigma_{12, t} \\\\ \\sigma_{22, t} \\end{bmatrix} \\end{equation*}\\] De esta forma, podemos establecer el modelo M-ARCH(1) con \\(K = 2\\) será de la forma: \\[\\begin{equation*} \\begin{bmatrix} \\sigma_{11, t} \\\\ \\sigma_{12, t} \\\\ \\sigma_{22, t} \\end{bmatrix} = \\begin{bmatrix} \\gamma_{10} \\\\ \\gamma_{20} \\\\ \\gamma_{30} \\end{bmatrix} + \\begin{bmatrix} \\gamma_{11} &amp; \\gamma_{12} &amp; \\gamma_{13} \\\\ \\gamma_{21} &amp; \\gamma_{22} &amp; \\gamma_{23} \\\\ \\gamma_{31} &amp; \\gamma_{32} &amp; \\gamma_{33} \\end{bmatrix} \\begin{bmatrix} U^2_{1, t-1} \\\\ U_{1, t-1} U_{2, t-1} \\\\ U^2_{2, t-1} \\end{bmatrix} \\end{equation*}\\] Como notarán, este tipo de procedimientos implica la estimación de muchos parámetros. En esta circunstancia, se suelen estimar modelos restringidos para reducir el número de coeficientes estimados. Por ejemplo, podríamos querer estimar un caso como: \\[\\begin{equation*} \\begin{bmatrix} \\sigma_{11, t} \\\\ \\sigma_{12, t} \\\\ \\sigma_{22, t} \\end{bmatrix} = \\begin{bmatrix} \\gamma_{10} \\\\ \\gamma_{20} \\\\ \\gamma_{30} \\end{bmatrix} + \\begin{bmatrix} \\gamma_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; \\gamma_{22} &amp; 0 \\\\ 0 &amp; 0 &amp; \\gamma_{33} \\end{bmatrix} \\begin{bmatrix} U^2_{1, t-1} \\\\ U_{1, t-1} U_{2, t-1} \\\\ U^2_{2, t-1} \\end{bmatrix} \\end{equation*}\\] Finalmente y de forma análoga al caso univariado, podemos plantear un modelo M-GARCH(p, q) como: \\[\\begin{equation} Vech(\\Sigma_{t | t-1}) = \\boldsymbol{\\gamma}_0 + \\sum_{j = 1}^q \\Gamma_j Vech(\\mathbf{U}_{t-j} \\mathbf{U}_{t-j}&#39;) + \\sum_{m = 1}^p \\mathbf{G}_m Vech(\\Sigma_{t-m | t-m-1}) \\label{M_GARCH} \\end{equation}\\] Donde cada una de las \\(\\mathbf{G}_m\\) es una matriz de coeficientes. Para ilustrar este caso, retomemos el ejemplo anterior, pero ahora para un modelo M-GARCH(1, 1) con \\(K = 2\\) de forma que tendríamos: \\[\\begin{eqnarray*} \\begin{bmatrix} \\sigma_{11, t} \\\\ \\sigma_{12, t} \\\\ \\sigma_{22, t} \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} \\gamma_{10} \\\\ \\gamma_{20} \\\\ \\gamma_{30} \\end{bmatrix} + \\begin{bmatrix} \\gamma_{11} &amp; \\gamma_{12} &amp; \\gamma_{13} \\\\ \\gamma_{21} &amp; \\gamma_{22} &amp; \\gamma_{23} \\\\ \\gamma_{31} &amp; \\gamma_{32} &amp; \\gamma_{33} \\end{bmatrix} \\begin{bmatrix} U^2_{1, t-1} \\\\ U_{1, t-1} U_{2, t-1} \\\\ U^2_{2, t-1} \\end{bmatrix} \\\\ &amp; &amp; + \\begin{bmatrix} g_{11} &amp; g_{12} &amp; g_{13} \\\\ g_{21} &amp; g_{22} &amp; g_{23} \\\\ g_{31} &amp; g_{32} &amp; g_{33} \\end{bmatrix} \\begin{bmatrix} \\sigma_{11, t-1} \\\\ \\sigma_{12, t-1} \\\\ \\sigma_{22, t-1} \\end{bmatrix} \\end{eqnarray*}\\] 6.6 Pruebas para detectar efectos ARCH La prueba que mostraremos es conocida como una ARCH-LM, la cual está basada en una regresión de los residuales estimados de un modelo VAR(p) o cualquier otra estimación que deseemos probar, con el objeto de determinar si existen efectos ARCH –esta prueba se puede simplificar para el caso univariado–. Partamos de plantear: \\[\\begin{eqnarray} Vech(\\hat{\\mathbf{U}}_t \\hat{\\mathbf{U}}_t&#39;) &amp; = &amp; \\mathbf{B}_0 + \\mathbf{B}_1 Vech(\\hat{\\mathbf{U}}_{t-1} \\hat{\\mathbf{U}}_{t-1}&#39;) + \\ldots \\\\ \\nonumber &amp; &amp; + \\mathbf{B}_q Vech(\\hat{\\mathbf{U}}_{t-q} \\hat{\\mathbf{U}}_{t-q}&#39;) + \\varepsilon_t \\tag{6.4} \\end{eqnarray}\\] Dada la estimación en la ecuación (6.4), plantearemos la estructura de hipótesis dada por: \\[\\begin{eqnarray*} H_0 &amp; : &amp; \\mathbf{B}_1 = \\mathbf{B}_2 = \\ldots = \\mathbf{B}_q = 0 \\\\ H_a &amp; : &amp; No H_0 \\end{eqnarray*}\\] La estadística de prueba será determinada por: \\[\\begin{equation} LM_{M-ARCH} = \\frac{1}{2} T K (K + 1) - Traza \\left( \\hat{\\Sigma}_{ARCH} \\hat{\\Sigma}^{-1}_{0} \\right) \\sim \\chi^2_{[q K^2 (K + 1)^2 / 4]} \\end{equation}\\] Donde la matriz \\(\\hat{\\Sigma}_{ARCH}\\) se calcula de acuerdo con la ecuación (6.4) y la matriz \\(\\hat{\\Sigma}_{0}\\) sin considerar una estructura dada para los errores. "],["modelos-de-datos-panel-panel-var-y-otros-modelos-no-lineales.html", "Chapter 7 Modelos de Datos Panel, Panel VAR y otros Modelos No Lineales 7.1 Modelos de Datos Panel y Panel VAR 7.2 Ejemplos: Panel VAR(p) 7.3 Cointegración 7.4 Modelos No Lineales 7.5 Ejemplo: Modelos de Cambio de Regímen (TAR)", " Chapter 7 Modelos de Datos Panel, Panel VAR y otros Modelos No Lineales 7.1 Modelos de Datos Panel y Panel VAR 7.1.1 Motivación Los datos panel son una extensión natural del análisis de series de tiempo en el que observamos una misma variable a lo largo del tiempo, pero para múltiples individuos. Denotemos con \\(Y_{it}\\) el componente o individuo \\(i\\), \\(i = 1, 2, \\ldots, N\\) en el tiempo \\(t\\), \\(t = 1, 2, \\ldots, T\\). Típicamente, las series de tiempo en forma de panel se caracterizan por ser de dimensión de individuos corta y de dimensión temporal larga. En general, escribiremos: \\[\\begin{equation} Y_{it} = \\alpha_i + \\beta_i X_{it} + U_{it} \\end{equation}\\] Donde \\(\\alpha_i\\) es un efecto fijo especificado como: \\[\\begin{equation} \\alpha_i = \\beta_0 + \\gamma_i Z_i \\end{equation}\\] 7.1.2 Pruebas de Raíces Unitarias en Panel Las pruebas de raíces unitarias para panel suelen ser usadas principalmente en casos macroeconómicos. De forma similar al caso de series univariadas, asumiremos una forma de AR(1) para una serie en panel: \\[\\begin{equation} \\Delta Y_{it} = \\mu_i + \\rho_i Y_{i t-1} + \\sum_{i = 1}^{k_i} \\varphi_{ij} \\Delta Y_{i t-j} + \\varepsilon_{it} (\\#eq:eq_AR_Panel) \\end{equation}\\] Donde \\(i = 1, \\ldots, N\\), \\(t = 1, \\ldots, T\\) y \\(\\varepsilon_{it}\\) es una v.a. iid que cumple con: \\[\\begin{eqnarray*} \\mathbb{E}[\\varepsilon_{it}] &amp; = &amp; 0 \\\\ \\mathbb{E}[\\varepsilon_{it}^2] &amp; = &amp; \\sigma^2_i &lt; \\infty \\\\ \\mathbb{E}[\\varepsilon_{it}^4] &amp; &lt; &amp; \\infty \\end{eqnarray*}\\] Al igual que en el caso univariado, en este tipo de pruebas buscamos identificar cuándo las series son I(1) y cuándo I(0). Para tal efecto, la prueba de raíz unitaria que utilizaremos está basada en una prueba Dickey-Fuller Aumentada en la cual la hipótesis nula (\\(H_0\\)) es que todas las series en el panel son no estacionarias, es decir, son I(1). Es decir, \\[\\begin{equation} H_0 : \\rho_i = 0 \\end{equation}\\] Por su parte, en el caso de la hipótesis nula tendremos dos: 7.1.3 Ejemplo: Pruebas de Raíces Unitarias en Panel Dependencies and Setup Download data from library PLM data(&quot;EmplUK&quot;, package=&quot;plm&quot;) data(&quot;Produc&quot;, package=&quot;plm&quot;) data(&quot;Grunfeld&quot;, package=&quot;plm&quot;) data(&quot;Wages&quot;, package=&quot;plm&quot;) Describe data: Grunfeld data (Grunfeld 1958) comprising 20 annual observations on the three variables real gross investment (invest), real value of the firm (value), and real value of the capital stock (capital) for 10 large US firms for the years 1935–1954 names(Grunfeld) ## [1] &quot;firm&quot; &quot;year&quot; &quot;inv&quot; &quot;value&quot; &quot;capital&quot; head(Grunfeld) ## firm year inv value capital ## 1 1 1935 317.6 3078.5 2.8 ## 2 1 1936 391.8 4661.7 52.6 ## 3 1 1937 410.6 5387.1 156.9 ## 4 1 1938 257.7 2792.2 209.2 ## 5 1 1939 330.8 4313.2 203.4 ## 6 1 1940 461.2 4643.9 207.2 Invest &lt;- data.frame(split( Grunfeld$inv, Grunfeld$firm )) # individuals in columns names(Invest) ## [1] &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; &quot;X5&quot; &quot;X6&quot; &quot;X7&quot; &quot;X8&quot; &quot;X9&quot; &quot;X10&quot; names(Invest) &lt;- c( &quot;Firm_1&quot;, &quot;Firm_2&quot;, &quot;Firm_3&quot;, &quot;Firm_4&quot;, &quot;Firm_5&quot;, &quot;Firm_6&quot;, &quot;Firm_7&quot;, &quot;Firm_8&quot;, &quot;Firm_9&quot;, &quot;Firm_10&quot; ) names(Invest) ## [1] &quot;Firm_1&quot; &quot;Firm_2&quot; &quot;Firm_3&quot; &quot;Firm_4&quot; &quot;Firm_5&quot; &quot;Firm_6&quot; &quot;Firm_7&quot; ## [8] &quot;Firm_8&quot; &quot;Firm_9&quot; &quot;Firm_10&quot; Plot: plot(Invest$Firm_1, type = &quot;l&quot;, col = 1, ylim = c(0, 1500), lty = 1, xlab = &quot;Tiempo&quot;, ylab = &quot;Real gross investment&quot;) lines(Invest$Firm_2, type = &quot;l&quot;, col = 2, lty = 2) lines(Invest$Firm_3, type = &quot;l&quot;, col = 3, lty = 1) lines(Invest$Firm_4, type = &quot;l&quot;, col = 4, lty = 2) lines(Invest$Firm_5, type = &quot;l&quot;, col = 5, lty = 1) lines(Invest$Firm_6, type = &quot;l&quot;, col = 6, lty = 2) lines(Invest$Firm_7, type = &quot;l&quot;, col = 7, lty = 1) lines(Invest$Firm_8, type = &quot;l&quot;, col = 8, lty = 2) lines(Invest$Firm_9, type = &quot;l&quot;, col = 9, lty = 1) lines(Invest$Firm_10, type = &quot;l&quot;, col = 10, lty = 2) legend(&quot;topleft&quot;, legend=c(&quot;Firm_1&quot;, &quot;Firm_2&quot;, &quot;Firm_3&quot;, &quot;Firm_4&quot;, &quot;Firm_5&quot;, &quot;Firm_6&quot;, &quot;Firm_7&quot;, &quot;Firm_8&quot;, &quot;Firm_9&quot;, &quot;Firm_10&quot;), col = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), lty = 1:2) Figure 7.1: Evolución de la inversión por empresa Unit Root Test: Test specifies the type of test to be performed among Levin et al. (2002), Im et al. (2003), Maddala and Wu (1999) and Hadri (2000) Consider Levin et al. (2002) purtest(log(Invest), test = &quot;levinlin&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4) ## Warning in selectT(l, theTs): the time series is short ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: log(Invest) ## z = -0.63214, p-value = 0.2636 ## alternative hypothesis: stationarity Same via: ts_LInvest &lt;- ts(log(Invest), start = 1935, end = 1954, freq = 1) ts_DLInvest &lt;- diff(ts(log(Invest), start = 1935, end = 1954, freq = 1), lag = 1, differences = 1) purtest(ts_LInvest, test = &quot;levinlin&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4) ## Warning in selectT(l, theTs): the time series is short ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: ts_LInvest ## z = -0.63214, p-value = 0.2636 ## alternative hypothesis: stationarity summary(purtest(ts_LInvest, test = &quot;levinlin&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4)) ## Warning in selectT(l, theTs): the time series is short ## Levin-Lin-Chu Unit-Root Test ## Exogenous variables: Individual Intercepts ## Automatic selection of lags using AIC: 0 - 3 lags (max: 4) ## statistic: -0.632 ## p-value: 0.264 ## ## lags obs rho trho p.trho sigma2ST sigma2LT ## Firm_1 0 19 0.006034802 0.05068864 0.96193110 0.03592530 0.014769876 ## Firm_2 1 18 -0.597193025 -2.95851997 0.03895745 0.05020448 0.014565004 ## Firm_3 3 16 -0.257291999 -1.97135237 0.29979349 0.03073205 0.019141095 ## Firm_4 0 19 -0.172070863 -1.21388533 0.67084314 0.06196079 0.018454390 ## Firm_5 1 18 -0.444726602 -1.69693864 0.43286539 0.04165516 0.006949122 ## Firm_6 2 17 0.062922587 0.72626225 0.99275186 0.02442139 0.009212851 ## Firm_7 2 17 -0.013830411 -0.08442715 0.94937802 0.03230724 0.007274365 ## Firm_8 1 18 -0.365024064 -2.26588313 0.18328720 0.06270208 0.026960667 ## Firm_9 0 19 -0.247940802 -1.72877417 0.41668489 0.05073398 0.013291332 ## Firm_10 1 18 -0.197003144 -1.52323519 0.52199891 0.07672404 0.062496477 purtest(ts_DLInvest, test = &quot;levinlin&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4) ## Warning in selectT(l, theTs): the time series is short ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: ts_DLInvest ## z = -8.5076, p-value &lt; 0.00000000000000022 ## alternative hypothesis: stationarity summary(purtest(ts_DLInvest, test = &quot;levinlin&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4)) ## Warning in selectT(l, theTs): the time series is short ## Levin-Lin-Chu Unit-Root Test ## Exogenous variables: Individual Intercepts ## Automatic selection of lags using AIC: 0 - 3 lags (max: 4) ## statistic: -8.508 ## p-value: 0 ## ## lags obs rho trho p.trho sigma2ST sigma2LT ## Firm_1 0 18 -0.9778879 -4.196070 0.000665206460867 0.03693616 0.007853674 ## Firm_2 3 15 -2.7288380 -4.327510 0.000392850467043 0.02832436 0.031954702 ## Firm_3 2 16 -1.7848678 -5.650917 0.000000790572465 0.03819653 0.022971185 ## Firm_4 1 17 -1.5737470 -5.320458 0.000004309793729 0.04231849 0.030313940 ## Firm_5 2 16 -2.8118586 -5.181259 0.000008564306919 0.02737020 0.015226407 ## Firm_6 1 17 -1.4850311 -4.776618 0.000057171776145 0.02517911 0.008588652 ## Firm_7 1 17 -2.1076263 -6.541451 0.000000005364403 0.03232079 0.012780800 ## Firm_8 2 16 -1.7790739 -4.976338 0.000022818242011 0.03277746 0.034905699 ## Firm_9 0 18 -1.2600946 -5.304469 0.000004667479100 0.05666016 0.019502840 ## Firm_10 0 18 -0.7889550 -3.421651 0.010275559745253 0.08661396 0.030216984 Consider Im-Pesaran-Shin Unit-Root Test (2003) purtest(ts_LInvest, test = &quot;ips&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4) ## ## Im-Pesaran-Shin Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: ts_LInvest ## Wtbar = 0.6833, p-value = 0.7528 ## alternative hypothesis: stationarity summary(purtest(ts_LInvest, test = &quot;ips&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4)) ## Im-Pesaran-Shin Unit-Root Test ## Exogenous variables: Individual Intercepts ## Automatic selection of lags using AIC: 0 - 3 lags (max: 4) ## statistic (Wtbar): 0.683 ## p-value: 0.753 ## ## lags obs rho trho p.trho mean var ## Firm_1 0 19 0.006034802 0.05068864 0.96193110 -1.5204 0.8654 ## Firm_2 1 18 -0.597193025 -2.95851997 0.03895745 -1.5108 0.9534 ## Firm_3 3 16 -0.257291999 -1.97135237 0.29979349 -1.3754 1.1522 ## Firm_4 0 19 -0.172070863 -1.21388533 0.67084314 -1.5204 0.8654 ## Firm_5 1 18 -0.444726602 -1.69693864 0.43286539 -1.5108 0.9534 ## Firm_6 2 17 0.062922587 0.72626225 0.99275186 -1.4034 1.0344 ## Firm_7 2 17 -0.013830411 -0.08442715 0.94937802 -1.4034 1.0344 ## Firm_8 1 18 -0.365024064 -2.26588313 0.18328720 -1.5108 0.9534 ## Firm_9 0 19 -0.247940802 -1.72877417 0.41668489 -1.5204 0.8654 ## Firm_10 1 18 -0.197003144 -1.52323519 0.52199891 -1.5108 0.9534 purtest(ts_DLInvest, test = &quot;ips&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4) ## ## Im-Pesaran-Shin Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: ts_DLInvest ## Wtbar = -11.14, p-value &lt; 0.00000000000000022 ## alternative hypothesis: stationarity summary(purtest(ts_DLInvest, test = &quot;ips&quot;, exo = &quot;intercept&quot;, lags = &quot;AIC&quot;, pmax = 4)) ## Im-Pesaran-Shin Unit-Root Test ## Exogenous variables: Individual Intercepts ## Automatic selection of lags using AIC: 0 - 3 lags (max: 4) ## statistic (Wtbar): -11.14 ## p-value: 0 ## ## lags obs rho trho p.trho mean var ## Firm_1 0 18 -0.9778879 -4.196070 0.000665206460867 -1.5188 0.8798 ## Firm_2 3 15 -2.7288380 -4.327510 0.000392850467043 -1.3660 1.1810 ## Firm_3 2 16 -1.7848678 -5.650917 0.000000790572465 -1.3952 1.0562 ## Firm_4 1 17 -1.5737470 -5.320458 0.000004309793729 -1.5082 0.9726 ## Firm_5 2 16 -2.8118586 -5.181259 0.000008564306919 -1.3952 1.0562 ## Firm_6 1 17 -1.4850311 -4.776618 0.000057171776145 -1.5082 0.9726 ## Firm_7 1 17 -2.1076263 -6.541451 0.000000005364403 -1.5082 0.9726 ## Firm_8 2 16 -1.7790739 -4.976338 0.000022818242011 -1.3952 1.0562 ## Firm_9 0 18 -1.2600946 -5.304469 0.000004667479100 -1.5188 0.8798 ## Firm_10 0 18 -0.7889550 -3.421651 0.010275559745253 -1.5188 0.8798 7.1.4 Ejemplo: Modelo Dif-in-Dif para el INPC de análisis clínicos (update nov-2022) Este ejemplo modela el efecto que tiene que una empresa de laboratorios de análisis clínicos compre a sus competidores en diferentes ciudades del país. Para lo cual usaremos información del Índice Nacional de Precios al Consumidor de los servicios de análisis clínicos. Dependencies and Setup Importamos Datos desde un archivo de Excel. Los datos están en formato panel e incluyen: Partamos de la siguiente ecuación: \\[\\begin{equation} y_{it} = \\beta_1 + \\beta_2 T_t + \\beta_3 D_t + \\theta T_t \\times D_t + \\boldsymbol{\\gamma} \\mathbf{X}_{it} + \\varepsilon_{it} (\\#eq:eq_DiD) \\end{equation}\\] Donde \\(y_{it}\\) es la variable sobre la cual queremos evaluar el efecto de un tratamiento, \\(\\mathbf{X}_{it}\\) es un conjunto de variables de control, que incluye a las variables de efectos fijos (temporales, de individuos o del tipo que se decida) y, en su caso, una tendencia, entre otras. Finalmente, las variables \\(T_t\\) y \\(D_i\\) son variables dicotómicas que identifican con el valor de 1 el momento a partir del cual se observa el tratamiento y con 0 en cualquier otro caso, y con 1 los individuos que fueron tratados y con 0 en cualquier otro caso, respectivamente. De esta forma, el producto \\(T_t \\times D_t\\) sería una variable dicotómica que indicaría con un 1 a aquellos individuos que fueron tratados a partir del momento en que se implementó el tratamiento y con 0 en cualquier otro caso. Bajo este escenario, \\(\\theta\\) es un coeficiente que captura el efecto del tratamiento, condicional a que se ha controlado por una serie de factores. Otra forma de ver lo que se indica en la ecuación @ref(eq:eq_DiD) es como la comparación de la diferencia en diferencia descrita por: \\[\\begin{eqnarray} E &amp; = &amp; [ (\\overline{y}_{exit} | treatment) - (\\overline{y}_{baseline} | treatment) ] \\nonumber \\\\ &amp; &amp; - [ (\\overline{y}_{exit} | placebo) - (\\overline{y}_{baseline} | placebo) ] (\\#eq:eq_DiD_E) \\end{eqnarray}\\] La expresión @ref(eq:eq_DiD_E) cuantifica el efecto que tiene el tratamiento como la diferencia de medias de dos grupos: un grupo tratado y otro no tratado. Dentro de los cuales se han comparado las medias respecto de una línea base. Podemos plantear esto desde esta otra perspectiva. Pensemos que solo tenemos dos periodos \\(t = {1, 2}\\), en el cual en 1 no ha ocurrido el tratamiento y en 2 ya ha ocurrido. Así, el cambio en la variable de respuesta para los individuos tratados será (suponiendo que omitimos la matriz \\(\\mathbf{X}_{it}\\) en la ecuación @ref(eq:eq_DiD)): \\[\\begin{eqnarray} \\mathbb{E}[ y_{i2} | D_i = 1 ] - \\mathbb{E}[ y_{i1} | D_i = 1 ] &amp; = &amp; ( \\beta_1 + \\beta_2 + \\beta_3 + \\theta ) - ( \\beta_1 + \\beta_3 + \\theta ) \\nonumber \\\\ &amp; = &amp; \\beta_2 + \\theta (\\#eq:eq_DiD_Treat) \\end{eqnarray}\\] Ahora, sobre los no tratados: \\[\\begin{eqnarray} \\mathbb{E}[ y_{i2} | D_i = 0 ] - \\mathbb{E}[ y_{i1} | D_i = 0 ] &amp; = &amp; ( \\beta_1 + \\beta_2 ) - ( \\beta_1 ) \\nonumber \\\\ &amp; = &amp; \\beta_2 (\\#eq:eq_DiD_NoTreat) \\end{eqnarray}\\] Así, la diferencia en diferencia resulta de restar la ecuación @ref(eq:eq_DiD_NoTreat) a la ecuación @ref(eq:eq_DiD_Treat): \\[\\begin{eqnarray} &amp; &amp; ( \\mathbb{E}[ y_{i2} | D_i = 1 ] - \\mathbb{E}[ y_{i1} | D_i = 1 ] ) - ( \\mathbb{E}[ y_{i2} | D_i = 0 ] - \\mathbb{E}[ y_{i1} | D_i = 0 ] ) \\nonumber \\\\ &amp; &amp; = \\theta (\\#eq:eq_Aplicada) \\end{eqnarray}\\] En nuestro caso, utilizaremos como tratamiento la entrada al mercado de un laboratorio de análisis clínicos en una ciudad determinada. ## [1] 13420 17 ## # A tibble: 6 × 17 ## Fecha Mes Anio Ciudad INPC Dummy_FIRMA Marca_01 Marca_02 Marca_03 Marca_04 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jul … 7 2002 Área … 68.3… 0 0 0 0 0 ## 2 Ago … 8 2002 Área … 68.3… 0 0 0 0 0 ## 3 Sep … 9 2002 Área … 68.3… 0 0 0 0 0 ## 4 Oct … 10 2002 Área … 68.3… 0 0 0 0 0 ## 5 Nov … 11 2002 Área … 68.3… 0 0 0 0 0 ## 6 Dic … 12 2002 Área … 68.3… 0 0 0 0 0 ## # ℹ 7 more variables: Marca_05 &lt;dbl&gt;, Marca_06 &lt;dbl&gt;, Marca_07 &lt;dbl&gt;, ## # Marca_08 &lt;dbl&gt;, Marca_09 &lt;dbl&gt;, Marca_10 &lt;dbl&gt;, Marca_11 &lt;dbl&gt; Selección de ciudades: Data &lt;- Data[ which( Data$Ciudad != &#39;Atlacomulco, Edo. de Méx.&#39; &amp; Data$Ciudad != &#39;Cancún, Q.R.&#39; &amp; Data$Ciudad != &#39;Coatzacoalcos, Ver.&#39; &amp; Data$Ciudad != &#39;Esperanza, Son.&#39; &amp; Data$Ciudad != &#39;Izúcar de Matamoros, Pue.&#39; &amp; Data$Ciudad != &#39;Pachuca, Hgo.&#39; &amp; Data$Ciudad != &#39;Tuxtla Gutiérrez, Chis.&#39; &amp; Data$Ciudad != &#39;Zacatecas, Zac.&#39; ), ] Tranformaciones: # Log de INPC Data$LINPC &lt;- log( as.numeric(Data$INPC) , base = exp(1)) # Volvemos factor a la fecha Data$Periodo &lt;- factor(Data$Fecha, order = TRUE, levels = c( &quot;Jul 2002&quot;, &quot;Ago 2002&quot;, &quot;Sep 2002&quot;, &quot;Oct 2002&quot;, &quot;Nov 2002&quot;, &quot;Dic 2002&quot;, &quot;Ene 2003&quot;, &quot;Feb 2003&quot;, &quot;Mar 2003&quot;, &quot;Abr 2003&quot;, &quot;May 2003&quot;, &quot;Jun 2003&quot;, &quot;Jul 2003&quot;, &quot;Ago 2003&quot;, &quot;Sep 2003&quot;, &quot;Oct 2003&quot;, &quot;Nov 2003&quot;, &quot;Dic 2003&quot;, &quot;Ene 2004&quot;, &quot;Feb 2004&quot;, &quot;Mar 2004&quot;, &quot;Abr 2004&quot;, &quot;May 2004&quot;, &quot;Jun 2004&quot;, &quot;Jul 2004&quot;, &quot;Ago 2004&quot;, &quot;Sep 2004&quot;, &quot;Oct 2004&quot;, &quot;Nov 2004&quot;, &quot;Dic 2004&quot;, &quot;Ene 2005&quot;, &quot;Feb 2005&quot;, &quot;Mar 2005&quot;, &quot;Abr 2005&quot;, &quot;May 2005&quot;, &quot;Jun 2005&quot;, &quot;Jul 2005&quot;, &quot;Ago 2005&quot;, &quot;Sep 2005&quot;, &quot;Oct 2005&quot;, &quot;Nov 2005&quot;, &quot;Dic 2005&quot;, &quot;Ene 2006&quot;, &quot;Feb 2006&quot;, &quot;Mar 2006&quot;, &quot;Abr 2006&quot;, &quot;May 2006&quot;, &quot;Jun 2006&quot;, &quot;Jul 2006&quot;, &quot;Ago 2006&quot;, &quot;Sep 2006&quot;, &quot;Oct 2006&quot;, &quot;Nov 2006&quot;, &quot;Dic 2006&quot;, &quot;Ene 2007&quot;, &quot;Feb 2007&quot;, &quot;Mar 2007&quot;, &quot;Abr 2007&quot;, &quot;May 2007&quot;, &quot;Jun 2007&quot;, &quot;Jul 2007&quot;, &quot;Ago 2007&quot;, &quot;Sep 2007&quot;, &quot;Oct 2007&quot;, &quot;Nov 2007&quot;, &quot;Dic 2007&quot;, &quot;Ene 2008&quot;, &quot;Feb 2008&quot;, &quot;Mar 2008&quot;, &quot;Abr 2008&quot;, &quot;May 2008&quot;, &quot;Jun 2008&quot;, &quot;Jul 2008&quot;, &quot;Ago 2008&quot;, &quot;Sep 2008&quot;, &quot;Oct 2008&quot;, &quot;Nov 2008&quot;, &quot;Dic 2008&quot;, &quot;Ene 2009&quot;, &quot;Feb 2009&quot;, &quot;Mar 2009&quot;, &quot;Abr 2009&quot;, &quot;May 2009&quot;, &quot;Jun 2009&quot;, &quot;Jul 2009&quot;, &quot;Ago 2009&quot;, &quot;Sep 2009&quot;, &quot;Oct 2009&quot;, &quot;Nov 2009&quot;, &quot;Dic 2009&quot;, &quot;Ene 2010&quot;, &quot;Feb 2010&quot;, &quot;Mar 2010&quot;, &quot;Abr 2010&quot;, &quot;May 2010&quot;, &quot;Jun 2010&quot;, &quot;Jul 2010&quot;, &quot;Ago 2010&quot;, &quot;Sep 2010&quot;, &quot;Oct 2010&quot;, &quot;Nov 2010&quot;, &quot;Dic 2010&quot;, &quot;Ene 2011&quot;, &quot;Feb 2011&quot;, &quot;Mar 2011&quot;, &quot;Abr 2011&quot;, &quot;May 2011&quot;, &quot;Jun 2011&quot;, &quot;Jul 2011&quot;, &quot;Ago 2011&quot;, &quot;Sep 2011&quot;, &quot;Oct 2011&quot;, &quot;Nov 2011&quot;, &quot;Dic 2011&quot;, &quot;Ene 2012&quot;, &quot;Feb 2012&quot;, &quot;Mar 2012&quot;, &quot;Abr 2012&quot;, &quot;May 2012&quot;, &quot;Jun 2012&quot;, &quot;Jul 2012&quot;, &quot;Ago 2012&quot;, &quot;Sep 2012&quot;, &quot;Oct 2012&quot;, &quot;Nov 2012&quot;, &quot;Dic 2012&quot;, &quot;Ene 2013&quot;, &quot;Feb 2013&quot;, &quot;Mar 2013&quot;, &quot;Abr 2013&quot;, &quot;May 2013&quot;, &quot;Jun 2013&quot;, &quot;Jul 2013&quot;, &quot;Ago 2013&quot;, &quot;Sep 2013&quot;, &quot;Oct 2013&quot;, &quot;Nov 2013&quot;, &quot;Dic 2013&quot;, &quot;Ene 2014&quot;, &quot;Feb 2014&quot;, &quot;Mar 2014&quot;, &quot;Abr 2014&quot;, &quot;May 2014&quot;, &quot;Jun 2014&quot;, &quot;Jul 2014&quot;, &quot;Ago 2014&quot;, &quot;Sep 2014&quot;, &quot;Oct 2014&quot;, &quot;Nov 2014&quot;, &quot;Dic 2014&quot;, &quot;Ene 2015&quot;, &quot;Feb 2015&quot;, &quot;Mar 2015&quot;, &quot;Abr 2015&quot;, &quot;May 2015&quot;, &quot;Jun 2015&quot;, &quot;Jul 2015&quot;, &quot;Ago 2015&quot;, &quot;Sep 2015&quot;, &quot;Oct 2015&quot;, &quot;Nov 2015&quot;, &quot;Dic 2015&quot;, &quot;Ene 2016&quot;, &quot;Feb 2016&quot;, &quot;Mar 2016&quot;, &quot;Abr 2016&quot;, &quot;May 2016&quot;, &quot;Jun 2016&quot;, &quot;Jul 2016&quot;, &quot;Ago 2016&quot;, &quot;Sep 2016&quot;, &quot;Oct 2016&quot;, &quot;Nov 2016&quot;, &quot;Dic 2016&quot;, &quot;Ene 2017&quot;, &quot;Feb 2017&quot;, &quot;Mar 2017&quot;, &quot;Abr 2017&quot;, &quot;May 2017&quot;, &quot;Jun 2017&quot;, &quot;Jul 2017&quot;, &quot;Ago 2017&quot;, &quot;Sep 2017&quot;, &quot;Oct 2017&quot;, &quot;Nov 2017&quot;, &quot;Dic 2017&quot;, &quot;Ene 2018&quot;, &quot;Feb 2018&quot;, &quot;Mar 2018&quot;, &quot;Abr 2018&quot;, &quot;May 2018&quot;, &quot;Jun 2018&quot;, &quot;Jul 2018&quot;, &quot;Ago 2018&quot;, &quot;Sep 2018&quot;, &quot;Oct 2018&quot;, &quot;Nov 2018&quot;, &quot;Dic 2018&quot;, &quot;Ene 2019&quot;, &quot;Feb 2019&quot;, &quot;Mar 2019&quot;, &quot;Abr 2019&quot;, &quot;May 2019&quot;, &quot;Jun 2019&quot;, &quot;Jul 2019&quot;, &quot;Ago 2019&quot;, &quot;Sep 2019&quot;, &quot;Oct 2019&quot;, &quot;Nov 2019&quot;, &quot;Dic 2019&quot;, &quot;Ene 2020&quot;, &quot;Feb 2020&quot;, &quot;Mar 2020&quot;, &quot;Abr 2020&quot;, &quot;May 2020&quot;, &quot;Jun 2020&quot;, &quot;Jul 2020&quot;, &quot;Ago 2020&quot;, &quot;Sep 2020&quot;, &quot;Oct 2020&quot;, &quot;Nov 2020&quot;, &quot;Dic 2020&quot;, &quot;Ene 2021&quot;, &quot;Feb 2021&quot;, &quot;Mar 2021&quot;, &quot;Abr 2021&quot;, &quot;May 2021&quot;, &quot;Jun 2021&quot;, &quot;Jul 2021&quot;, &quot;Ago 2021&quot;, &quot;Sep 2021&quot;, &quot;Oct 2021&quot;, &quot;Nov 2021&quot;, &quot;Dic 2021&quot;, &quot;Ene 2022&quot;, &quot;Feb 2022&quot;, &quot;Mar 2022&quot;, &quot;Abr 2022&quot;, &quot;May 2022&quot;, &quot;Jun 2022&quot;, &quot;Jul 2022&quot;, &quot;Ago 2022&quot;, &quot;Sep 2022&quot;, &quot; Oct 2022&quot;, &quot;Nov 2022&quot;, &quot;Dic 2022&quot; )) Plot Data %&gt;% ggplot( aes(x = Periodo, y = LINPC, group = Ciudad, color = Ciudad )) + geom_line() + theme(legend.title = element_text(color = &quot;black&quot;, size = 8), legend.text = element_text(color = &quot;black&quot;, size = 5 )) + theme(legend.position=&quot;bottom&quot;) + theme(axis.text.x = element_text( size = 8, angle = 90)) Hagamos una pruebas de Raíces Unitarias. Change to format wide: ## # A tibble: 6 × 48 ## Fecha `Acapulco, Gro.` `Aguascalientes, Ags.` Área Metropolitana de la Cd…¹ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abr 2003 4.39 4.35 4.26 ## 2 Abr 2004 4.39 4.36 4.31 ## 3 Abr 2005 4.44 4.36 4.36 ## 4 Abr 2006 4.47 4.33 4.40 ## 5 Abr 2007 4.54 4.38 4.42 ## 6 Abr 2008 4.60 4.41 4.44 ## # ℹ abbreviated name: ¹​`Área Metropolitana de la Cd. de México` ## # ℹ 44 more variables: `Campeche, Camp.` &lt;dbl&gt;, `Cd. Acuña, Coah.` &lt;dbl&gt;, ## # `Cd. Jiménez, Chih.` &lt;dbl&gt;, `Cd. Juárez, Chih.` &lt;dbl&gt;, ## # `Chetumal, Q.R.` &lt;dbl&gt;, `Chihuahua, Chih.` &lt;dbl&gt;, `Colima, Col.` &lt;dbl&gt;, ## # `Córdoba, Ver.` &lt;dbl&gt;, `Cortazar, Gto.` &lt;dbl&gt;, `Cuernavaca, Mor.` &lt;dbl&gt;, ## # `Culiacán, Sin.` &lt;dbl&gt;, `Durango, Dgo.` &lt;dbl&gt;, `Fresnillo, Zac.` &lt;dbl&gt;, ## # `Guadalajara, Jal.` &lt;dbl&gt;, `Hermosillo, Son.` &lt;dbl&gt;, … Time Series: ts_LINPC &lt;- ts( LINPC[c( 2:48 )], start = c(2002, 7), freq = 12) ts_DLINPC &lt;- diff(ts( LINPC[c( 2:48 )], start = c(2002, 7), freq = 12), lag = 1, differences = 1) Unit Root Test: Test specifies the type of test to be performed among Levin et al. (2002), Im et al. (2003), Maddala and Wu (1999) and Hadri (2000) Optimal lags: \\[\\begin{eqnarray*} p &amp; = &amp; Int(4*(T/100)^{(1/4)}) \\\\ &amp; = &amp; Int(4*(244/100)^{(1/4)}) \\\\ &amp; = &amp; Int(4.99) \\\\ &amp; = &amp; 4 \\end{eqnarray*}\\] Consider Levin et al. (2002): purtest(ts_LINPC, test = &quot;levinlin&quot;, exo = &quot;trend&quot;, # exo = c(&quot;none&quot;, &quot;intercept&quot;, &quot;trend&quot;), lags = &quot;AIC&quot;, pmax = 4) ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts and ## Trend) ## ## data: ts_LINPC ## z = -56.022, p-value &lt; 2.2e-16 ## alternative hypothesis: stationarity purtest(ts_DLINPC, test = &quot;levinlin&quot;, exo = &quot;trend&quot;, # exo = c(&quot;none&quot;, &quot;intercept&quot;, &quot;trend&quot;), lags = &quot;AIC&quot;, pmax = 4) ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts and ## Trend) ## ## data: ts_DLINPC ## z = -107.84, p-value &lt; 2.2e-16 ## alternative hypothesis: stationarity Consider Choi (2001): purtest(ts_LINPC, test = &quot;logit&quot;, exo = &quot;trend&quot;, # exo = c(&quot;none&quot;, &quot;intercept&quot;, &quot;trend&quot;), lags = &quot;AIC&quot;, pmax = 4) ## ## Choi&#39;s Logit Unit-Root Test (ex. var.: Individual Intercepts and Trend) ## ## data: ts_LINPC ## L* = -85.506, df = 239, p-value &lt; 2.2e-16 ## alternative hypothesis: stationarity purtest(ts_DLINPC, test = &quot;logit&quot;, exo = &quot;trend&quot;, # exo = c(&quot;none&quot;, &quot;intercept&quot;, &quot;trend&quot;), lags = &quot;AIC&quot;, pmax = 4) ## ## Choi&#39;s Logit Unit-Root Test (ex. var.: Individual Intercepts and Trend) ## ## data: ts_DLINPC ## L* = -278.56, df = 239, p-value &lt; 2.2e-16 ## alternative hypothesis: stationarity Consider Hadri (2000): purtest(ts_LINPC, test = &quot;hadri&quot;, exo = &quot;trend&quot;, # exo = c(&quot;none&quot;, &quot;intercept&quot;, &quot;trend&quot;), lags = &quot;AIC&quot;, pmax = 4) ## ## Hadri Test (ex. var.: Individual Intercepts and Trend) (Heterosked. ## Consistent) ## ## data: ts_LINPC ## z = -2.7, p-value = 0.9965 ## alternative hypothesis: at least one series has a unit root purtest(ts_DLINPC, test = &quot;hadri&quot;, exo = &quot;trend&quot;, # exo = c(&quot;none&quot;, &quot;intercept&quot;, &quot;trend&quot;), lags = &quot;AIC&quot;, pmax = 4) ## ## Hadri Test (ex. var.: Individual Intercepts and Trend) (Heterosked. ## Consistent) ## ## data: ts_DLINPC ## z = -8.9351, p-value = 1 ## alternative hypothesis: at least one series has a unit root Estimación PANEL con TREND: # Modelo Final fixed &lt;- plm( LINPC ~ Dummy_FIRMA + Marca_01 + Marca_02 + Marca_03 + Marca_04 + Marca_05 + Marca_06 + Marca_07 + Marca_08 + Marca_09 + Marca_10 + Marca_11 + as.numeric(Periodo), data = Data, index=c(&quot;Ciudad&quot;, &quot;Periodo&quot;), model = &quot;within&quot;) ## Warning in pdata.frame(data, index): at least one NA in at least one index dimension in resulting pdata.frame ## to find out which, use, e.g., table(index(your_pdataframe), useNA = &quot;ifany&quot;) summary(fixed) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = LINPC ~ Dummy_FIRMA + Marca_01 + Marca_02 + Marca_03 + ## Marca_04 + Marca_05 + Marca_06 + Marca_07 + Marca_08 + Marca_09 + ## Marca_10 + Marca_11 + as.numeric(Periodo), data = Data, model = &quot;within&quot;, ## index = c(&quot;Ciudad&quot;, &quot;Periodo&quot;)) ## ## Balanced Panel: n = 47, T = 243, N = 11421 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.4220510 -0.0416329 0.0026154 0.0471479 0.2275376 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## Dummy_FIRMA -0.05972092 0.01892851 -3.1551 0.0016087 ** ## Marca_01 0.04545555 0.02069006 2.1970 0.0280423 * ## Marca_02 0.05452288 0.02372909 2.2977 0.0215956 * ## Marca_03 -0.02925504 0.02041744 -1.4328 0.1519296 ## Marca_04 -0.07440826 0.02418665 -3.0764 0.0021000 ** ## Marca_05 0.02633713 0.01045219 2.5198 0.0117567 * ## Marca_06 0.12521494 0.01991655 6.2870 3.356e-10 *** ## Marca_07 -0.09242328 0.02514895 -3.6750 0.0002389 *** ## Marca_08 0.03362324 0.02086579 1.6114 0.1071193 ## Marca_09 0.05630828 0.02121641 2.6540 0.0079655 ** ## Marca_10 -0.04866105 0.02638856 -1.8440 0.0652062 . ## Marca_11 -0.03582620 0.03031018 -1.1820 0.2372361 ## as.numeric(Periodo) 0.00220384 0.00001018 216.4961 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 330.79 ## Residual Sum of Squares: 60.08 ## R-Squared: 0.81838 ## Adj. R-Squared: 0.81743 ## F-statistic: 3937.79 on 13 and 11361 DF, p-value: &lt; 2.22e-16 # Modelo Final fixed &lt;- plm( LINPC ~ Dummy_FIRMA + as.numeric(Periodo), data = Data, index=c(&quot;Ciudad&quot;, &quot;Periodo&quot;), model = &quot;within&quot;) ## Warning in pdata.frame(data, index): at least one NA in at least one index dimension in resulting pdata.frame ## to find out which, use, e.g., table(index(your_pdataframe), useNA = &quot;ifany&quot;) summary(fixed) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = LINPC ~ Dummy_FIRMA + as.numeric(Periodo), data = Data, ## model = &quot;within&quot;, index = c(&quot;Ciudad&quot;, &quot;Periodo&quot;)) ## ## Balanced Panel: n = 47, T = 243, N = 11421 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.4337032 -0.0421466 0.0026796 0.0480377 0.2504866 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## Dummy_FIRMA -0.02239578 0.00417897 -5.3592 0.00000008525 *** ## as.numeric(Periodo) 0.00220318 0.00001032 213.4825 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 330.79 ## Residual Sum of Squares: 61.94 ## R-Squared: 0.81275 ## Adj. R-Squared: 0.81196 ## F-statistic: 24680.5 on 2 and 11372 DF, p-value: &lt; 2.22e-16 7.1.5 Panel VAR En esta sección extenderemos el caso del modelo VAR(p) a uno en forma panel. En este caso asumimos que las variables exógenas son los \\(p\\) rezagos de las \\(k\\) variables endógenas. Consideremos el siguiente caso de un modelo panel VAR con efectos fijos –ciertamente es posible hacer estimaciones con efectos aleatorios, no obstante, requiere de supuestos adicionales que no contemplamos en estas notas–, el cual es la forma más común de estimación: \\[\\begin{equation} \\mathbf{Y}_{it} = \\mu_i + \\sum_{l = 1}^p \\mathbf{A}_l \\mathbf{Y}_{i t - l} + \\mathbf{B} \\mathbf{X}_{it} + \\varepsilon_{it} (\\#eq:eq_PVAR) \\end{equation}\\] Donde \\(\\mathbf{Y}_{it}\\) es un vector de variables endógenas estacionarias para la unidad de corte transversal \\(i\\) en el tiempo \\(t\\), \\(\\mathbf{X}_{it}\\) es una matriz que contiene variables exógenas, y \\(\\varepsilon_{it}\\) es un término de error que cumple con: \\[\\begin{eqnarray*} \\mathbb{E}[\\varepsilon_{it}] &amp; = &amp; 0 \\\\ Var[\\varepsilon_{it}] &amp; = &amp; \\Sigma_\\varepsilon \\end{eqnarray*}\\] Donde \\(\\Sigma_\\varepsilon\\) es una matriz definida positiva. Para el porceso de estimación, la ecuación @ref(eq:eq_PVAR) se modifica en su versión en diferencias para quedar como: \\[\\begin{equation} \\Delta \\mathbf{Y}_{it} = \\sum_{l = 1}^p \\mathbf{A}_l \\Delta \\mathbf{Y}_{i t - l} + \\mathbf{B} \\Delta \\mathbf{X}_{it} + \\Delta \\varepsilon_{it} (\\#eq:eq_Dinamic_PVAR) \\end{equation}\\] La ecuación @ref(eq:eq_Dinamic_PVAR) se estima por un GMM. 7.2 Ejemplos: Panel VAR(p) Dependencies and Setup 7.2.1 Ejemplo 1 We used the dynamic panel literature by Arellano and Bond (1991), Blundell and Bond (1998) and Roodman (2009b). This data set describes employment, wages, capital and output of 140 firms in the United Kingdom from 1976 to 1984. We estimate: Employment is explained by past values of employment (“l” lags), current and first lag of wages and output and current value of capital. ## [1] &quot;c1&quot; &quot;ind&quot; &quot;year&quot; &quot;emp&quot; &quot;wage&quot; &quot;cap&quot; ## [7] &quot;indoutpt&quot; &quot;n&quot; &quot;w&quot; &quot;k&quot; &quot;ys&quot; &quot;rec&quot; ## [13] &quot;yearm1&quot; &quot;id&quot; &quot;nL1&quot; &quot;nL2&quot; &quot;wL1&quot; &quot;kL1&quot; ## [19] &quot;kL2&quot; &quot;ysL1&quot; &quot;ysL2&quot; &quot;yr1976&quot; &quot;yr1977&quot; &quot;yr1978&quot; ## [25] &quot;yr1979&quot; &quot;yr1980&quot; &quot;yr1981&quot; &quot;yr1982&quot; &quot;yr1983&quot; &quot;yr1984&quot; ## c1 ind year emp wage cap indoutpt n w k ## 1 1-1 7 1977 5.041 13.1516 0.5894 95.7072 1.617604 2.576543 -0.5286502 ## 2 2-1 7 1978 5.600 12.3018 0.6318 97.3569 1.722767 2.509746 -0.4591824 ## 3 3-1 7 1979 5.015 12.8395 0.6771 99.6083 1.612433 2.552526 -0.3899363 ## 4 4-1 7 1980 4.715 13.8039 0.6171 100.5501 1.550749 2.624951 -0.4827242 ## 5 5-1 7 1981 4.093 14.2897 0.5076 99.5581 1.409278 2.659539 -0.6780615 ## 6 6-1 7 1982 3.166 14.8681 0.4229 98.6151 1.152469 2.699218 -0.8606195 ## ys rec yearm1 id nL1 nL2 wL1 kL1 kL2 ## 1 4.561294 1 1977 1 NA NA NA NA NA ## 2 4.578383 2 1977 1 1.617604 NA 2.576543 -0.5286502 NA ## 3 4.601245 3 1978 1 1.722767 1.617604 2.509746 -0.4591824 -0.5286502 ## 4 4.610656 4 1979 1 1.612433 1.722767 2.552526 -0.3899363 -0.4591824 ## 5 4.600741 5 1980 1 1.550749 1.612433 2.624951 -0.4827242 -0.3899363 ## 6 4.591224 6 1981 1 1.409278 1.550749 2.659539 -0.6780615 -0.4827242 ## ysL1 ysL2 yr1976 yr1977 yr1978 yr1979 yr1980 yr1981 yr1982 yr1983 ## 1 NA NA 0 1 0 0 0 0 0 0 ## 2 4.561294 NA 0 0 1 0 0 0 0 0 ## 3 4.578383 4.561294 0 0 0 1 0 0 0 0 ## 4 4.601245 4.578383 0 0 0 0 1 0 0 0 ## 5 4.610656 4.601245 0 0 0 0 0 1 0 0 ## 6 4.600741 4.610656 0 0 0 0 0 0 1 0 ## yr1984 ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 Estimación #?pvargmm Arellano_Bond_1991_table4b &lt;- pvargmm( dependent_vars = c(&quot;n&quot;), lags = 2, exog_vars = c(&quot;w&quot;, &quot;wL1&quot;, &quot;k&quot;, &quot;ys&quot;, &quot;ysL1&quot;, &quot;yr1979&quot;, &quot;yr1980&quot;, &quot;yr1981&quot;, &quot;yr1982&quot;, &quot;yr1983&quot;, &quot;yr1984&quot;), transformation = &quot;fd&quot;, data = abdata, panel_identifier = c(&quot;id&quot;, &quot;year&quot;), steps = c(&quot;twostep&quot;), system_instruments = FALSE, max_instr_dependent_vars = 99, min_instr_dependent_vars = 2L, collapse = FALSE) summary(Arellano_Bond_1991_table4b) Dynamic Panel VAR estimation, two-step GMM Transformation: First-differencesGroup variable: idTime variable: yearNumber of observations = 611Number of groups = 140Obs per group: min = 4Obs per group: avg = 4.36428571428571Obs per group: max = 6Number of instruments = 38   n lag1_n 0.4742*   (0.1854) lag2_n -0.0530   (0.0517) w -0.5132***   (0.1456) wL1 0.2246   (0.1419) k 0.2927***   (0.0626) ys 0.6098***   (0.1563) ysL1 -0.4464*   (0.2173) yr1979 0.0105   (0.0099) yr1980 0.0247   (0.0158) yr1981 -0.0158   (0.0267) yr1982 -0.0374   (0.0300) yr1983 -0.0393   (0.0347) yr1984 -0.0495   (0.0349) ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Instruments for equation Standardw, wL1, k, ys, ysL1, yr1979, yr1980, yr1981, yr1982, yr1983, yr1984 GMM-typeDependent vars: L(2,6))Collapse = FALSE Hansen test of overid. restrictions: chi2(25) = 30.11 Prob &gt; chi2 = 0.22(Robust, but weakened by many instruments.) 7.2.2 Ejemplo 2: We used the panel data set consists of 265 Swedish municipalities and covers 9 years (1979-1987). These variables include total expenditures (expenditures), total own-source revenues (revenues) and intergovernmental grants received by the municipality (grants). Source: Dahlberg and Johansson (2000) Grants from the central to the local government are of three kinds: support to municipalities with small tax capacity, grants toward the running of certain local government activities and grants toward certain investments. data(&quot;Dahlberg&quot;) names(Dahlberg) ## [1] &quot;id&quot; &quot;year&quot; &quot;expenditures&quot; &quot;revenues&quot; &quot;grants&quot; head(Dahlberg) ## id year expenditures revenues grants ## 1 114 1979 0.0229736 0.0181770 0.0054429 ## 2 114 1980 0.0266307 0.0209142 0.0057304 ## 3 114 1981 0.0273253 0.0210836 0.0056647 ## 4 114 1982 0.0288704 0.0234310 0.0058859 ## 5 114 1983 0.0226474 0.0179979 0.0055908 ## 6 114 1984 0.0215601 0.0179949 0.0047536 Estimación: ex1_dahlberg_data &lt;- pvargmm(dependent_vars = c(&quot;expenditures&quot;, &quot;revenues&quot;, &quot;grants&quot;), lags = 1, transformation = &quot;fod&quot;, data = Dahlberg, panel_identifier=c(&quot;id&quot;, &quot;year&quot;), steps = c(&quot;twostep&quot;), system_instruments = FALSE, max_instr_dependent_vars = 99, max_instr_predet_vars = 99, min_instr_dependent_vars = 2L, min_instr_predet_vars = 1L, collapse = FALSE ) ## Warning in pvargmm(dependent_vars = c(&quot;expenditures&quot;, &quot;revenues&quot;, &quot;grants&quot;), : ## The matrix D_e is singular, therefore the general inverse is used summary(ex1_dahlberg_data) Dynamic Panel VAR estimation, two-step GMM Transformation: Forward orthogonal deviationsGroup variable: idTime variable: yearNumber of observations = 1855Number of groups = 265Obs per group: min = 7Obs per group: avg = 7Obs per group: max = 7Number of instruments = 252   expenditures revenues grants lag1_expenditures 0.2846*** 0.2583** 0.0167   (0.0664) (0.0795) (0.0172) lag1_revenues -0.0470 0.0588 -0.0405**   (0.0637) (0.0726) (0.0151) lag1_grants -1.6746*** -2.2367*** 0.3204***   (0.2818) (0.2846) (0.0521) ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Instruments for equation Standard GMM-typeDependent vars: L(2,7))Collapse = FALSE Hansen test of overid. restrictions: chi2(243) = 263.01 Prob &gt; chi2 = 0.18(Robust, but weakened by many instruments.) Model selection procedure of Andrews and Lu (2001) to select the optimal lag length for our example Andrews_Lu_MMSC(ex1_dahlberg_data) ## $MMSC_BIC ## [1] -1610.877 ## ## $MMSC_AIC ## [1] -234.9924 ## ## $MMSC_HQIC ## [1] -792.3698 ex2_dahlberg_data &lt;- pvargmm(dependent_vars = c(&quot;expenditures&quot;, &quot;revenues&quot;, &quot;grants&quot;), lags = 2, transformation = &quot;fod&quot;, data = Dahlberg, panel_identifier=c(&quot;id&quot;, &quot;year&quot;), steps = c(&quot;twostep&quot;), system_instruments = FALSE, max_instr_dependent_vars = 99, max_instr_predet_vars = 99, min_instr_dependent_vars = 2L, min_instr_predet_vars = 1L, collapse = FALSE) ## Warning in pvargmm(dependent_vars = c(&quot;expenditures&quot;, &quot;revenues&quot;, &quot;grants&quot;), : ## The matrix D_e is singular, therefore the general inverse is used summary(ex2_dahlberg_data) Dynamic Panel VAR estimation, two-step GMM Transformation: Forward orthogonal deviationsGroup variable: idTime variable: yearNumber of observations = 1590Number of groups = 265Obs per group: min = 6Obs per group: avg = 6Obs per group: max = 6Number of instruments = 243   expenditures revenues grants lag1_expenditures 0.2572** 0.2486* 0.0135   (0.0893) (0.1018) (0.0178) lag1_revenues -0.1219 -0.0634 -0.0293   (0.0879) (0.0997) (0.0171) lag1_grants -3.0718*** -3.5849*** 0.1581*   (0.5941) (0.6168) (0.0636) lag2_expenditures -0.0247 0.0252 0.0178   (0.0791) (0.0834) (0.0157) lag2_revenues -0.2584** -0.2446** -0.0237   (0.0785) (0.0800) (0.0157) lag2_grants -1.5265*** -1.6687*** 0.0702   (0.1873) (0.2113) (0.0586) ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Instruments for equation Standard GMM-typeDependent vars: L(2,6))Collapse = FALSE Hansen test of overid. restrictions: chi2(225) = 260.11 Prob &gt; chi2 = 0.054(Robust, but weakened by many instruments.) Andrews_Lu_MMSC(ex2_dahlberg_data) ## $MMSC_BIC ## [1] -1486.935 ## ## $MMSC_AIC ## [1] -213.8917 ## ## $MMSC_HQIC ## [1] -734.1071 Sstability of the autoregressive process: stab_ex1_dahlberg_data &lt;- stability(ex1_dahlberg_data) print(stab_ex1_dahlberg_data) ## Eigenvalue stability condition: ## ## Eigenvalue Modulus ## 1 0.5370657+0.00000000i 0.53706574 ## 2 0.0633651+0.06442426i 0.09036383 ## 3 0.0633651-0.06442426i 0.09036383 ## ## All the eigenvalues lie inside the unit circle. ## PVAR satisfies stability condition. plot(stab_ex1_dahlberg_data) Generate impulse response functions. # ex1_dahlberg_data_oirf &lt;- oirf(ex1_dahlberg_data, n.ahead = 8) # # ex1_dahlberg_data_girf &lt;- girf(ex1_dahlberg_data, n.ahead = 8, ma_approx_steps= 8) # # ex1_dahlberg_data_bs &lt;- bootstrap_irf(ex1_dahlberg_data, typeof_irf = c(&quot;GIRF&quot;), # n.ahead = 8, # nof_Nstar_draws = 500, # confidence.band = 0.95) # # plot(ex1_dahlberg_data_girf, ex1_dahlberg_data_bs) 7.3 Cointegración [PENDIENTE] 7.4 Modelos No Lineales 7.4.1 Modelos de cambio de régimen En años recientes, los modelos de serie de tiempo han sido incorporados en análisis de la existencia de diferentes estados que son generados por procesos estocásticos subyacentes. En esta sección del curso revisaremos algunos modelos de cambio de régimen. Restringimos nuestra revisión a modelos que asuman que la dinámica de las series puede ser descrita por modelos del tipo AR y dejamos fuera procesos del tipo MA. En general distinguimos que existen dos tipos de modelos: 7.4.2 Regímenes determinados por información observable En estos casos asumimos que el régimen ocurre en un momento \\(t\\) y puede ser determinado por una variable observable. Este modelo es conocido como el modelo autoregresivo con umbral (TAR, Threshold Autoregressive model). En este caso también diremos que cuando el régimen está determinado por la información de la misma serie será llamado Self-Exciting TAR (SETAR). Veámos un caso particular. Supongamos que existe un umbral, \\(c\\), para el régimen que está determinado por \\(q_t = y_{t-1}\\) y que el estado de la naturaleza nos permite establecer dos estados o regímenes: \\[\\begin{equation} y_t = \\begin{cases} \\phi_{01} + \\phi_{11} y_{t-1} + \\varepsilon_t \\text{ si } y_{t-1} \\leq c \\\\ \\phi_{02} + \\phi_{12} y_{t-1} + \\varepsilon_t \\text{ si } y_{t-1} &gt; c \\end{cases} \\end{equation}\\] Donde asumiremos que \\(\\varepsilon_t\\) es i.i.d y que cumple con: \\[\\begin{equation*} \\mathbb{E}[\\varepsilon_t | \\Omega_{t-1}] = 0 \\end{equation*}\\] Donde \\(\\Omega_{t-1} = \\{ y_{t-1}, y_{t-2}, \\ldots \\}\\). Existe una variante de este modelo que suaviza la transición entre regímenes conocido como Smooth Transition AR (STAR) y puede ser especificada en su modalidad de dos regímenes como: \\[\\begin{equation} y_t = (\\phi_{01} + \\phi_{11} y_{t-1}) (1 - G(y_{t-1}; \\gamma, c)) + (\\phi_{02} + \\phi_{12} y_{t-1}) G(y_{t-1}; \\gamma, c) + \\varepsilon_t \\end{equation}\\] Donde \\(G(y_{t-1}; \\gamma, c)\\) es una función de distribución de probabilidad que suaviza la transición entre regímenes. La práctica común es suponer que esta tiene una forma logística: \\[\\begin{equation} G(y_{t-1}; \\gamma, c) = \\frac{1}{1 + e^{-\\gamma (y_{t-1} - c)}} \\end{equation}\\] Es posible hacer extensiones de lo anterior a modelos de orden superior; dando como resultado: \\[\\begin{equation} y_t = \\begin{cases} \\phi_{01} + \\phi_{11} y_{t-1} + \\phi_{21} y_{t-2} + \\ldots + \\phi_{p_1 1} y_{t-p_1} + \\varepsilon_t \\text{ si } y_{t-1} \\leq c \\\\ \\phi_{02} + \\phi_{12} y_{t-1} + \\phi_{22} y_{t-2} + \\ldots + \\phi_{p_2 2} y_{t-p_2} + \\varepsilon_t \\text{ si } y_{t-1} &gt; c \\end{cases} \\end{equation}\\] En el segundo caso: \\[\\begin{eqnarray*} y_t &amp; = &amp; (\\phi_{01} + \\phi_{11} y_{t-1} + \\phi_{21} y_{t-2} + \\ldots + \\phi_{p_1 1} y_{t-p_1}) (1 - G(y_{t-1}; \\gamma, c)) \\\\ &amp; &amp; + (\\phi_{02} + \\phi_{12} y_{t-1} + \\phi_{22} y_{t-2} + \\ldots + \\phi_{p_2 2} y_{t-p_2}) G(y_{t-1}; \\gamma, c) \\\\ &amp; &amp; + \\varepsilon_t \\end{eqnarray*}\\] De igual forma que en el caso de los modelos ARIMA y VAR, el número de rezagos utilizados es determinado mediante el uso de criterios de información como el de Akaike: \\[\\begin{equation} AIC(p_1, p_2) = n_1 ln(\\hat{\\sigma}^2_1) + n_2 ln(\\hat{\\sigma}^2_2) + 2(p_1 + 1) + 2(p_2 + 1) \\end{equation}\\] Los modelos SETAR y STAR generan procesos estacionarios siempre que cumplan ciertas condiciones. En estas notas nos enfocaremos únicamente en el modelo SETAR; el cual genera en un proceso estacionario cuando: Finalmente, en ocasiones podemos estar interesados en modelos donde los regímenes sean más de 2, es decir, digamos \\(m\\) umbrales bajo un modelo SETAR o STAR. Por ejemplo, en el caso de un modelo SETAR podemos verificar que \\(m\\) regímenes implican \\(m + 1\\) umbrales: \\(c_0, c_1, \\ldots, c_m\\). En cuyo caso: \\[\\begin{equation*} -\\infty = c_0 &lt; c_1 &lt; \\ldots &lt; c_{m-1} &lt; c_m = \\infty \\end{equation*}\\] Así, tendríamos ecuaciones: \\[\\begin{equation} y_t = \\phi_{0j} + \\phi_{ij} y_{t-1} + \\varepsilon_t \\text{ si } c_{j-1} &lt; y_{t-1} &lt; c_j \\end{equation}\\] Para \\(j = 1, 2, \\ldots, m\\). De forma similar, podemos recomponer el modelo STAR. 7.4.3 Regímenes determinados por variables no observables Este tipo de modelos asume que el régimen ocurre en el momento \\(t\\) y que no puede ser observado, ya que este es determinado por un proceso no observable, el cual denotamos como \\(s_t\\). En el caso de dos regímenes, \\(s_t\\) puede ser asumido como que toma 2 valores: 1 y 2, por ejemplo. Supongamos que el proceso subyacente tiene una forma del tipo AR(1) dado por: \\[\\begin{equation} y_t = \\begin{cases} \\phi_{01} + \\phi_{11} y_{t-1} + \\varepsilon_t \\text{ si } s_t = 1 \\\\ \\phi_{02} + \\phi_{12} y_{t-1} + \\varepsilon_t \\text{ si } s_t = 2 \\end{cases} \\label{eq_swching_obs} \\end{equation}\\] O en un formato más corto de notación: \\[\\begin{equation} y_t = \\phi_{0 s_t} + \\phi_{1 s_t} y_{t-1} + \\varepsilon_t \\end{equation}\\] Para complementar el modelo, las propiedades del proceso \\(s_t\\) necesitan ser especificadas. El modelo más popular dentro de esta familia es el propuesto por James Hamilton en 1989, el cual es conocido como Markov Switching Model (MSM), en el cual el proceso \\(s_t\\) se asume como un proceso de Markov de primer orden. Esto implica que el régimen actual \\(s_t\\) sólo depende del período \\(s_{t-1}\\). Así, el modelo es completado mediante la definición de las probabilidades de transición para moverse del un estado a otro: \\[\\begin{eqnarray*} \\mathbb{P}(s_t = 1 | s_{t-1} = 1) &amp; = &amp; p_{11} \\\\ \\mathbb{P}(s_t = 2 | s_{t-1} = 1) &amp; = &amp; p_{12} \\\\ \\mathbb{P}(s_t = 1 | s_{t-1} = 2) &amp; = &amp; p_{21} \\\\ \\mathbb{P}(s_t = 2 | s_{t-1} = 2) &amp; = &amp; p_{22} \\end{eqnarray*}\\] Así, \\(p_{ij}\\) es igual a la probabilidad de que la cadena de Markov pase del estado \\(i\\) en el momento \\(t-1\\) al estado \\(j\\) en el tiempo \\(t\\). En todo caso asumiremos que \\(p_{ij} &gt; 0\\) y que: \\[\\begin{eqnarray*} p_{11} + p_{12} = 1 \\\\ p_{21} + p_{22} = 1 \\end{eqnarray*}\\] Otro tipo de probabilidades a analizar son las probabilidades incondicionales de \\(\\mathbb{P}(s_t = i)\\), \\(i = 1, 2\\). Usando la teoría ergódica de las cadenas de Markov, estas probabilidades están dadas por: \\[\\begin{eqnarray*} \\mathbb{P}(s_t = 1) &amp; = &amp; \\frac{1 - p_{22}}{2 - p_{11} - p_{22}} \\\\ \\mathbb{P}(s_t = 2) &amp; = &amp; \\frac{1 - p_{11}}{2 - p_{11} - p_{22}} \\end{eqnarray*}\\] Un caso más general es el de múltiples regímenes en el cual \\(s_t\\) puede tomar cualquier valor \\(m &gt; 2\\), \\(m \\in \\mathbb{N}\\). Este modelo se puede escribir como: \\[\\begin{equation} y_t = \\phi_{0j} + \\phi_{1j} y_{t-1} + \\varepsilon_t \\text{ si } s_t = j \\text{, para } j = 1, 2, \\ldots, m \\end{equation}\\] Donde las probabilidades de transición estarán dadas por: \\[\\begin{equation} p_{ij} = \\mathbb{P}(s_t = j | s_{t-1} = i) \\text{ para } i , j = 1, 2, \\ldots, m \\end{equation}\\] Donde la ecuación anterior satisface que \\(p_{ij} &gt; 0\\), \\(\\forall i, j = = 1, 2, \\ldots, m\\) y que: \\[\\begin{equation*} \\sum_{j=1}^m p_{ij} = 1 \\text{ para } i = 1, 2, \\ldots, m \\end{equation*}\\] Finalmente, plantearemos el procedimiento empírico seguido para la estimación de este tipo de modelos: 7.5 Ejemplo: Modelos de Cambio de Regímen (TAR) Dependencies and Setup: Data: Monthly rates of deaths due to flu in the United States for 11 years ## X.8113721 ## 1 0.4458291 ## 2 0.3415985 ## 3 0.2774243 ## 4 0.2484958 ## 5 0.2525427 ## 6 0.2466902 Convert time series flu &lt;- ts(flu) D_flu = diff(flu, lag = 1) Plotting plot(flu, type = &quot;b&quot;, col = &quot;darkred&quot;, ylab = &quot;&quot;, main = &quot;Monthly rates of deaths due to flu in the United States&quot;) plot(D_flu, type=&quot;b&quot;, col = &quot;darkred&quot;, ylab = &quot;&quot;, main = &quot;Diff Monthly rates of deaths due to flu in the United States&quot;) The tsDyn package in R has simplified this code into a handful of steps: #?setar D_flu_tar4_05 &lt;- setar(D_flu, m = 4, thDelay = 0, th = 0.05) ## Warning: ## With the threshold you gave (0.05) there is a regime with less than trim=15% observations (86.51%, 13.49%, ) ## Warning: Possible unit root in the high regime. Roots are: 0.6182 0.6244 0.6244 ## 0.6182 summary(D_flu_tar4_05) ## ## Non linear autoregressive model ## ## SETAR model ( 2 regimes) ## Coefficients: ## Low regime: ## const.L phiL.1 phiL.2 phiL.3 phiL.4 ## 0.004432028 0.501179574 -0.206693004 0.120140800 -0.122254140 ## ## High regime: ## const.H phiH.1 phiH.2 phiH.3 phiH.4 ## 0.4079353 -0.7483325 -1.0323129 -2.0450407 -6.7117769 ## ## Threshold: ## -Variable: Z(t) = + (1) X(t)+ (0)X(t-1)+ (0)X(t-2)+ (0)X(t-3) ## -Value: 0.05 (fixed) ## Proportion of points in low regime: 86.51% High regime: 13.49% ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.1319314 -0.0217073 0.0015801 0.0196093 0.2674245 ## ## Fit: ## residuals variance = 0.002166, AIC = -778, MAPE = 412.4% ## ## Coefficient(s): ## ## Estimate Std. Error t value Pr(&gt;|t|) ## const.L 0.0044320 0.0051791 0.8558 0.3938357 ## phiL.1 0.5011796 0.0833821 6.0106 2.043e-08 *** ## phiL.2 -0.2066930 0.0608839 -3.3949 0.0009313 *** ## phiL.3 0.1201408 0.0576505 2.0839 0.0392868 * ## phiL.4 -0.1222541 0.0522363 -2.3404 0.0209137 * ## const.H 0.4079353 0.0314121 12.9866 &lt; 2.2e-16 *** ## phiH.1 -0.7483325 0.1118329 -6.6915 7.455e-10 *** ## phiH.2 -1.0323129 0.1420203 -7.2688 4.018e-11 *** ## phiH.3 -2.0450407 0.7055163 -2.8986 0.0044573 ** ## phiH.4 -6.7117769 0.8367945 -8.0208 7.911e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold ## Variable: Z(t) = + (1) X(t) + (0) X(t-1)+ (0) X(t-2)+ (0) X(t-3) ## ## Value: 0.05 (fixed) plot(D_flu_tar4_05) If we do not provide a threshold to the th option, setar searches over a grid to choose a threshold ~ 0.038: D_flu_tar4 &lt;- setar(D_flu, m = 4, thDelay = 0) ## Warning: Possible unit root in the high regime. Roots are: 0.6316 0.7222 0.7222 ## 0.6316 summary(D_flu_tar4) ## ## Non linear autoregressive model ## ## SETAR model ( 2 regimes) ## Coefficients: ## Low regime: ## const.L phiL.1 phiL.2 phiL.3 phiL.4 ## -0.00006287604 0.44640751264 -0.23158878472 0.10701408961 -0.14406085874 ## ## High regime: ## const.H phiH.1 phiH.2 phiH.3 phiH.4 ## 0.3486150 -0.5903335 -1.0318488 -2.4053812 -4.8052422 ## ## Threshold: ## -Variable: Z(t) = + (1) X(t)+ (0)X(t-1)+ (0)X(t-2)+ (0)X(t-3) ## -Value: 0.03798 ## Proportion of points in low regime: 84.92% High regime: 15.08% ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.277686 -0.020554 0.005116 0.020762 0.119627 ## ## Fit: ## residuals variance = 0.002405, AIC = -762, MAPE = 372.5% ## ## Coefficient(s): ## ## Estimate Std. Error t value Pr(&gt;|t|) ## const.L -0.000062876 0.005572290 -0.0113 0.9910158 ## phiL.1 0.446407513 0.089159422 5.0068 1.921e-06 *** ## phiL.2 -0.231588785 0.064328877 -3.6001 0.0004636 *** ## phiL.3 0.107014090 0.061034049 1.7534 0.0820953 . ## phiL.4 -0.144060859 0.055198346 -2.6099 0.0102109 * ## const.H 0.348615027 0.027460007 12.6954 &lt; 2.2e-16 *** ## phiH.1 -0.590333531 0.109881869 -5.3724 3.869e-07 *** ## phiH.2 -1.031848767 0.149570541 -6.8987 2.641e-10 *** ## phiH.3 -2.405381238 0.691344783 -3.4793 0.0007014 *** ## phiH.4 -4.805242210 0.828241480 -5.8017 5.451e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold ## Variable: Z(t) = + (1) X(t) + (0) X(t-1)+ (0) X(t-2)+ (0) X(t-3) ## ## Value: 0.03798 plot(D_flu_tar4) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
